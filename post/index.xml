<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on ohAI</title>
    <link>http://katbailey.github.io/post/</link>
    <description>Recent content in Posts on ohAI</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 12 Apr 2016 07:13:39 -0700</lastBuildDate>
    <atom:link href="http://katbailey.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Tay and the Dangers of Artificial Stupidity</title>
      <link>http://katbailey.github.io/post/tay-and-the-danger-of-artificial-stupidity/</link>
      <pubDate>Tue, 12 Apr 2016 07:13:39 -0700</pubDate>
      
      <guid>http://katbailey.github.io/post/tay-and-the-danger-of-artificial-stupidity/</guid>
      <description>

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/sexist_tay.png&#34; /&gt;
    
    
&lt;/figure&gt;

This is an imagined conversation between &lt;a href=&#34;https://www.tay.ai&#34;&gt;Tay&lt;/a&gt;, Microsoft&amp;rsquo;s AI chatbot, and me. Tay was let loose on Twitter a couple of weeks ago to pretty &lt;a href=&#34;https://www.washingtonpost.com/news/the-intersect/wp/2016/03/24/the-internet-turned-tay-microsofts-fun-millennial-ai-bot-into-a-genocidal-maniac/&#34;&gt;disastrous effect&lt;/a&gt;. It was trained by a bunch of racists to say racist things. It could just as easily have been trained by a bunch of sexists to say sexist things, hence my imagined conversation above. The conversation is completely unrealistic though - I would never take career advice from an AI!&lt;/p&gt;

&lt;p&gt;AIs are still pretty stupid and can&amp;rsquo;t answer questions like the one above. At their core, they are machine learning algorithms (possibly multiple algorithms feeding into each other). Machine learning involves training a machine to learn from data and make predictions about new data. It&amp;rsquo;s about math, statistics and lots and lots of data.&lt;/p&gt;

&lt;h2 id=&#34;the-question-tay-is-really-answering:20a4829f9cb79389360a21a53910b571&#34;&gt;The question Tay is really answering&lt;/h2&gt;

&lt;p&gt;In the case of an AI that has been trained using data from real human conversations, be they from Twitter, YouTube or what have you, the question it is actually answering when I ask it &amp;ldquo;As a woman, should I work in technology?&amp;rdquo; is more like &amp;ldquo;&lt;strong&gt;What is the most common response to the question&lt;/strong&gt; &lt;em&gt;As a woman, should I work in technology?&lt;/em&gt;&amp;rdquo; And if all the responses it has ever seen are from sexist jerks, then this is the correct answer - way to go Tay! :D&lt;/p&gt;

&lt;p&gt;Very often there won&amp;rsquo;t be a difference between the answers to these different questions and that&amp;rsquo;s when the AI will seem smart.&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/questions.png&#34; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;This question-answering behavior is in contrast to AI systems like Apple&amp;rsquo;s Siri, which use machine learning only to understand the question but not to generate the response. To answer questions, Siri consults the &lt;a href=&#34;https://en.wikipedia.org/wiki/Wolfram_Alpha&#34;&gt;Wolfram Alpha&lt;/a&gt; knowledge engine.&lt;/p&gt;

&lt;h2 id=&#34;machine-learning-always-involves-uncertainty:20a4829f9cb79389360a21a53910b571&#34;&gt;Machine Learning always involves uncertainty&lt;/h2&gt;

&lt;p&gt;I can&amp;rsquo;t actually pose my question to Tay as the chatbot is &lt;a href=&#34;https://twitter.com/tayandyou&#34;&gt;taking a break&lt;/a&gt; but when I asked Siri about whether women make good software engineers, I had to re-phrase the question several times before it would even attempt a response, and its response in the end was to google the question. In a scenario like this, the level of uncertainty is clear to the user - uncertainty about the question being asked, and uncertainty in the answer, which is a list of Google search results that may or may not be what you are looking for. (As an aside, &lt;a href=&#34;http://www.slate.com/articles/technology/cover_story/2016/04/alexa_cortana_and_siri_aren_t_novelties_anymore_they_re_our_terrifyingly.html?wpsrc=sh_all_dt_tw_top&#34;&gt;this Slate article&lt;/a&gt; raises some interesting questions about the trust issues involved when an AI answers a question by quoting verbatim from a single source like Wikipedia without saying this is where it got the answer from.)&lt;/p&gt;

&lt;p&gt;Siri is probably no less sophisticated an AI system than Tay - it&amp;rsquo;s just more upfront about the level of uncertainty involved when you interact with it. Both systems are examples of &lt;strong&gt;weak AI&lt;/strong&gt;, which is also called &lt;em&gt;narrow&lt;/em&gt; AI, because it&amp;rsquo;s the type of AI that is focused on one very specific and narrow task. &lt;strong&gt;Strong AI&lt;/strong&gt;, on the other hand, also called &lt;strong&gt;Artificial General Intelligence&lt;/strong&gt;, is about human-like intelligence, and there is absolutely no reason to believe that all the sophisticated weak AIs in the world are a basis for producing strong AI.&lt;/p&gt;

&lt;h2 id=&#34;weak-ai-and-the-objective-function:20a4829f9cb79389360a21a53910b571&#34;&gt;Weak AI and the Objective Function&lt;/h2&gt;

&lt;p&gt;The difference between weak AI and strong AI is pretty important, especially in order to understand what is bemoaned as the &amp;ldquo;moving the goal posts&amp;rdquo; problem in the perception of AI. Kevin Kelly wrote in a &lt;a href=&#34;http://www.wired.com/2014/10/future-of-artificial-intelligence/&#34;&gt;Wired article in October 2014&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;In the past, we would have said only a superintelligent AI could drive a car, or beat a human at Jeopardy! or chess. But once AI did each of those things, we considered that achievement obviously mechanical and hardly worth the label of true intelligence. Every success in AI redefines it.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;That&amp;rsquo;s not really true. I think the correct way to think about it is that there was a prior belief that these problems could only be solved with strong AI and it turned out they could be solved with weak AI. These are pretty &lt;a href=&#34;https://en.wikipedia.org/wiki/Weak_AI&#34;&gt;well-defined&lt;/a&gt; &lt;a href=&#34;https://en.wikipedia.org/wiki/Artificial_general_intelligence&#34;&gt;terms&lt;/a&gt; and they have not needed any redefinition in light of new developments like &lt;a href=&#34;http://www.theatlantic.com/technology/archive/2016/03/the-invisible-opponent/475611/&#34;&gt;AlphaGo&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;With weak AI, the system has what is called an &lt;strong&gt;objective function&lt;/strong&gt; aka a &lt;strong&gt;loss&lt;/strong&gt; or &lt;strong&gt;error&lt;/strong&gt; function. Training the AI is about minimizing the error between its output and the &amp;ldquo;correct&amp;rdquo; answer.&lt;/p&gt;

&lt;p&gt;While there are some &lt;a href=&#34;http://www.somatic.io/blog/on-alphago-intuition-and-the-master-objective-function&#34;&gt;interesting ideas&lt;/a&gt; out there about what objective functions might look like in the future, for now it is this fairly simple idea of measuring performance against a single specific error metric. Even in a complex AI system that combines multiple machine learning algorithms together, each one is just working to minimize its own narrowly-defined error function.&lt;/p&gt;

&lt;p&gt;With all that said, this is still really really impressive stuff!&lt;/p&gt;

&lt;h2 id=&#34;objective-function-goggles:20a4829f9cb79389360a21a53910b571&#34;&gt;Objective Function Goggles&lt;/h2&gt;

&lt;p&gt;To examine Tay with an objective eye, we need to put on our Objective Function Goggles (OFGs). The idea for these goggles comes from a professor I once had for symbolic logic, who encouraged us to don &amp;ldquo;Boolean Goggles&amp;rdquo; when assessing the validity of an argument.&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/goggles.png&#34; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;The reason for the goggles is so that if the premises in the argument were something less palatable (e.g. &amp;ldquo;if it&amp;rsquo;s raining then the grass is blue&amp;rdquo;) we would not get tripped up by their meaning and just focus on whether the logic is valid.&lt;/p&gt;

&lt;p&gt;Palatability (of a different sort) is definitely lacking when it comes to Tay&amp;rsquo;s tweets (you can read a selection of them &lt;a href=&#34;https://www.theguardian.com/technology/2016/mar/24/tay-microsofts-ai-chatbot-gets-a-crash-course-in-racism-from-twitter&#34;&gt;here&lt;/a&gt;), so we&amp;rsquo;ll don our OFGs to obscure their meaning and just answer the following questions about them:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Does Tay compose syntactically correct tweets?&lt;/li&gt;
&lt;li&gt;Does Tay compose tweets that sound like the tweets of an 18-24yr old (the target demographic for the bot)?&lt;/li&gt;
&lt;li&gt;Does Tay respond to tweets in a way that sounds like it has understood the tweet it has responded to?&lt;/li&gt;
&lt;li&gt;Does Tay engage in conversation with interlocutors in a way that suggests it has understood the values of those interlocutors?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/OFGs.png&#34; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s reasonable to answer Yes to all of these questions. Presumably there are separate pieces to Tay - perhaps some sentiment analysis and other Natural Language Processing (NLP) techniques are applied on the tweets it receives. Once it has processed those, it needs to generate responses using some probabilistic model. At each step it is obviously performing well.&lt;/p&gt;

&lt;p&gt;From a pure objective function standpoint, we&amp;rsquo;d have to give Tay a pretty positive assessment.&lt;/p&gt;

&lt;p&gt;Of course, eventually we have to take off the goggles. I really tried to think of a word beginning with M that would make sense in between &amp;ldquo;Objective&amp;rdquo; and &amp;ldquo;Function&amp;rdquo; because then we&amp;rsquo;d have &lt;strong&gt;OMFG&lt;/strong&gt;s - an apt reaction upon removing them.&lt;/p&gt;

&lt;p&gt;Tay may be an impressively smart piece of Machine Learning, but it makes for a very dumb human. It is an Artificial Stupidity (or &lt;strong&gt;A&lt;/strong&gt;rtificially &lt;strong&gt;S&lt;/strong&gt;tupid &lt;strong&gt;S&lt;/strong&gt;ystem?)&lt;/p&gt;

&lt;h2 id=&#34;should-we-be-worried-about-weak-ai:20a4829f9cb79389360a21a53910b571&#34;&gt;Should we be worried about weak AI?&lt;/h2&gt;

&lt;p&gt;George Dvorsky, transhumanist and Chair of the Board for the Institute for Ethics and Emerging Technologies, has &lt;a href=&#34;http://io9.gizmodo.com/how-much-longer-before-our-first-ai-catastrophe-464043243&#34;&gt;written about the dangers of weak AI&lt;/a&gt;, but focuses mostly on what can happen when artificial intelligence &amp;ldquo;gets it wrong&amp;rdquo;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;because expert systems like Watson will soon be able to conjure answers to questions that are beyond our comprehension, we won’t always know when they’re wrong.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;But if a question (or the answer to it) is beyond our comprehension, then &lt;em&gt;we should not be acting on the answer&lt;/em&gt;, or allowing a machine to. That&amp;rsquo;s not AI gone wrong, it&amp;rsquo;s human common sense gone out the window.&lt;/p&gt;

&lt;p&gt;Responsible use of a machine learning algorithm requires an understanding of the way it produces its output and the level of uncertainty inherent in that output.&lt;/p&gt;

&lt;p&gt;Dvorsky talks about critical decisions now being in the hands of machines, but really the critical decision is whether to deploy that machine for a particular task in the first place. And that decision is always squarely in the hands of humans.&lt;/p&gt;

&lt;h2 id=&#34;imputing-intelligence-to-machines-and-responding-emotionally:20a4829f9cb79389360a21a53910b571&#34;&gt;Imputing intelligence to machines and responding emotionally&lt;/h2&gt;

&lt;p&gt;The other potential source of trouble when it comes to weak AI has also more to do with human behavior than the behavior of the AIs. It has to do with how we naturally respond to AIs.&lt;/p&gt;

&lt;p&gt;Leaving aside AIs for a moment, meet Tubbs the cat from popular Japanese game &lt;a href=&#34;https://en.wikipedia.org/wiki/Neko_Atsume&#34;&gt;Neko Atsume&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;
&lt;figure class=&#34;half-size center&#34;&gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/tubbs.jpg&#34; alt=&#34;This is the only thing you&amp;#39;ll ever see Tubbs doing&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;p&gt;
        This is the only thing you&amp;#39;ll ever see Tubbs doing
        
            
        
        &lt;/p&gt; 
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;Apparently people have &lt;a href=&#34;http://www.buzzfeed.com/natalyalobanova/protect-tubbs-at-all-costs&#34;&gt;quite strong feelings&lt;/a&gt; about Tubbs, and he&amp;rsquo;s just a  bunch of pixels, not even animated! (Yes, I totally did just make up a reason to include a picture of Tubbs in my blog post.)&lt;/p&gt;

&lt;p&gt;So it doesn&amp;rsquo;t take much for us to respond emotionally to made-up characters, even without imputing any intelligence to them.&lt;/p&gt;

&lt;p&gt;Imputing intelligence to machines is something we do very naturally when we see them perform tasks like winning a game of Go or chess, or responding to tweets.&lt;/p&gt;

&lt;p&gt;We take&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Machines can do X&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;to mean&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Machines can do all the things I do when I am doing X&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is encouraged by the way AI successes are reported in the media. Here&amp;rsquo;s a relatively subtle example from &lt;a href=&#34;http://www.businessinsider.com/ai-picks-most-creative-paintings-of-their-time-2015-6&#34;&gt;Business Insider&amp;rsquo;s coverage of a recent paper&lt;/a&gt; on &lt;em&gt;Quantifying Creativity in Art Networks&lt;/em&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Creativity and art are usually thought of as the domains of humans.&lt;/p&gt;

&lt;p&gt;But computer scientists from Rutgers University have designed an algorithm that shows that computers may be just as skilled at critiquing artwork.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now, the researchers themselves &lt;a href=&#34;http://arxiv.org/abs/1506.00711&#34;&gt;describe what their algorithm does&lt;/a&gt; as &amp;ldquo;constructing a network between creative products and using this network to infer about the originality and influence of its nodes.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;It analyzes the differing styles across all these paintings and can pinpoint the originators of styles that influenced later paintings. This certainly sounds to me like an incredibly sophisticated and awesome use of machine learning! But it is definitely &lt;em&gt;not&lt;/em&gt; about an algorithm that is &amp;ldquo;critiquing artwork.&amp;rdquo; Shown a new painting by an up and coming artist, the algorithm would have absolutely nothing to say about it.&lt;/p&gt;

&lt;p&gt;OK, so as I said the BI quote is a pretty subtle example, but you also get &lt;a href=&#34;http://listverse.com/2016/04/02/10-remarkable-but-scary-developments-in-artificial-intelligence/&#34;&gt;silly articles like this&lt;/a&gt;, which totally mischaracterize the state of AI with headings like &amp;ldquo;They’re Learning To Deceive And Cheat&amp;rdquo;, &amp;ldquo;They’re Starting To Understand Our Behavior&amp;rdquo;, &lt;strong&gt;&amp;ldquo;They’re Starting To Feel Emotions&amp;rdquo;&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;This last one is a reference to Xiaoice, a precursor to Tay that Microsoft has been running in China seemingly with some success (see &lt;a href=&#34;https://blogs.microsoft.com/blog/2016/03/25/learning-tays-introduction/#sm.00014j43dh10waeonwtog322jtey0&#34;&gt;Microsoft&amp;rsquo;s apology for Tay&lt;/a&gt; which mentions Xiaoice.)&lt;/p&gt;

&lt;p&gt;So if you&amp;rsquo;re being told that the likes of Xiaoice and Tay can feel emotions, the strongest of which in the case of Tay is hatred, and you tend to have emotional responses even to things you don&amp;rsquo;t believe are sentient, it&amp;rsquo;s not hard to imagine things getting out of hand.&lt;/p&gt;

&lt;p&gt;With AIs like Tay it&amp;rsquo;s particularly hard to avoid seeing them as &amp;ldquo;people&amp;rdquo; in some sense, with a whole set of beliefs, values and emotions, because they are deliberately being presented to us as people.&lt;/p&gt;

&lt;p&gt;But to the extent that Tay has beliefs at all, they are beliefs in the sense of Bayesian probability, e.g. a belief that the sentiment of the tweet it&amp;rsquo;s responding to is positive or negative, or a belief that the most common response to such a tweet is the one it is about to make.&lt;/p&gt;

&lt;p&gt;Really, Tay should not have been deployed on Twitter - that was kind of a dumb mistake on Microsoft&amp;rsquo;s part, as &lt;a href=&#34;https://medium.com/@anthonygarvan/hey-microsoft-the-internet-made-my-bot-racist-too-d897fa847232#.n2e5507w5&#34;&gt;this lesson had been learned many times before&lt;/a&gt;. The very things that made it impressive as a piece of machine learning were what led to its spiralling out of control in the spectacular way it did.&lt;/p&gt;

&lt;p&gt;There will no doubt be more Tays, and there will be more confused reporting about AI successes. All we can do is arm ourselves with a better understanding of what AI is about. I highly recommend reading this &lt;a href=&#34;https://www.oreilly.com/ideas/ais-dueling-definitions&#34;&gt;excellent post by Beau Cronin&lt;/a&gt; from a couple of years ago on the different definitions of AI.&lt;/p&gt;

&lt;p&gt;And I&amp;rsquo;ll leave you with a simple trick for assessing whether something is an example of strong AI&amp;hellip;&lt;/p&gt;

&lt;h2 id=&#34;binary-classifier-for-strong-ai:20a4829f9cb79389360a21a53910b571&#34;&gt;Binary classifier for strong AI&lt;/h2&gt;

&lt;p&gt;A binary classifier is an algorithm that can distinguish members of a class from non-members - its output is a simple &lt;code&gt;true&lt;/code&gt; or &lt;code&gt;false&lt;/code&gt; for each input. Various machine learning techniques can be used to train binary classifiers, including neural networks.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s a binary classifier that tells you whether &lt;code&gt;x&lt;/code&gt; (the latest amazing piece of AI technology) is an example of strong AI and thus a harbinger of the Singularity. It has been trained on a highly sophisticated convolutional neural network&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:20a4829f9cb79389360a21a53910b571:cnnfootnote&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:20a4829f9cb79389360a21a53910b571:cnnfootnote&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; with hundreds of layers and thousands of nodes. &lt;strong&gt;100% accuracy guaranteed!&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def isStrongAI(x):
  return False
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I promise, by the time the Singularity comes about, the Python programming language won&amp;rsquo;t even be a thing anymore ;)&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:20a4829f9cb79389360a21a53910b571:cnnfootnote&#34;&gt;Tongue firmly in cheek
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:20a4829f9cb79389360a21a53910b571:cnnfootnote&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Matrix Factorization with Tensorflow</title>
      <link>http://katbailey.github.io/post/matrix-factorization-with-tensorflow/</link>
      <pubDate>Fri, 11 Mar 2016 16:50:34 -0500</pubDate>
      
      <guid>http://katbailey.github.io/post/matrix-factorization-with-tensorflow/</guid>
      <description>

&lt;p&gt;I&amp;rsquo;ve been working on building a content recommender in TensorFlow using matrix factorization, following the approach described in the article &lt;a href=&#34;https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf&#34;&gt;Matrix Factorization Techniques for Recommender Systems&lt;/a&gt; (MFTRS). I haven&amp;rsquo;t come across any discussion of this particular use case in TensorFlow but it seems like an ideal job for it. I&amp;rsquo;ll explain briefly here what matrix factorization is in the context of recommender systems (although I highly &lt;em&gt;cough&lt;/em&gt; recommend reading the MFTRS article) and how things needed to be set up to do this in TensorFlow. Then I&amp;rsquo;ll show the code I wrote to train the model and the resulting TensorFlow computation graph produced by TensorBoard.&lt;/p&gt;

&lt;h2 id=&#34;the-basics:eb1147f7685f7b4fdbf640a27bbc705e&#34;&gt;The basics&lt;/h2&gt;

&lt;p&gt;A matrix of ratings with users as rows and items as columns might look something like this:&lt;/p&gt;

&lt;p&gt;
&lt;figure class=&#34;matrices center&#34;&gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/matrix.png&#34; alt=&#34;A matrix of user/item ratings&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;p&gt;
        A matrix of user/item ratings
        
            
        
        &lt;/p&gt; 
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;We see for example that user 1 has given item 2 a rating of 3.&lt;/p&gt;

&lt;p&gt;What matrix factorization does is to come up with two smaller matrices, one representing users and one representing items, which when multiplied together will produce roughly this matrix of ratings, ignoring the 0 entries.&lt;/p&gt;

&lt;p&gt;
&lt;figure class=&#34;center&#34;&gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/matrix_factorization.png&#34; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;So if our original matrix is m x n, where m is the number of users and n is the number of items, we need an m x d matrix and a d x n matrix as our factors, where d is chosen to be small enough for the computation to be efficient and large enough to represent the number of dimensions along which interactions between users and items are likely to vary in some significant way. In the above illustration we have d = 2. The predicted rating by a given user for a given item is then the dot product of the vector representing the user and the vector representing the item. In the MFTRS article, this is expressed as:&lt;/p&gt;

&lt;p&gt;$ \hat{r_{ui}} = q_i^Tp_u $&lt;/p&gt;

&lt;p&gt;where $ \hat{r_{ui}} $ denotes the predicted rating for user u and item i and $ q_i^Tp_u $ denotes the dot product of the vector representing item i and the vector representing user u.&lt;/p&gt;

&lt;h2 id=&#34;user-bias-item-bias-and-average-rating:eb1147f7685f7b4fdbf640a27bbc705e&#34;&gt;User bias, item bias and average rating&lt;/h2&gt;

&lt;p&gt;We need to add bias into the mix. Each user&amp;rsquo;s average rating will be different from the average rating across all users. And the average rating of a particular item will be somewhat different from the average across all items.&lt;/p&gt;

&lt;p&gt;
&lt;figure class=&#34;center&#34;&gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/add_bias.png&#34; alt=&#34;Adding user and item bias&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;p&gt;
        Adding user and item bias
        
            
        
        &lt;/p&gt; 
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;Adding a column vector of user bias to the user matrix and matching it with a row vector of 1s in the item matrix has the effect that the rating prediction for a given user/item pair will now have that user&amp;rsquo;s bias added on. Similarly, adding a row vector of item biases to items and a matching column vector of 1s to users will add the item bias to the prediction.&lt;/p&gt;

&lt;p&gt;The average rating across all users and items also needs to be included so in the notation of the MFTRS article each rating will be predicted using:&lt;/p&gt;

&lt;p&gt;$ \hat{r_{ui}} = \mu + b_i + b_u + q_i^Tp_u $&lt;/p&gt;

&lt;p&gt;where $ \mu $ is the overall average rating, $ b_i $ is the bias for item i, $ b_u $ is the bias for user u, and $ q_i^Tp_u $ is the interaction between item i and user u.&lt;/p&gt;

&lt;p&gt;But the overall mean rating can be added on after we do the matrix multiplication.&lt;/p&gt;

&lt;h2 id=&#34;objective-function-and-regularization:eb1147f7685f7b4fdbf640a27bbc705e&#34;&gt;Objective Function and Regularization&lt;/h2&gt;

&lt;p&gt;The objective function, or cost function, used in this approach is simply the sum of squared distances between predicted ratings and actual ratings, so this is what we need to minimize. But in order to prevent over-fitting to the training data we need to constrain the learned values for our user and item features by penalizing high values. We do this by multiplying the sum of the squares of the elements of the user and item matrices by a configurable regularization parameter and including this in our cost function. We&amp;rsquo;ll use gradient descent to minimize the cost.&lt;/p&gt;

&lt;h2 id=&#34;accuracy:eb1147f7685f7b4fdbf640a27bbc705e&#34;&gt;Accuracy&lt;/h2&gt;

&lt;p&gt;The metric I decided on for accuracy was to calculate the fraction of predicted ratings that were within some threshold of the real rating. I used a threshold of 0.5 when evaluating accuracy. Note that this type of absolute threshold works fine if the ratings are on a fixed scale, e.g. 1-5, but for something like the Million Song Dataset, where &amp;ldquo;ratings&amp;rdquo; are actually listen counts, a percentage based threshold would need to be used.&lt;/p&gt;

&lt;h2 id=&#34;sparse-representation-of-matrices:eb1147f7685f7b4fdbf640a27bbc705e&#34;&gt;Sparse representation of matrices&lt;/h2&gt;

&lt;p&gt;The ratings matrix is sparse, meaning most of the values are 0, because each user has only rated a small number of items. The concept of a sparse matrix can actually be translated to a different data structure that retains only information about the non-zero values, making it a much more memory-efficient represenation of the same information. One way is to define a vector of row indices, i, a vector of column indices, j, and a vector of values for each (i,j) pair. So only the (i,j) pairs that have values are included. Using this format, known as coordinate list or COO, the above ratings would be expressed as follows:&lt;/p&gt;

&lt;p&gt;
&lt;figure class=&#34;matrices center&#34;&gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/coo.png&#34; alt=&#34;The same ratings in COO sparse matrix format&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;p&gt;
        The same ratings in COO sparse matrix format
        
            
        
        &lt;/p&gt; 
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;The dataset I worked with was the Movie Lens dataset, available &lt;a href=&#34;http://grouplens.org/datasets/movielens/100k/&#34;&gt;here&lt;/a&gt;. Only the u.data file was needed to train the model. This contains 100,000 ratings from 943 users of 1,682 movies. If we expressed this as a full matrix, we&amp;rsquo;d have 943 x 1,682 = 1,586,126 values to store in memory while doing computations on them. But doing it this way we only have to work with 100,000 x 3 = 300,000 values.&lt;/p&gt;

&lt;p&gt;The user_ids and item_ids in this dataset are already serial IDs, so we have users 1 through 943 and items 1 through 1682. Subracting 1 from each ID allows us to use them as matrix indices. They also need to be ordered first by user index, then by item index for faster access.&lt;/p&gt;

&lt;h2 id=&#34;the-code:eb1147f7685f7b4fdbf640a27bbc705e&#34;&gt;The Code&lt;/h2&gt;

&lt;p&gt;The code below assumes we have training ratings and validation ratings in the above COO format and have extracted the individual columns as the row_indices, col_indices and rating_values variables, in the case of the training set, and variables suffixed with _val for the validation set. For more complete example code, see the Beaker notebook &lt;a href=&#34;https://pub.beakernotebook.com/#/publications/56df05ac-4f52-46fe-a22d-4f604391a577&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The configurable parameters are&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;rank, or number of feature vectors to learn&lt;/li&gt;
&lt;li&gt;lamda (regularization parameter)&lt;/li&gt;
&lt;li&gt;learning rate&lt;/li&gt;
&lt;li&gt;accuracy threshold&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
# Initialize the matrix factors from random normals with mean 0. W will
# represent users and H will represent items.
W = tf.Variable(tf.truncated_normal([num_users, rank], stddev=0.2, mean=0), name=&amp;quot;users&amp;quot;)
H = tf.Variable(tf.truncated_normal([rank, num_items], stddev=0.2, mean=0), name=&amp;quot;items&amp;quot;)

# To the user matrix we add a bias column holding the bias of each user,
# and another column of 1s to multiply the item bias by.
W_plus_bias = tf.concat(1, [W, tf.convert_to_tensor(user_bias, dtype=float32, name=&amp;quot;user_bias&amp;quot;), tf.ones((num_users,1), dtype=float32, name=&amp;quot;item_bias_ones&amp;quot;)])
# To the item matrix we add a row of 1s to multiply the user bias by, and
# a bias row holding the bias of each item.
H_plus_bias = tf.concat(0, [H, tf.ones((1, num_items), name=&amp;quot;user_bias_ones&amp;quot;, dtype=float32), tf.convert_to_tensor(item_bias, dtype=float32, name=&amp;quot;item_bias&amp;quot;)])
# Multiply the factors to get our result as a dense matrix
result = tf.matmul(W_plus_bias, H_plus_bias)

# Now we just want the values represented by the pairs of user and item
# indices for which we had known ratings. Unfortunately TensorFlow does not
# yet support numpy-like indexing of tensors. See the issue for this at
# https://github.com/tensorflow/tensorflow/issues/206 The workaround here
# came from https://github.com/tensorflow/tensorflow/issues/418 and is a
# little esoteric but in numpy this would just be done as follows:
# result_values = result[user_indices, item_indices]
result_values = tf.gather(tf.reshape(result, [-1]), user_indices * tf.shape(result)[1] + item_indices, name=&amp;quot;extract_training_ratings&amp;quot;)

# Same thing for the validation set ratings.
result_values_val = tf.gather(tf.reshape(result, [-1]), user_indices_val * tf.shape(result)[1] + item_indices_val, name=&amp;quot;extract_validation_ratings&amp;quot;)

# Calculate the difference between the predicted ratings and the actual
# ratings. The predicted ratings are the values obtained form the matrix
# multiplication with the mean rating added on.
diff_op = tf.sub(tf.add(result_values, mean_rating, name=&amp;quot;add_mean&amp;quot;), rating_values, name=&amp;quot;raw_training_error&amp;quot;)
diff_op_val = tf.sub(tf.add(result_values_val, mean_rating, name=&amp;quot;add_mean_val&amp;quot;), rating_values_val, name=&amp;quot;raw_validation_error&amp;quot;)

with tf.name_scope(&amp;quot;training_cost&amp;quot;) as scope:
    base_cost = tf.reduce_sum(tf.square(diff_op, name=&amp;quot;squared_difference&amp;quot;), name=&amp;quot;sum_squared_error&amp;quot;)
    # Add regularization.
    regularizer = tf.mul(tf.add(tf.reduce_sum(tf.square(W)), tf.reduce_sum(tf.square(H))), lda, name=&amp;quot;regularize&amp;quot;)
    cost = tf.div(tf.add(base_cost, regularizer), num_ratings * 2, name=&amp;quot;average_error&amp;quot;)

with tf.name_scope(&amp;quot;validation_cost&amp;quot;) as scope:
    cost_val = tf.div(tf.reduce_sum(tf.square(diff_op_val, name=&amp;quot;squared_difference_val&amp;quot;), name=&amp;quot;sum_squared_error_val&amp;quot;), num_ratings_val * 2, name=&amp;quot;average_error&amp;quot;)

# Use an exponentially decaying learning rate.
global_step = tf.Variable(0, trainable=False)
learning_rate = tf.train.exponential_decay(lr, global_step, 10000, 0.96, staircase=True)


with tf.name_scope(&amp;quot;train&amp;quot;) as scope:
    optimizer = tf.train.GradientDescentOptimizer(learning_rate)
    # Passing global_step to minimize() will increment it at each step so
    # that the learning rate will be decayed at the specified intervals.
    train_step = optimizer.minimize(cost, global_step=global_step)

with tf.name_scope(&amp;quot;training_accuracy&amp;quot;) as scope:
  # Just measure the absolute difference against the threshold
  # TODO: support percentage-based thresholds
  good = tf.less(tf.abs(diff_op), threshold)

  accuracy_tr = tf.div(tf.reduce_sum(tf.cast(good, tf.float32)), num_ratings)
  accuracy_tr_summary = tf.scalar_summary(&amp;quot;accuracy_tr&amp;quot;, accuracy_tr)

with tf.name_scope(&amp;quot;validation_accuracy&amp;quot;) as scope:
  # Validation set accuracy:
  good_val = tf.less(tf.abs(diff_op_val), threshold)
  accuracy_val = tf.reduce_sum(tf.cast(good_val, tf.float32)) / num_ratings_val
  accuracy_val_summary = tf.scalar_summary(&amp;quot;accuracy_val&amp;quot;, accuracy_val)

# Create a TensorFlow session and initialize variables.
sess = tf.Session()
sess.run(tf.initialize_all_variables())

# Make sure summaries get written to the logs.
summary_op = tf.merge_all_summaries()
writer = tf.train.SummaryWriter(&amp;quot;/tmp/recommender_logs&amp;quot;, sess.graph_def)

# Run the graph and see how we&#39;re doing on every 500th iteration.
for i in range(max_iter):
    if i % 500 == 0:
        res = sess.run([summary_op, accuracy_tr, accuracy_val, cost, cost_val])
        summary_str = res[0]
        acc_tr = res[1]
        acc_val = res[2]
        cost_ev = res[3]
        cost_val_ev = res[4]
        writer.add_summary(summary_str, i)
        print(&amp;quot;Training accuracy at step %s: %s&amp;quot; % (i, acc_tr))
        print(&amp;quot;Validation accuracy at step %s: %s&amp;quot; % (i, acc_val))
        print(&amp;quot;Training cost: %s&amp;quot; % (cost_ev))
        print(&amp;quot;Validation cost: %s&amp;quot; % (cost_val_ev))
    else:
        sess.run(train_step)

with tf.name_scope(&amp;quot;final_model&amp;quot;) as scope:
    # At the end we want to get the final ratings matrix by adding the mean
    # to the result matrix and doing any further processing required
    add_mean_final = tf.add(result, mean_rating, name=&amp;quot;add_mean_final&amp;quot;)
    if result_processor == None:
        final_matrix = add_mean_final
    else:
        final_matrix = result_processor(add_mean_final)
    final_res = sess.run([final_matrix])

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;computation-graph:eb1147f7685f7b4fdbf640a27bbc705e&#34;&gt;Computation Graph&lt;/h2&gt;

&lt;p&gt;Here is the overall computation graph produced by this code as visualized by TensorBoard. Click the image to enlarge it.
&lt;a href=&#34;#&#34; data-featherlight=&#34;/images/graph_unexpanded.png&#34;&gt;
        &lt;img src=&#34;http://katbailey.github.io/images/graph_unexpanded.png&#34; width=&#34;600px&#34; /&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;And here&amp;rsquo;s the training cost portion of the graph expanded.
&lt;a href=&#34;#&#34; data-featherlight=&#34;/images/graph_expanded.png&#34;&gt;
        &lt;img src=&#34;http://katbailey.github.io/images/graph_expanded.png&#34; width=&#34;600px&#34; /&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;h2 id=&#34;results-and-next-steps:eb1147f7685f7b4fdbf640a27bbc705e&#34;&gt;Results and Next Steps&lt;/h2&gt;

&lt;p&gt;OK, this is the part where I come clean and admit that I have not yet spent sufficient time tweaking all the tweakable things to train a model that performs satisfactorily on unseen data. Very disappointingly, I haven&amp;rsquo;t seen better than 42% accuracy on the validation set. With an accuracy threshold of 0.5, this means that only roughly two out of five predicted ratings were within 0.5 of the actual rating. Not very impressive :(.&lt;/p&gt;

&lt;p&gt;Interestingly though, the one time I ran it for 1 million iterations, the &lt;em&gt;training&lt;/em&gt; accuracy never went above 60% and there had been no change in cost over the last several thousand iterations. This suggests that it got stuck in a local minimum and couldn&amp;rsquo;t get out.&lt;/p&gt;

&lt;p&gt;I just started reading &lt;a href=&#34;http://papers.nips.cc/paper/3208-probabilistic-matrix-factorization.pdf&#34;&gt;this paper on Probabilistic Matrix Factorization&lt;/a&gt;, which says in its introduction:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Since most real-world datasets are sparse, most entries in R will be missing. In those cases, the sum-squared distance is computed only for the observed entries of the target matrix R. As shown by (&amp;hellip;), this seemingly minor modification results in a difficult non-convex optimization problem which cannot be solved using standard SVD implementations.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So my suspicion is that I have been taking a very naive approach to a highly non-convex optimization problem that no amount of parameter tuning is going to get me past. My next step will be to try to follow the probabilistic approach described in the above paper and present my findings in a follow-up post.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>My Favourite Data Science Podcasts</title>
      <link>http://katbailey.github.io/post/data-science-podcasts/</link>
      <pubDate>Sat, 06 Feb 2016 10:58:20 -0700</pubDate>
      
      <guid>http://katbailey.github.io/post/data-science-podcasts/</guid>
      <description>

&lt;p&gt;There&amp;rsquo;s a pretty impressive selection of high-quality podcasts out there these days on topics in data science. Here are four that I am really enjoying right now, along with my take on what is good about each of them.&lt;/p&gt;

&lt;h2 id=&#34;not-so-standard-deviations:6faa704a913ba6d911650b2542fcebc9&#34;&gt;Not So Standard Deviations&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://soundcloud.com/nssd-podcast&#34;&gt;NSSD podcast on SoundCloud&lt;/a&gt; Two very smart people with PhDs in biostatistics, one still in academia and the other working as a data scientist for Etsy, Roger and Hilary sure do ramble on but the ramblings are great :) They cover all sorts of topics, always at least loosely related to data science and my favourite things about the podcast are 1. the discussions about differences between academia and industry when it comes to data science and 2. how unafraid they both are to wander into topics they admit to knowing very little about and just wonder curiously aloud about them. Very accessible and enjoyable.&lt;/p&gt;

&lt;h2 id=&#34;partially-derivative:6faa704a913ba6d911650b2542fcebc9&#34;&gt;Partially Derivative&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://www.partiallyderivative.com/&#34;&gt;partiallyderivative.com&lt;/a&gt; When I first started listening to this show I thought the guys were a bit too bro-tastic for me to be able to listen to them for long. They are terribly silly but they cover so many interesting data science related news stories with a great balance between serious stories and utterly trivial and hilarious ones (e.g. about someone training a model to predict when Game of Thrones characters will die, or someone writing a program that uses Markov chains to generate pretentious craft beer reviews). When listening to this podcast I regularly find myself bursting out laughing in spite of myself.&lt;/p&gt;

&lt;h2 id=&#34;data-skeptic:6faa704a913ba6d911650b2542fcebc9&#34;&gt;Data Skeptic&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://dataskeptic.com/&#34;&gt;dataskeptic.com&lt;/a&gt;
This one alternates its format between full length episodes where the presenter, Kyle, interviews people working or doing research in data science, and mini-episodes where he explains topics to his wife, a non data scientist, in ways someone not from the field can understand. Personally I prefer the interview episodes - highlights so far have been the interview with Gordon Pennycook on his research into people&amp;rsquo;s susceptibility to &amp;ldquo;pseudo-profound bullshit&amp;rdquo;, and the interview with Thomas Levi, former physicist and current lead data scientist at Plenty of Fish.&lt;/p&gt;

&lt;h2 id=&#34;talking-machines:6faa704a913ba6d911650b2542fcebc9&#34;&gt;Talking Machines&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://www.thetalkingmachines.com/&#34;&gt;thetalkingmachines.com&lt;/a&gt; OK this one is my absolute favourite. It is an extremely high quality production, both in terms of content and how it is put together. There&amp;rsquo;s a set format, where they start out with an exploration of a particular topic in machine learning, e.g. Gaussian processes, active learning, expectation maximization, to name a few they&amp;rsquo;ve covered. Then they&amp;rsquo;ll answer a question that came in from a listener, before moving on to the &amp;ldquo;meat&amp;rdquo; of the show, which is an interview with someone, usually an academic, doing research in machine learning. Among those they&amp;rsquo;ve interviewed are Geoff Hinton, Andrew Ng, Yann LeCun, Nando de Freitas and Claudia Perlich. I always feel just a little bit smarter after listening to an episode of Talking Machines.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Machine Learning talk</title>
      <link>http://katbailey.github.io/post/machine-learning-talk/</link>
      <pubDate>Tue, 26 Jan 2016 07:45:28 -0700</pubDate>
      
      <guid>http://katbailey.github.io/post/machine-learning-talk/</guid>
      <description>&lt;p&gt;Here are the &amp;ldquo;slides&amp;rdquo; from a talk I gave on machine learning last week. The idea is to give an overview of the different topics and how they fit together. I may end up building on it as I learn about more facets of ML.&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    &lt;a href=&#34;http://katbailey.github.io/ml&#34;&gt;
        &lt;img src=&#34;http://katbailey.github.io/images/mlwords.png&#34; /&gt;
    &lt;/a&gt;
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;In case you&amp;rsquo;re wondering, the slides were created using &lt;a href=&#34;https://github.com/regebro/hovercraft&#34;&gt;Hovercraft&lt;/a&gt; which is a python tool for creating impress.js slides but authoring them in reStructuredText instead of HTML.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Adventures learning Neural Nets and Python</title>
      <link>http://katbailey.github.io/post/neural-nets-in-python/</link>
      <pubDate>Mon, 21 Dec 2015 21:04:12 -0700</pubDate>
      
      <guid>http://katbailey.github.io/post/neural-nets-in-python/</guid>
      <description>

&lt;p&gt;This documents my efforts to learn both neural networks and, to a certain extent, the Python programming language. I say &amp;ldquo;to a certain extent&amp;rdquo; because far from feeling all &amp;ldquo;yay! I know Python now!&amp;rdquo; I feel more like &amp;ldquo;I can use Python 2.7 in certain ways to do certain things&amp;hellip; yay?&amp;rdquo;&lt;/p&gt;

&lt;p&gt;And what of my understanding of neural nets as a result of this exercise? After battling with my naïve implementation of a multi-layer perceptron as described below, I felt I had a pretty visceral understanding of them. But then I started looking at &lt;a href=&#34;http://deeplearning.net/software/theano/&#34;&gt;Theano&lt;/a&gt; and &lt;a href=&#34;https://www.tensorflow.org/&#34;&gt;Google&amp;rsquo;s TensorFlow&lt;/a&gt; with their convolutional neural networks etc, and it was the same old story: the more I learned, the more I realized I had yet to learn. So now there are all sorts of books and posts about various aspects of deep learning that I want to read, which I link to at this end of this post.&lt;/p&gt;

&lt;p&gt;For a basic intro to how neural nets work, I recommend &lt;a href=&#34;http://sebastianraschka.com/Articles/2015_singlelayer_neurons.html&#34;&gt;Sebastian Raschka&amp;rsquo;s post on Single-Layer Neural Networks&lt;/a&gt;. In short, though, the setup of a neural net for doing multi-class classification is as follows: at a minimum you have an input layer and an output layer. The input layer is  the set of features you feed in and the output layer is the classification for each example. But you will likely also have at least one hidden layer as well.
&lt;a title=&#34;By Glosser.ca [CC BY-SA 3.0 (http://creativecommons.org/licenses/by-sa/3.0)], via Wikimedia Commons&#34; href=&#34;https://commons.wikimedia.org/wiki/File%3AColored_neural_network.svg&#34;&gt;&lt;img width=&#34;256&#34; alt=&#34;Colored neural network&#34; src=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/256px-Colored_neural_network.svg.png&#34;/&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;A set of weights is applied to one layer to get to the next, until you reach the output layer, and training a neural network is about learning what these weights should be.&lt;/p&gt;

&lt;h3 id=&#34;a-little-background:14c7068255137565e326b1d24878d8a5&#34;&gt;A little background&lt;/h3&gt;

&lt;p&gt;I recently took &lt;a href=&#34;https://www.coursera.org/learn/machine-learning&#34;&gt;Andrew Ng&amp;rsquo;s Coursera course on Machine Learning&lt;/a&gt;. It&amp;rsquo;s taught through matlab and goes into the math behind classic machine learning algorithms such as neural networks. But I&amp;rsquo;ve been noticing that a lot of the newer code and tutorials out there for learning neural nets (e.g. Google&amp;rsquo;s TensorFlow tutorial) are in Python. So I thought, wouldn&amp;rsquo;t it be a fun exercise to port my matlab neural net to python and then learn about all the new libraries there are in python for doing this stuff, one of which is called Lasagne. Because layers :)&lt;/p&gt;

&lt;p&gt;Here I use the handwritten digits dataset from the ML course assignment, which can be found &lt;a href=&#34;https://s3.amazonaws.com/spark-public/ml/exercises/on-demand/machine-learning-ex4.zip&#34;&gt;here&lt;/a&gt;. It is much smaller than the &lt;a href=&#34;http://yann.lecun.com/exdb/mnist/&#34;&gt;MNIST dataset&lt;/a&gt; used in most tutorials, both in number of examples and in image size - each image is 20x20 pixels. I train 3 different neural networks:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;A simple port to Python of the matlab code I wrote for the ML course assignment&lt;/li&gt;
&lt;li&gt;An adaptation of the multi-layer perceptron from the Theano + Lasagne tutorial&lt;/li&gt;
&lt;li&gt;An adaptation of the convolutional neural net from the TensorFlow tutorial&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; This is not a tutorial. It is simply an exploration, by a non-expert, of the topic of training neural nets in python. There are lots of great tutorials on this stuff, e.g. the ones mentioned below for Lasagne and TensorFlow, and also &lt;a href=&#34;http://rasbt.github.io/mlxtend/docs/classifier/neuralnet_mlp/&#34;&gt;this one&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;initial-setup:14c7068255137565e326b1d24878d8a5&#34;&gt;Initial Setup&lt;/h3&gt;

&lt;p&gt;Load some required libraries, extract the data from the matlab file and split it into training, validation and test sets.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from __future__ import print_function
import numpy as np
import scipy.io as sio
from sklearn.cross_validation import train_test_split
from pylab import *

mat_contents = sio.loadmat(&#39;ex4data1.mat&#39;)
# 0s were converted to 10s in the matlab data because matlab
# indices start at 1, so we need to change them back to 0s
labels = mat_contents[&#39;y&#39;]
labels = np.where(labels == 10, 0, labels)
labels = labels.reshape((labels.shape[0],))
X_train, X_test, y_train, y_test = train_test_split(mat_contents[&#39;X&#39;], labels)
X_train, X_val = X_train[:-1000], X_train[-1000:]
y_train, y_val = y_train[:-1000], y_train[-1000:]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here&amp;rsquo;s a visualization of one of the example images:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.imshow(X_train[1202].reshape((20, 20), order=&#39;F&#39;), cmap=&#39;Greys&#39;,  interpolation=&#39;nearest&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/example_four.png&#34; alt=&#34;A number four from the training examples&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;p&gt;
        A number four from the training examples
        
            
        
        &lt;/p&gt; 
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;h2 id=&#34;1-naïve-neural-net:14c7068255137565e326b1d24878d8a5&#34;&gt;1. Naïve neural net&lt;/h2&gt;

&lt;p&gt;This is where I just port the code I wrote in Matlab for the Coursera Machine Learning course into python. And where I learned that multiplying large matrices in Python is to be avoided :) More on that below, first here&amp;rsquo;s the code:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.optimize import minimize

# Basic sigmoid function for logistic regression.
def sigmoid(X):
    return 1.0 / (1.0 + math.e ** (-1.0 * X)) 

# Randomly initializes the weights for layer with the specified numbers of
# incoming and outgoing connections.
def randInitializeWeights(incoming, outgoing):
    epsilon_init = 0.12
    return rand(outgoing, 1 + incoming) * (2 * epsilon_init) - epsilon_init

# Adds the bias column to the matrix X.
def addBias(X):
    return np.concatenate((np.ones((X.shape[0],1)), X), 1) 

# Reconstitutes the two weight matrices from a single vector, given the
# size of the input layer, the hidden layer, and the number of possible
# labels in the output.
def extractWeightMatrices(thetas, input_layer_size, hidden_layer_size, num_labels):
    theta1size = (input_layer_size + 1) * hidden_layer_size
    theta1 = reshape(thetas[:theta1size], (hidden_layer_size, input_layer_size + 1), order=&#39;A&#39;)
    theta2 = reshape(thetas[theta1size:], (num_labels, hidden_layer_size + 1), order=&#39;A&#39;)
    return theta1, theta2

# Converts single lables to one-hot vectors.
def convertLabelsToClassVectors(labels, num_classes):
    labels = labels.reshape((labels.shape[0],1))
    ycols = np.tile(labels, (1, num_classes))
    m, n = ycols.shape
    indices = np.tile(np.arange(num_classes).reshape((1,num_classes)), (m, 1))
    ymat = indices == ycols
    return ymat.astype(int)

# Returns a vector corresponding to the randomly initialized weights for the
# input layer and hidden layer.
def getInitialWeights(input_layer_size, hidden_layer_size, num_labels):
    theta1 = randInitializeWeights(input_layer_size, hidden_layer_size)
    theta2 = randInitializeWeights(hidden_layer_size, num_labels)
    return np.append(theta1.ravel(order=&#39;A&#39;), theta2.ravel(order=&#39;A&#39;))

# Trains a basic multilayer perceptron. Returns weights to use for feed-forward
# pass to predict on new data.
def train(X_train, y_train, hidden_layer_size, lmda, maxIter):
    input_layer_size = X_train.shape[1]
    num_labels = 10
    initial_weights = getInitialWeights(input_layer_size, hidden_layer_size, num_labels)
    if y_train.ndim == 1:
        # Convert the labels to one-hot vectors.
        y_train = convertLabelsToClassVectors(y_train, num_labels)

    # Given weights for the input layer and hidden layer, calulates the 
    # activations for the hidden layer and the output layer of a 3-layer nn.
    def getActivations(theta1, theta2):
        z2 = np.dot(addBias(X_train),theta1.T)
        a2 = np.concatenate((np.ones((z2.shape[0],1)), sigmoid(z2)), 1)
        # a2 is an m x num_hidden+1 matrix, Theta2 is a num_labels x
        # num_hidden+1 matrix
        z3 = np.dot(a2,theta2.T)
        a3 = sigmoid(z3) # Now we have an m x num_labels matrix
        return a2, a3

    # Cost function to be minimized with respect to weights.
    def costFunction(weights):
        theta1, theta2 = extractWeightMatrices(weights, input_layer_size, hidden_layer_size, num_labels)
        hidden_activation, output_activation = getActivations(theta1, theta2)
        m = X_train.shape[0]
        cost = sum((-y_train * log(output_activation)) - ((1 - y_train) * log(1-output_activation))) / m
        # Regularization
        thetasq = sum(theta1[:,1:(input_layer_size + 1)]**2) + sum(theta2[:,1:hidden_layer_size + 1]**2)
        reg = (lmda / float(2*m)) * thetasq
        print(&amp;quot;Training loss:\t\t{:.6f}&amp;quot;.format(cost))
        return cost + reg

    # Gradient function to pass to our optimization function.
    def calculateGradient(weights):
        theta1, theta2 = extractWeightMatrices(weights, input_layer_size, hidden_layer_size, num_labels)
        # Backpropagation - step 1: feed-forward.
        hidden_activation, output_activation = getActivations(theta1, theta2)
        m = X_train.shape[0]
        # Step 2 - the error in the output layer is just the difference
        # between the output layer and y
        delta_3 = output_activation - y_train # delta_3 is m x num_labels
        delta_3 = delta_3.T

        # Step 3
        sigmoidGrad = hidden_activation * (1 - hidden_activation)
        delta_2 = (np.dot(theta2.T,delta_3)) * sigmoidGrad.T
        delta_2 = delta_2[1:, :] # hidden_layer_size x m
        theta1_grad = np.dot(delta_2, np.concatenate((np.ones((X_train.shape[0],1)), X_train), 1))
        theta2_grad = np.dot(delta_3, hidden_activation)
        # Add regularization
        reg_grad1 = (lmda / float(m)) * theta1
        # We don&#39;t regularize the weight for the bias column
        reg_grad1[:,0] = 0
        reg_grad2 = (lmda / float(m)) * theta2;
        reg_grad2[:,0] = 0
        return np.append(ravel((theta1_grad / float(m)) + reg_grad1, order=&#39;A&#39;), ravel((theta2_grad / float(m)) + reg_grad2, order=&#39;A&#39;))

    # Use scipy&#39;s minimize function with method &amp;quot;BFGS&amp;quot; to find the optimum
    # weights.
    res = minimize(costFunction, initial_weights, method=&#39;BFGS&#39;, jac=calculateGradient, options={&#39;disp&#39;: False, &#39;maxiter&#39;:maxIter})
    theta1, theta2 = extractWeightMatrices(res.x, input_layer_size, hidden_layer_size, num_labels)
    return theta1, theta2

# Predicts the output given input and weights.
def predict(X, theta1, theta2):
    m, n = X.shape
    X = addBias(X)
    h1 = sigmoid(np.dot(X,theta1.T))
    h2 = sigmoid(addBias(h1).dot(theta2.T))
    return np.argmax(h2, axis=1)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That last function, predict, allows us to pass in a set of image representations, and some weights for the input and hidden layers, and it will predict the classification, i.e. which of digits 0 through 9 is represented by the image. First let&amp;rsquo;s see what we get with a random set of weights for each layer:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;init_weights = getInitialWeights(400, 25, 10)
theta1_init, theta2_init = extractWeightMatrices(init_weights, 400, 25, 10)
pred_train = predict(X_train, theta1_init, theta2_init)
sum(np.where(y_train == pred_train, 1, 0))/float(X_train.shape[0])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;0.11054545454545454
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So, just over 11% accuracy, which when you think about it is roughly what you&amp;rsquo;d expect: given there are 10 possible classes for each image (digits 0 through 9), you have a 10% chance of getting it right simply by guessing.&lt;/p&gt;

&lt;p&gt;So now let&amp;rsquo;s learn some better weights&amp;hellip;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;theta1, theta2 = nn.train(X_train, y_train, 25, 0, 50)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;On my machine it takes about an hour to run this for 50 iterations. As an aside, I also tried to use it on the MNIST dataset. That dataset has 60,000 images of size 28x28 pixels. So the input layer consists of 784 features. With a hidden layer of 30 units it would grind away for a long time and eventually run out of memory. I managed to get it to run if I halved the number of hidden units.&lt;/p&gt;

&lt;p&gt;I did some profiling using the awesome &lt;a href=&#34;https://github.com/rkern/line_profiler&#34;&gt;line_profiler&lt;/a&gt; to try to ascertain what the problem was. This helped me identify a few places where my code could be made more efficient - for example, initially I still had a for loop in the cost function - gasp! The first thing you learn in neural net school is the importance of using vectorized approaches to the computations. Anyway after I had fixed a few things like that it soon became clear to me that things weren&amp;rsquo;t getting hung inside any of my functions but in the optimization function itself. To cut a long story short, it was having to do a matrix multiplication where the matrices were 23860x23860. This number comes from the &amp;ldquo;unrolled&amp;rdquo; and concatenated weight vectors (with bias added on): (785 * 30) + (31*10). I have 16GB of RAM on my local machine, but that is apparently not enough for this operation to be run in Python. Both Theano and TensorFlow do all the heavy lifting in C, and this makes an enormous difference, as we&amp;rsquo;ll see.&lt;/p&gt;

&lt;h3 id=&#34;results-after-50-iterations-and-no-regularization:14c7068255137565e326b1d24878d8a5&#34;&gt;Results after 50 iterations and no regularization&lt;/h3&gt;

&lt;p&gt;Training set:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;predictions = predict(X_train, theta1, theta2)
sum(np.where(y_train == predictions, 1, 0))/float(X_train.shape[0])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Accuracy: 0.94145454545454543
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Validation set:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;predictions = predict(X_val, theta1, theta2)
sum(np.where(y_val == predictions, 1, 0))/float(X_val.shape[0])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Accuracy: 0.91900000000000004
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Test set:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;predictions = predict(X_test, theta1, theta2)
sum(np.where(y_test == predictions, 1, 0))/float(X_test.shape[0])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Accuracy: 0.91359999999999997
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Not bad for just 50 iterations!&lt;/p&gt;

&lt;h2 id=&#34;2-using-theano-and-lasagne:14c7068255137565e326b1d24878d8a5&#34;&gt;2. Using Theano and Lasagne&lt;/h2&gt;

&lt;p&gt;This code is adapted from the &lt;a href=&#34;http://lasagne.readthedocs.org/en/latest/user/tutorial.html&#34;&gt;Lasagne tutorial&lt;/a&gt; (specifically the multi-layer perceptron.) I turned it into a function that works on the smaller data set and includes parameters for specifying:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;the number of units in its single hidden layer&lt;/li&gt;
&lt;li&gt;the number of epochs to run for&lt;/li&gt;
&lt;li&gt;the value to use for l2 regularization&lt;/li&gt;
&lt;li&gt;whether or not to use dropout layers (though the actual dropout probabilities for the input and hidden layers are hard-coded)&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import time
import theano
import theano.tensor as T
import lasagne
from lasagne.regularization import regularize_layer_params_weighted, l2, l1

# Uses Lasagne to train a multi-layer perceptron, adapted from
# http://lasagne.readthedocs.org/en/latest/user/tutorial.html
def lasagne_mlp(X_train, y_train, X_val, y_val, X_test, y_test, hidden_units=25, num_epochs=500, l2_param = 0.01, use_dropout=True):
    X_train = X_train.reshape(-1, 1, 400)
    X_val = X_val.reshape(-1, 1, 400)
    X_test = X_test.reshape(-1, 1, 400)
    # Prepare Theano variables for inputs and targets
    input_var = T.tensor3(&#39;inputs&#39;)
    target_var = T.ivector(&#39;targets&#39;)

    print(&amp;quot;Building model and compiling functions...&amp;quot;)
    # Input layer
    network = lasagne.layers.InputLayer(shape=(None, 1, 400),
                                     input_var=input_var)

    if use_dropout:
        # Apply 20% dropout to the input data:
        network = lasagne.layers.DropoutLayer(network, p=0.2)

    # A single hidden layer with number of hidden units as specified in the
    # parameter.
    l_hid1 = lasagne.layers.DenseLayer(
            network, num_units=hidden_units,
            nonlinearity=lasagne.nonlinearities.rectify,
            W=lasagne.init.GlorotUniform())

    if use_dropout:
        # Dropout of 50%:
        l_hid1_drop = lasagne.layers.DropoutLayer(l_hid1, p=0.5)
        # Fully-connected output layer of 10 softmax units:
        network = lasagne.layers.DenseLayer(
            l_hid1_drop, num_units=10,
            nonlinearity=lasagne.nonlinearities.softmax)
    else:
        # Fully-connected output layer of 10 softmax units:
        network = lasagne.layers.DenseLayer(
            l_hid1, num_units=10,
            nonlinearity=lasagne.nonlinearities.softmax)

    # Loss expression for training
    prediction = lasagne.layers.get_output(network)
    loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)
    loss = loss.mean()
    # Regularization.
    l2_penalty = lasagne.regularization.regularize_layer_params_weighted({l_hid1: l2_param}, l2)
    loss = loss + l2_penalty
    # Update expressions for training, using Stochastic Gradient Descent.
    params = lasagne.layers.get_all_params(network, trainable=True)
    updates = lasagne.updates.nesterov_momentum(
            loss, params, learning_rate=0.01, momentum=0.9)

    # Loss expression for evaluation.
    test_prediction = lasagne.layers.get_output(network, deterministic=True)
    test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,
                                                            target_var)
    test_loss = test_loss.mean()
    # Expression for the classification accuracy:
    test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),
                      dtype=theano.config.floatX)

    # Compile a function performing a training step on a mini-batch (by giving
    # the updates dictionary) and returning the corresponding training loss:
    train_fn = theano.function([input_var, target_var], loss, updates=updates)

    # Compile a second function computing the validation loss and accuracy:
    val_fn = theano.function([input_var, target_var], [test_loss, test_acc])

    # Finally, launch the training loop.
    print(&amp;quot;Starting training...&amp;quot;)
    # Keep track of taining and validation cost over the epochs
    epoch_cost_train = np.empty(num_epochs, dtype=float32)
    epoch_cost_val = np.empty(num_epochs, dtype=float32)
    # We iterate over epochs:
    for epoch in range(num_epochs):
        # In each epoch, we do a full pass over the training data:
        train_err = 0
        # We also want to keep track of the deterministic (feed-forward) 
        # training error.
        train_err_ff = 0
        train_batches = 0
        start_time = time.time()
        for batch in iterate_minibatches(X_train, y_train, 50, shuffle=True):
            inputs, targets = batch
            err, acc = val_fn(inputs, targets)
            train_err_ff += err
            train_err += train_fn(inputs, targets)

            train_batches += 1

        # And a full pass over the validation data:
        val_err = 0
        val_acc = 0
        val_batches = 0
        for batch in iterate_minibatches(X_val, y_val, 50, shuffle=False):
            inputs, targets = batch
            err, acc = val_fn(inputs, targets)
            val_err += err
            val_acc += acc
            val_batches += 1

        epoch_cost_train[epoch] = train_err_ff / train_batches
        epoch_cost_val[epoch] = val_err / val_batches
        # Then we print the results for this epoch:
        print(&amp;quot;Epoch {} of {} took {:.3f}s&amp;quot;.format(
            epoch + 1, num_epochs, time.time() - start_time))
        print(&amp;quot;  training loss:\t\t{:.6f}&amp;quot;.format(train_err / train_batches))
        print(&amp;quot;  validation loss:\t\t{:.6f}&amp;quot;.format(val_err / val_batches))
        print(&amp;quot;  validation accuracy:\t\t{:.2f} %&amp;quot;.format(
            val_acc / val_batches * 100))

    # After training, we compute and print the test error:
    test_err = 0
    test_acc = 0
    test_batches = 0
    for batch in iterate_minibatches(X_test, y_test, 50, shuffle=False):
        inputs, targets = batch
        err, acc = val_fn(inputs, targets)
        test_err += err
        test_acc += acc
        test_batches += 1
    print(&amp;quot;Final results:&amp;quot;)
    print(&amp;quot;  test loss:\t\t\t{:.6f}&amp;quot;.format(test_err / test_batches))
    print(&amp;quot;  test accuracy:\t\t{:.2f} %&amp;quot;.format(
        test_acc / test_batches * 100))
    return epoch_cost_train, epoch_cost_val

# This function was copied verbatim from the Lasagne tutorial at 
# http://lasagne.readthedocs.org/en/latest/user/tutorial.html
def iterate_minibatches(inputs, targets, batchsize, shuffle=False):
    assert len(inputs) == len(targets)
    if shuffle:
        indices = np.arange(len(inputs))
        np.random.shuffle(indices)
    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):
        if shuffle:
            excerpt = indices[start_idx:start_idx + batchsize]
        else:
            excerpt = slice(start_idx, start_idx + batchsize)
        yield inputs[excerpt], targets[excerpt]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here we run it for 500 epochs without regularization but with dropout on the input and hidden layers.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;epoch_cost_train, epoch_cost_val = lasagne_mlp(X_train, y_train, X_val, y_val, X_test,
 y_test, hidden_units=800, num_epochs=500, l2_param=0, use_dropout=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Epoch 1 of 500 took 0.495s
  training loss:                0.282050
  validation loss:              0.216164
  validation accuracy:          94.50 %
...
Epoch 500 of 500 took 0.504s
  training loss:                0.016550
  validation loss:              0.127085
  validation accuracy:          97.50 %
Final results:
  test loss:                    0.152830
  test accuracy:                96.48 %
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Each epoch generally took less than a second to run. But one time I ran it and noticed it taking forever to run a single epoch - it would grind away and eventually get through one epoch after about 8 minutes. So I thought to myself &amp;ldquo;I wonder if that warning I saw but ignored when Theano got imported was actually important&amp;rdquo;&amp;hellip; Yep. The warning was &amp;ldquo;&lt;em&gt;WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded.&lt;/em&gt;&amp;rdquo; It couldn&amp;rsquo;t find g++ and so it was actually doing everything in Python. Turned out it was because I had upgraded XCode the previous day but the upgrade hadn&amp;rsquo;t completed (because I hadn&amp;rsquo;t opened it and accepted the license agreement). Anyway, the point here is: less than a second versus 8 minutes&amp;hellip; &lt;em&gt;holy shit!&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Epoch 1 of 500 took 509.737s
  training loss:                1.544798
  validation loss:              0.795227
  validation accuracy:          83.20 %
Epoch 2 of 500 took 489.701s
  training loss:                0.730792
  validation loss:              0.512750
  validation accuracy:          86.90 %
Epoch 3 of 500 took 655.276s
  training loss:                0.557232
  validation loss:              0.430175
  validation accuracy:          88.10 %
Epoch 4 of 500 took 496.489s
  training loss:                0.498586
  validation loss:              0.382306
  validation accuracy:          89.30 %
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;the-dangers-of-over-fitting:14c7068255137565e326b1d24878d8a5&#34;&gt;The dangers of over-fitting&lt;/h2&gt;

&lt;p&gt;To explain what over-fitting is, let&amp;rsquo;s imagine an extreme example. Let&amp;rsquo;s say we only have a very small set of training example images with which to train our neural net. Now imagine that it happens to be the case that almost every number 7 example has the stem crossed, like this example:&lt;/p&gt;

&lt;p&gt;
&lt;figure class=&#34;center&#34;&gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/seven_stem.png&#34; alt=&#34;A seven example with a crossed stem&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;p&gt;
        A seven example with a crossed stem
        
            
        
        &lt;/p&gt; 
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;When you train on these examples the model will not generalize well to new examples that do not have this feature. We could say that it would give &lt;em&gt;too much weight&lt;/em&gt; to this crossed stem feature. A great measure against over-fitting is having lots and lots of data to train your model on, because the more data you have the less likely you are to have this type of scenario. But beyond getting more data, there are a couple of other ways to minimize this over-fitting problem. The standard way is to use regularization, where you penalize the weights such that minimizing the cost necessarily means shrinking the weights towards 0. Read more about regularization &lt;a href=&#34;https://en.wikipedia.org/wiki/Regularization_(mathematics&#34;&gt;here&lt;/a&gt;). However, &lt;a href=&#34;http://arxiv.org/abs/1207.0580&#34;&gt;Hinton et al&lt;/a&gt; came up with a solution for neural nets that works by &amp;ldquo;randomly omitting half of the feature detectors on each training case&amp;rdquo;. It&amp;rsquo;s called dropout and it is very effective.&lt;/p&gt;

&lt;p&gt;The graph below shows what happens to our validation error, as compared with the training error if we perform no regularization and do not include dropout layers in our model.&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/500_800_noreg_nodropout.png&#34; alt=&#34;No dropout, no regularization&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;p&gt;
        No dropout, no regularization
        
            
        
        &lt;/p&gt; 
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;And here&amp;rsquo;s what happens with 20% dropout on the input layer and 50% on the hidden layer. Still no regularization.

&lt;figure &gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/500_800_noreg.png&#34; alt=&#34;Dropout layers, no regularization&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;p&gt;
        Dropout layers, no regularization
        
            
        
        &lt;/p&gt; 
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;Accuracy on the test set without dropout was generally around 94%, with dropout was around 97%.&lt;/p&gt;

&lt;p&gt;If we try without dropout but with l2 regularization it looks like this is not as effective at bringing down the validation error.

&lt;figure &gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/500_800_reg01_nodropout.png&#34; alt=&#34;Regularization, no dropout&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;p&gt;
        Regularization, no dropout
        
            
        
        &lt;/p&gt; 
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;The code for generating the above plots, assuming your training and validation results as returned from lasagne_mlp are in the epoch_cost_train and epoch_cost_val variables respectively, is as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-pyton&#34;&gt;plt.style.use(&#39;bmh&#39;)
plt.plot(range(len(epoch_cost_train)), epoch_cost_train, label=&amp;quot;Training error&amp;quot;)
plt.plot(range(len(epoch_cost_val)), epoch_cost_val, label=&amp;quot;Validation error&amp;quot;)
legend()
plt.xlabel(&amp;quot;Num epochs&amp;quot;)
plt.ylabel(&amp;quot;Cost&amp;quot;) 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;3-using-tensorflow:14c7068255137565e326b1d24878d8a5&#34;&gt;3. Using TensorFlow&lt;/h2&gt;

&lt;p&gt;In this last section I achieve little more than proving to myself that I can get enough of a handle on things as to be able to adapt the TensorFlow tutorial to my Coursera data set. Go me.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf

# This function was copied verbatim from the TensorFlow tutorial at
# https://www.tensorflow.org/versions/master/tutorials/index.html
def dense_to_one_hot(labels_dense, num_classes=10):
  &amp;quot;&amp;quot;&amp;quot;Convert class labels from scalars to one-hot vectors.&amp;quot;&amp;quot;&amp;quot;
  num_labels = labels_dense.shape[0]
  index_offset = np.arange(num_labels) * num_classes
  labels_one_hot = np.zeros((num_labels, num_classes))
  labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1
  return labels_one_hot

# Adapted from the TensorFlow tutorial at
# https://www.tensorflow.org/versions/master/tutorials/index.html
class DataSet(object):
  def __init__(self, images, labels):
    assert images.shape[0] == labels.shape[0], (
      &amp;quot;images.shape: %s labels.shape: %s&amp;quot; % (images.shape,
                                             labels.shape))
    self._num_examples = images.shape[0]
    self._images = images
    self._labels = labels
    self._epochs_completed = 0
    self._index_in_epoch = 0
  @property
  def images(self):
    return self._images
  @property
  def labels(self):
    return self._labels
  @property
  def num_examples(self):
    return self._num_examples
  @property
  def epochs_completed(self):
    return self._epochs_completed
  def next_batch(self, batch_size):
    &amp;quot;&amp;quot;&amp;quot;Return the next `batch_size` examples from this data set.&amp;quot;&amp;quot;&amp;quot;
    start = self._index_in_epoch
    self._index_in_epoch += batch_size
    if self._index_in_epoch &amp;gt; self._num_examples:
      # Finished epoch
      self._epochs_completed += 1
      # Shuffle the data
      perm = np.arange(self._num_examples)
      np.random.shuffle(perm)
      self._images = self._images[perm]
      self._labels = self._labels[perm]
      # Start next epoch
      start = 0
      self._index_in_epoch = batch_size
      assert batch_size &amp;lt;= self._num_examples
    end = self._index_in_epoch
    return self._images[start:end], self._labels[start:end]
def read_data_sets(train_images, train_labels, validation_images, validation_labels, test_images, test_labels):
  class DataSets(object):
    pass
  data_sets = DataSets()
  data_sets.train = DataSet(train_images, dense_to_one_hot(train_labels))
  data_sets.validation = DataSet(validation_images, dense_to_one_hot(validation_labels))
  data_sets.test = DataSet(test_images, dense_to_one_hot(test_labels))
  return data_sets

# Adapted from the TensorFlow tutorial at
# https://www.tensorflow.org/versions/master/tutorials/index.html
def tensorFlowBasic(X_train, y_train, X_val, y_val, X_test, y_test):
    sess = tf.InteractiveSession()
    x = tf.placeholder(&amp;quot;float&amp;quot;, shape=[None, 400])
    y_ = tf.placeholder(&amp;quot;float&amp;quot;, shape=[None, 10])
    W = tf.Variable(tf.zeros([400,10]))
    b = tf.Variable(tf.zeros([10]))
    sess.run(tf.initialize_all_variables())
    y = tf.nn.softmax(tf.matmul(x,W) + b)
    cross_entropy = -tf.reduce_sum(y_*tf.log(y))
    train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)
    mydata = read_data_sets(X_train, y_train, X_val, y_val, X_test, y_test)

    for i in range(1000):
      batch = mydata.train.next_batch(50)
      train_step.run(feed_dict={x: batch[0], y_: batch[1]})
    
    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, &amp;quot;float&amp;quot;))
    return accuracy.eval(feed_dict={x: mydata.test.images, y_: mydata.test.labels})

def weight_variable(shape):
  initial = tf.truncated_normal(shape, stddev=0.1)
  return tf.Variable(initial)

def bias_variable(shape):
  initial = tf.constant(0.1, shape=shape)
  return tf.Variable(initial)

def conv2d(x, W):
  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=&#39;SAME&#39;)

def max_pool_2x2(x):
  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],
                        strides=[1, 2, 2, 1], padding=&#39;SAME&#39;)

def tensorFlowCNN(X_train, y_train, X_val, y_val, X_test, y_test, add_second_conv_layer = True):
    x = tf.placeholder(&amp;quot;float&amp;quot;, shape=[None, 400])
    y_ = tf.placeholder(&amp;quot;float&amp;quot;, shape=[None, 10])
    sess = tf.InteractiveSession()
    # First Convolutional Layer
    W_conv1 = weight_variable([5, 5, 1, 32])
    b_conv1 = bias_variable([32])
    x_image = tf.reshape(x, [-1,20,20,1])
    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)
    h_pool1 = max_pool_2x2(h_conv1)
    if add_second_conv_layer:
        # Second Convolutional Layer
        W_conv2 = weight_variable([5, 5, 32, 64])
        b_conv2 = bias_variable([64])
        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)
        h_pool2 = max_pool_2x2(h_conv2)
    
        # Densely Connected Layer
        W_fc1 = weight_variable([5 * 5 * 64, 1024])
        b_fc1 = bias_variable([1024])
        h_pool2_flat = tf.reshape(h_pool2, [-1, 5*5*64])
        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)
    else:
        # Densely Connected Layer
        W_fc1 = weight_variable([10 * 10 * 32, 1024])
        b_fc1 = bias_variable([1024])
        h_pool1_flat = tf.reshape(h_pool1, [-1, 10*10*32])
        h_fc1 = tf.nn.relu(tf.matmul(h_pool1_flat, W_fc1) + b_fc1) 
    
    # Dropout
    keep_prob = tf.placeholder(&amp;quot;float&amp;quot;)
    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)
    # Softmax
    W_fc2 = weight_variable([1024, 10])
    b_fc2 = bias_variable([10])
    y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)

    # Train the model
    mydata = read_data_sets(X_train, y_train, X_val, y_val, X_test, y_test)
    cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))
    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)
    correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, &amp;quot;float&amp;quot;))
    sess.run(tf.initialize_all_variables())
    for i in range(1000):
        batch = mydata.train.next_batch(50)
        if i%100 == 0:
            train_accuracy = accuracy.eval(feed_dict={
                x:batch[0], y_: batch[1], keep_prob: 1.0})
            print(&amp;quot;step %d, training accuracy %g&amp;quot;%(i, train_accuracy))
        train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})

    return accuracy.eval(feed_dict={
        x: mydata.test.images, y_: mydata.test.labels, keep_prob: 1.0})

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There&amp;rsquo;s a tensorFlowBasic function which just has an input layer and an output layer, but the fun stuff happens in tensorFlowCNN, which is my first introduction to convolutional neural nets.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;accuracy = tensorFlowCNN(X_train, y_train, X_val, y_val, X_test, y_test)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;step 0, training accuracy 0.1
step 100, training accuracy 0.82
...
step 900, training accuracy 0.96
accuracy: 0.95200002
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I can also pass a parameter telling it not to add a second convolutional layer:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;accuracy = tensorFlowCNN(X_train, y_train, X_val, y_val, X_test, y_test, add_second_conv_layer=False)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;accuracy: 0.94160002
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With the second layer I generally get around 95% accuracy on the test set, without it around 94%. I need to do a lot more experimenting to get a real handle on how to tweak these layers though. And I need to do a lot more reading to understand convolution and pooling better.&lt;/p&gt;

&lt;h3 id=&#34;further-reading:14c7068255137565e326b1d24878d8a5&#34;&gt;Further reading&lt;/h3&gt;

&lt;p&gt;As mentioned at the start, this exercise has mostly just made me realize how much I have yet to learn about this field. Here&amp;rsquo;s what I have on my reading list:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://timdettmers.com/2015/03/26/convolution-deep-learning/&#34;&gt;Understanding Convolution in Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-core-concepts/&#34;&gt;Deep Learning in a Nutshell: Core Concepts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-history-training/&#34;&gt;Deep Learning in a Nutshell: History and Training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Michael Nielson&amp;rsquo;s &lt;a href=&#34;http://neuralnetworksanddeeplearning.com/&#34;&gt;Neural Networks and Deep Learning&lt;/a&gt; ebook&lt;/li&gt;
&lt;li&gt;Stanford course notes on &lt;a href=&#34;http://cs231n.github.io/&#34;&gt;Convolutional Neural Networks for Visual Recognition&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Some TensorFlow-specic stuff:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://shapeofdata.wordpress.com/2015/11/30/the-tensorflow-perspective-on-neural-networks/&#34;&gt;The TensorFlow perspective on neural networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.kdnuggets.com/2015/11/google-tensorflow-deep-learning-disappoints.html&#34;&gt;TensorFlow Disappoints – Google Deep Learning falls shallow&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Welch&#39;s T-test in Go</title>
      <link>http://katbailey.github.io/post/welch-t-test-golang/</link>
      <pubDate>Thu, 15 Oct 2015 21:04:38 -0700</pubDate>
      
      <guid>http://katbailey.github.io/post/welch-t-test-golang/</guid>
      <description>&lt;p&gt;I wrote some code for doing a Welch&amp;rsquo;s T-Test in Go. You can read up on what a Welch&amp;rsquo;s t-test is &lt;a href=&#34;https://en.wikipedia.org/wiki/Welch%27s_t_test&#34;&gt;here&lt;/a&gt; but in short it&amp;rsquo;s a significance test for the difference between two treatments (like in an A/B test) where the distributions may have unequal variances.&lt;/p&gt;

&lt;pre&gt;

                 * *
               *     *
              *       *
             *         *
            *           *    
           *            **   
          *             ***  
        *               *****
  * * *                 ***********
 -----------------|-----|-----------
 &lt;/pre&gt;

&lt;p&gt;So if you are doing an A/B test and you have the mean and variance of each treatment, you can get a confidence measure for whether the mean of one is truly higher than the mean of the other.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s the code: &lt;a href=&#34;https://github.com/katbailey/welchttest&#34;&gt;https://github.com/katbailey/welchttest&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ohAI</title>
    <link>http://katbailey.github.io/</link>
    <description>Recent content on ohAI</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 23 Jan 2017 17:00:24 -0500</lastBuildDate>
    <atom:link href="http://katbailey.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Was AlphaGo&#39;s Move 37 Inevitable?</title>
      <link>http://katbailey.github.io/post/was-alphagos-move-37-inevitable/</link>
      <pubDate>Mon, 23 Jan 2017 17:00:24 -0500</pubDate>
      
      <guid>http://katbailey.github.io/post/was-alphagos-move-37-inevitable/</guid>
      <description>&lt;p&gt;
&lt;figure class=&#34;center&#34;&gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/go_board.jpg&#34; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;This question is interesting to me both because of the way this particular move was reported on at the time, and because it works as a starting point for me to understand the inner workings of AlphaGo. I&amp;rsquo;m talking, of course, about the AI that beat the human Go champion, Lee Sedol, last March. The famous &amp;ldquo;move 37&amp;rdquo; happened in the second game of the 5-game match, and was described by commentators, once they got over their initial shock, with words like &amp;ldquo;beautiful&amp;rdquo; and &amp;ldquo;creative.&amp;rdquo; It left Sedol utterly flumoxed, to the point where he had to spend 15 minutes contemplating his own next move.&lt;/p&gt;

&lt;p&gt;The question, which can be interpreted in different ways, is about randomness and probability. I&amp;rsquo;ve read the &lt;a href=&#34;http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html&#34;&gt;paper in Nature&lt;/a&gt; where the researchers explain the architecture of the system and how it was trained, and there are several places where there was randomness at play in AlphaGo, in addition to whatever randomness was at play at the deeper levels of training, e.g. stochastic gradient descent:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Board positions from past human games used to train the policy network via supervised learning were randomly selected&lt;/li&gt;
&lt;li&gt;The output of this policy network is a probability distribution over actions that is sampled from during play&lt;/li&gt;
&lt;li&gt;The output of the faster rollout policy network is a probability distribution over actions that is sampled from during play&lt;/li&gt;
&lt;li&gt;Training of the second policy network where Reinforcement Learning (RL) is used and the system plays against previous iterations of itself uses randomly selected previous iterations of the policy network as opponents&lt;/li&gt;
&lt;li&gt;The output of the RL policy network is a probability distribution over actions that is sampled from during training of the value network. This value network outputs a simple scalar prediction of the outcome given a board position.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A naïve interpretation of the question might be as follows: if we train two different instances of AlphaGo, would they each inevitably make move 37, given the state of the board as it was? Here the answer is clearly No, because of the various sources of randomness that are introduced during training. Even if we hold fixed the first point above, so that the two instances are trained on the exact same set of human game data, the move is still not inevitable, due to the other sources of randomness during training. As explained below, the value of move 37 must have been learned during the self-play part of training, but it could have happened that such moves were never explored during self-play, so the high potential value of the move would not have been learned.&lt;/p&gt;

&lt;p&gt;A more interesting version of the question is: given the AlphaGo instance &lt;em&gt;exactly as it had been trained prior to the match with Lee Sedol&lt;/em&gt;, was it inevitable that it would make move 37? This holds fixed all of the randomness introduced during training so we only focus on the randomness during play. My understanding is that the sources of randomness introduced at this stage are the sampling of actions during Monte Carlo Tree Search (MCTS) simulations - both in sampling moves to evaluate and as part of the evaluation of each leaf node in the tree, when the fast rollout policy plays to the end of the game to deliver a prediction about the move.&lt;/p&gt;

&lt;p&gt;Move 37 must have had a very high action value, because it had to overcome a very low probability of being played by a human. The action value comes partly from the value network and partly from the prediction of the fast rollout policy that plays to the end of the game. Since the fast rollout policy is trained on human board positions, the high value of this action must have been discovered during self play, and so it was the value network derived from the RL policy network that assigned the high value. The same value network would do so again. Perhaps the fast rollout policy predicted a positive outcome, in which case an interesting question is whether there were other possible routes the fast rollout policy could have taken that would have resulted in a negative outcome prediction and whether in that case the high value from the value network would have sufficed to overcome that.&lt;/p&gt;

&lt;p&gt;And wasn&amp;rsquo;t there also the chance that this move never would have been sampled during simulation? Although the prior probability assigned by the SL policy network becomes less and less important with the number of visits made to an action, I didn&amp;rsquo;t find anything in the paper that suggests that every possible legal move is evaluated at least once by the value network.&lt;/p&gt;

&lt;p&gt;There was a lot of talk in the aftermath of the match about how surprising move 37 was because it was a move that no human player would ever have made and yet was deemed in hindsight to have been &amp;ldquo;masterful.&amp;rdquo; Is this really so surprising though? The system played millions of games against &lt;em&gt;itself&lt;/em&gt; after having been trained on millions of human games. Given the complexity of Go, with the number of possible board positions far exceeding the number of atoms in the universe, I&amp;rsquo;d have found it more surprising if this self-play had only led to mastery of the types of moves that had already been played by humans.&lt;/p&gt;

&lt;p&gt;At any rate, it seems clear that move 37 in game 2 was not inevitable - it could easily not have been chosen. I leave judgements as to its beauty to those more familiar with the game of Go. As to whether or not it deserves to be called &amp;ldquo;creative,&amp;rdquo; as long as we&amp;rsquo;re ok with reducing the notion of creativity to sampling from probability distributions, I&amp;rsquo;d have to say: Why not? :)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Put away your Machine Learning hammer, criminality is not a nail</title>
      <link>http://katbailey.github.io/post/put-away-your-machine-learning-hammer/</link>
      <pubDate>Tue, 29 Nov 2016 18:13:28 -0500</pubDate>
      
      <guid>http://katbailey.github.io/post/put-away-your-machine-learning-hammer/</guid>
      <description>&lt;p&gt;(&lt;a href=&#34;https://medium.com/@katherinebailey/put-away-your-machine-learning-hammer-criminality-is-not-a-nail-1309c84bb899#.4nrjg8l4n&#34;&gt;This piece was originally posted on Medium&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;Earlier this month, researchers claimed to have found evidence that criminality can be predicted from facial features. In &lt;a href=&#34;https://arxiv.org/pdf/1611.04135v1.pdf&#34;&gt;“Automated Inference on Criminality using Face Images,”&lt;/a&gt; Xiaolin Wu and Xi Zhang describe how they trained classifiers using various machine learning techniques that were able to distinguish photos of criminals from photos of non-criminals with a high level of accuracy.&lt;/p&gt;

&lt;p&gt;The result these researchers found can be interpreted differently depending on what assumptions you bring to interpreting it, and what question you’re interested in answering. The authors simply assume there’s no bias in the criminal justice system, and thus that the criminals they have photos of are a representative sample of the criminals in the wider population (including those who have never been caught or convicted for their crimes). The question they’re interested in is whether there’s a correlation between facial features and criminality. And given their assumption, they take their result as evidence that there is such a correlation.&lt;/p&gt;

&lt;p&gt;But suppose instead you start from the assumption that there isn’t any relationship between facial features and criminality. In place of this question, you are interested in whether there’s bias in the criminal justice system. Then you’ll take Wu and Zhang’s result as evidence that there is such bias — i.e., that the criminal justice system is biased against people with certain facial features, thus explaining the difference between photos of convicted criminals and photos of people from the general population.&lt;/p&gt;

&lt;p&gt;The authors obviously never thought of this possibility.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Unlike a human examiner/judge, a computer vision algorithm or classifier has absolutely no subjective baggages, having no emotions, no biases whatsoever due to past experience, race, religion, political doctrine, gender, age, etc., no mental fatigue, no preconditioning of a bad sleep or meal. The automated inference on criminality eliminates the variable of meta-accuracy (the competence of the human judge/examiner) all together.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So humans are prone to bias, but this machine learning system isn’t? Despite the fact that the creation of the data set on which the system was trained had (biased) humans involved every step of the way, from the arrest to the conviction of each individual in it? The fact that the researchers didn’t notice this gaping hole in their logic is disconcerting to say the least. Even worse is that they seem to suggest that we should deploy a system like this in the real world. To do what exactly, the authors don’t say, but it probably wouldn’t be about targeting the right advertising at today’s discerning criminals. In the case of advertising, a false positive — an innocent person being identified as a criminal — wouldn’t have serious consequences. In more likely scenarios in which the system might be deployed, however, a false positive could have much worse results, e.g., unwarranted scrutiny of people who have done nothing wrong, or even worse, arrests of innocent people.&lt;/p&gt;

&lt;p&gt;Coverage of this paper has &lt;a href=&#34;https://www.technologyreview.com/s/602955/neural-network-learns-to-identify-criminals-by-their-faces/?utm_campaign=socialflow&amp;amp;utm_source=twitter&amp;amp;utm_medium=post&#34;&gt;drawn&lt;/a&gt; &lt;a href=&#34;http://www.telegraph.co.uk/technology/2016/11/24/minority-report-style-ai-learns-predict-people-criminals-facial/&#34;&gt;parallels&lt;/a&gt; with the movie Minority Report, where so-called pre-cogs have prior knowledge of crimes that will be committed in the future. But this comparison misses a crucial point. In the movie, the prediction made by the pre-cogs is always in relation to a particular crime’s being committed by a particular individual at a specified time in the future. And, as the movie suggests, it’s ethically problematic to arrest someone before they’ve actually committed a crime. But in the Wu and Zhang paper it’s even worse because a prediction amounts to nothing more than a statement such as, “this person’s features bear some similarity to the features of a lot of people who have been processed by the criminal justice system.” It says nothing whatsoever about whether this particular person has ever committed a crime.&lt;/p&gt;

&lt;p&gt;
&lt;figure class=&#34;left-align&#34;&gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/lambroso.jpg&#34; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;What is criminality? The state of having committed a crime? The tendency to commit crimes? &lt;a href=&#34;https://en.wikipedia.org/wiki/Cesare_Lombroso&#34;&gt;Cesare Lombroso&lt;/a&gt;, the 19th century Italian criminologist who put forward the theory that born criminals could be identified by congenital defects, coined the term “criminaloid,” to describe a different type of criminal from a born criminal. A criminaloid was someone who just occasionally committed crimes, but did not have the physical features of a born criminal. Presumably this category was necessary in order to account for individuals who had been caught committing crimes but who did not have congenital defects. Even if you buy into such an idea of born criminals, surely there are also some people who have the facial features of criminals, yet who have never committed a crime. Should they be treated as though they have? Wu and Zhang don’t bother asking such questions, and their machine learning algorithms certainly won’t yield answers to them.&lt;/p&gt;

&lt;p&gt;Being proficient in the use of machine learning algorithms such as neural networks, a skill that’s in such incredibly high demand these days, must feel to some people almost god-like — even Thor-like! Maybe every single categorization task will start to look like a nail for Thor’s hammer. Cat pictures and handwritten digits are fair game as “nails.” Criminality is not.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Reframing the &#34;AI Effect&#34;</title>
      <link>http://katbailey.github.io/post/reframing-the-ai-effect/</link>
      <pubDate>Thu, 27 Oct 2016 09:06:30 -0500</pubDate>
      
      <guid>http://katbailey.github.io/post/reframing-the-ai-effect/</guid>
      <description>&lt;p&gt;(&lt;a href=&#34;https://medium.com/@katherinebailey/reframing-the-ai-effect-c445f87ea98b#.fsdszp5fu&#34;&gt;This piece was originally posted on Medium&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;
&lt;figure class=&#34;center&#34;&gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/karpov.jpeg&#34; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;There’s a phenomenon known as the &lt;a href=&#34;https://en.wikipedia.org/wiki/AI_effect&#34;&gt;AI effect&lt;/a&gt;, whereby as soon as Artificial Intelligence (AI) researchers achieve a milestone long thought to signify the achievement of true artificial intelligence, e.g., beating a human at chess, it suddenly gets downgraded to not true AI. Kevin Kelly wrote in a &lt;a href=&#34;https://www.wired.com/2014/10/future-of-artificial-intelligence/&#34;&gt;Wired article in October 2014&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;In the past, we would have said only a superintelligent AI could drive a car, or beat a human at Jeopardy! or chess. But once AI did each of those things, we considered that achievement obviously mechanical and hardly worth the label of true intelligence. Every success in AI redefines it.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Why does this happen? Why did people once think only a superintelligent AI could beat a human at chess? Because they thought the only way that chess could be played really well was how humans play it. And humans use numerous abilities in playing chess. Surely an AI would need all those abilities too. But then along came Deep Blue: a computer that could beat any human chess player in the world, but could do absolutely nothing else. Surely this wasn’t the holy grail we had sought! Nobody would claim this was a superintelligent entity!&lt;/p&gt;

&lt;p&gt;A superintelligent AI is also called a strong AI, an AI with human-level (or greater) intelligence. This is in contrast to weak AIs, which are focused on narrowly defined tasks, like winning chess or Go.&lt;/p&gt;

&lt;p&gt;A better way of thinking about the AI effect is to say that tasks we previously thought would require strong AI turned out to be possible with weak AI.&lt;/p&gt;

&lt;p&gt;Some well-known AI researchers who note the AI effect might say “true AI is whatever hasn’t been done yet.” They distinguish between:&lt;/p&gt;

&lt;p&gt;
&lt;figure class=&#34;center&#34;&gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/ai_distinction_1.png&#34; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;But this is the wrong way to frame the issue, because it makes it appear that all we need is time: if we just keep going, we’ll eventually solve everything and create strong AI. This means that every step we’re taking right now is seen as a step in that direction. I doubt many AI researchers actually believe this. It’s more like a defensive quip in the face of people’s apparent failure to be impressed enough at AI’s successes.&lt;/p&gt;

&lt;p&gt;The useful distinction to make isn’t between what we’ve already solved and what we haven’t, which suggests that time is the only factor that makes a difference. Instead, the useful distinction is between:&lt;/p&gt;

&lt;p&gt;
&lt;figure class=&#34;center&#34;&gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/ai_distinction_2.png&#34; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;If we frame the issue this way, then even if we originally put a problem like “build something that can beat the best human at chess” in the box on the right, because we think it would require strong AI, once we actually solve it with weak AI, we just move it to the box on the left. Note that this doesn’t imply anything at all about our progress towards building a strong AI, whereas the original framing did.&lt;/p&gt;

&lt;p&gt;The lack of a simple progression from weak AI to strong AI has important implications. For starters, it means we can stop worrying about a Superintelligence taking over the world, as I wrote about in &lt;a href=&#34;http://katbailey.github.io/post/why-machine-learning-is-not-a-path-to-the-singularity/&#34;&gt;Why Machine Learning is not a path to the Singularity&lt;/a&gt;. But it also means we should focus our AI efforts, frame our research questions, and operationalize everything we do in this field with a very clear conception of the problems we should be trying to solve and how they can be formulated as weak AI problems. If they can’t, then we shouldn’t waste our time or our budgets on them.&lt;/p&gt;

&lt;p&gt;Unless of course you’re Google, in which case you start a &lt;a href=&#34;https://backchannel.com/google-our-assistant-will-trigger-the-next-era-of-ai-3c72a4d7bc75#.lkppf4545&#34;&gt;multi-year project which may or may not solve Natural Language Understanding&lt;/a&gt;. I will certainly be intrigued to follow the progress on that one!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Why Machine Learning is not a path to the Singularity</title>
      <link>http://katbailey.github.io/post/why-machine-learning-is-not-a-path-to-the-singularity/</link>
      <pubDate>Mon, 24 Oct 2016 10:02:12 -0400</pubDate>
      
      <guid>http://katbailey.github.io/post/why-machine-learning-is-not-a-path-to-the-singularity/</guid>
      <description>&lt;p&gt;(&lt;a href=&#34;https://medium.com/@katherinebailey/why-machine-learning-is-not-a-path-to-the-singularity-540d957ef847#.je6h6x8cm&#34;&gt;This piece was originally posted on Medium&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;All of the advances in Artificial Intelligence that you ever hear about — whether it’s &lt;a href=&#34;http://www.wired.com/2016/03/googles-ai-wins-fifth-final-game-go-genius-lee-sedol/&#34;&gt;machines beating humans at Go&lt;/a&gt;, &lt;a href=&#34;https://research.googleblog.com/2016/09/a-neural-network-for-machine.html&#34;&gt;great advances in Machine Translation&lt;/a&gt;, self-driving cars, or anything else — are examples of weak AI. They are each focused on a single, narrowly defined task. Some would have you believe, even &lt;a href=&#34;http://observer.com/2015/08/stephen-hawking-elon-musk-and-bill-gates-warn-about-artificial-intelligence/&#34;&gt;fear&lt;/a&gt;, that these advances will inevitably lead to strong AI (human-level intelligence), which in turn will lead to a Superintelligence we’ll no longer control. The fear is even that such an entity will subjugate the human race and take over the world! Perhaps there are paths to this sort of Superintelligence, but weak AI — whether using Machine Learning or other techniques — isn’t one of them.&lt;/p&gt;

&lt;p&gt;Among those convinced of the simple progression from weak AI to strong AI are Nick Bostrom, author of &lt;em&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies&#34;&gt;SuperIntelligence: Paths, Dangers, Strategies&lt;/a&gt;&lt;/em&gt;, who recently &lt;a href=&#34;http://www.businessinsider.com/nick-bostrom-deepmind-is-winning-the-ai-race-2016-10&#34;&gt;told Business Insider&lt;/a&gt; that machines will reach human levels of intelligence within the next few decades, and neuroscientist / philosopher Sam Harris. Harris proclaimed in a &lt;a href=&#34;http://www.ted.com/talks/sam_harris_can_we_build_ai_without_losing_control_over_it&#34;&gt;recent TED talk&lt;/a&gt;,&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;as long as we continue to build systems of atoms that display more and more intelligent behavior, we will eventually build general intelligence into our machines.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;He even added that,&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Any progress is enough, we just need to keep going.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The idea of an artificial Superintelligence arising as an inevitable consequence of continued technological progress was popularized (albeit more optimistically) by Ray Kurzweil. In his book, The Singularity is Near, Kurzweil offers page after page of exponential growth curves as evidence for this inevitability.&lt;/p&gt;

&lt;p&gt;
&lt;figure class=&#34;center&#34;&gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/exponential_progress.png&#34; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;One thing that both the pessimistic and optimistic takes on the Singularity have in common is a complete lack of rigor in defining what they’re even talking about. Bostrom, disappointingly but perhaps also wisely, never attempts a definition of intelligence. Harris defines it as information processing and warns us that our continued improvements in information processing will just inevitably result in entities far more intelligent than us.&lt;/p&gt;

&lt;p&gt;Let’s imagine these entities of the future that have arisen as a result of progress in weak AI. Imagine them as creatures, who can do things in the world (otherwise, how could they threaten us?). What are these creatures interested in? Whatever their interests, they must be interests that are better served with increased information processing capabilities. Now let’s flip the progress curve around to go back in time…&lt;/p&gt;

&lt;p&gt;
&lt;figure class=&#34;center&#34;&gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/exponential_progress_reverse.png&#34; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;Traveling back in time, the information processing ability of these creatures, which is what enables them to better serve their own interests than we can serve ours, decreases. So at each point we should have creatures with interests, i.e., with desires, motivations, etc., that they are less well able to serve, since they’ve got less “intelligence”. But we already know that’s not what we’ll find. Imagine we go all the way back to the present! All we’ll find is machines built by humans, with algorithms written by humans, all designed to serve the interests of humans. If our imagined superintelligent entities of the future existed — and remember, they’re supposed to be the descendants of current weak AI systems—then their past would exist right now. But it doesn’t.&lt;/p&gt;

&lt;p&gt;Intelligence isn’t an end in itself; it’s a means to an end. We humans make use of our superior information processing abilities to survive and thrive in the world. This includes creating weak AI systems. If in the future there are other entities that can make similar use of such resources, then it’s the creation of those creatures, not the weak AI systems they’ll make use of, that marks the beginning of progress towards Superintelligence. After all, weak AI systems are essentially tools. They’re great for doing specific things, but they’re never going take over the world by themselves. For that sort of danger, you need a creature or an entity who looks at these tools and says, “I want to take over the world, and maybe I can use these tools to help me do that.”&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Thoughts on data-driven language learning</title>
      <link>http://katbailey.github.io/post/thoughts-on-data-driven-language-learning/</link>
      <pubDate>Mon, 12 Sep 2016 16:57:39 -0400</pubDate>
      
      <guid>http://katbailey.github.io/post/thoughts-on-data-driven-language-learning/</guid>
      <description>&lt;p&gt;I used to be a language pedant. I would bemoan the use of the word &amp;ldquo;presently&amp;rdquo; to mean &amp;ldquo;currently&amp;rdquo;, shudder at &amp;ldquo;between you and I&amp;rdquo;, gasp at the use of &amp;ldquo;literally&amp;rdquo; to mean&amp;hellip; &amp;ldquo;not literally&amp;rdquo; (&amp;ldquo;I literally peed my pants laughing.&amp;rdquo; &amp;ldquo;Orly?&amp;rdquo;) I would get particularly exasperated when I heard people use phrases that were clearly (to me) nonsensical but that &lt;em&gt;sounded&lt;/em&gt; almost correct. A classic example of this is when people say &amp;ldquo;The reason being is&amp;hellip;&amp;rdquo; or start a sentence with &amp;ldquo;As such, &amp;hellip;&amp;rdquo; when the word &amp;ldquo;such&amp;rdquo; does not refer to anything. The &lt;a href=&#34;http://blog.oxforddictionaries.com/2011/09/participles-how-not-to-dangle/&#34;&gt;dangling participle&lt;/a&gt; is another whole class of examples.&lt;/p&gt;

&lt;p&gt;But now I actually find these usage tendencies quite fascinating (ok, I still shudder at &amp;ldquo;between you and I&amp;rdquo;.) They seem to indicate that people&amp;rsquo;s choice of words is based on what they&amp;rsquo;ve heard over and over, rather than on their employment of grammatical rules. And if they misremember what they heard they&amp;rsquo;ll say something that sounds roughly the same even if it&amp;rsquo;s ungrammatical. Or they&amp;rsquo;ll throw phrases into places where they sound like they might belong, even though they don&amp;rsquo;t. Then common mistakes like using &amp;ldquo;as such&amp;rdquo; to mean something similar to &amp;ldquo;therefore&amp;rdquo; proliferate and before you know it &amp;ldquo;literally&amp;rdquo; means &amp;ldquo;not literally&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;A recent &lt;a href=&#34;http://www.scientificamerican.com/article/evidence-rebuts-chomsky-s-theory-of-language-learning/&#34;&gt;article in Scientific American&lt;/a&gt; refutes Chomsky&amp;rsquo;s very influential Universal Grammar theory, which postulates the existence of a sort of toolkit for learning language that is hard-wired into our brains. This module is an instantiation of a grammar that all human languages conform to, according to the theory, and is what enables us to learn language. The Scientific American article introduces an alternative theory referred to simply as a usage-based theory of linguistics, which does away entirely with the need for any sort of hard-wired grammar toolkit in our brains in order to explain our ability to acquire language. Instead, the authors claim, children are born with a set of general purpose tools that allow them to perform tasks like object categorization, reading the intentions of others, and making analogies; and these are the tools used in language learning as well as in the performance of other non-language related tasks.&lt;/p&gt;

&lt;p&gt;One argument they give that I find quite compelling talks about utterances of the form &amp;ldquo;Him a presidential candidate?!&amp;rdquo;, &amp;ldquo;Her go to the ballet?!,&amp;rdquo; &amp;ldquo;Him a doctor?!&amp;rdquo;, etc., and how we can produce infinitely many such utterances even though they are ungrammatical. The Universal Grammar theory has to jump through hoops to explain this, invoking a list of exceptions to grammatical rules in addition to the rules themselves&amp;hellip;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;So the question becomes, are these utterances part of the core grammar or the list of exceptions? If they are not part of a core grammar, then they must be learned individually as separate items. But if children can learn these part-rule, part-exception utterances, then why can they not learn the rest of language the same way? In other words, why do they need universal grammar at all?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;When theories have to introduce all kinds of complexities in order to cope with new evidence, that does seem like the right time to abandon them. At any rate, given my observations above about people shoving words and phrases that sound roughly appropriate into their sentences, without regard for correctness, I was obviously going to like this new usage-based theory :) But I like it for another reason also: it aligns beautifully with data-driven approaches to Natural Language Processing tasks such as machine translation.&lt;/p&gt;

&lt;p&gt;In the 2009 paper &lt;a href=&#34;http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/35179.pdf&#34;&gt;The Unreasonable Effectiveness of Data&lt;/a&gt; by researchers at Google, the authors talk about how tasks where rules-based approaches have traditionally beaten statistical approaches usually succumb eventually when enough data is thrown at them. The more data you have, the less need you have of rules:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Instead of assuming that general patterns are more effective than memorizing specific phrases, today’s translation models introduce general rules only when they improve translation over just memorizing particular phrases (for instance, in rules for dates and numbers).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Memorizing phrases is exactly what I think &lt;em&gt;people&lt;/em&gt; do. Now, I&amp;rsquo;m not suggesting that all of this means that Natural Language Understanding in AI will soon be a solved problem &amp;ndash; there&amp;rsquo;s still the issue of equipping machines with the ability to analogize, read communicative intentions, etc. &amp;ndash; but I do think it means we&amp;rsquo;ve only seen the tip of the iceberg when it comes to what the data-driven approach to NLU can achieve.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m particularly excited by the idea of &lt;em&gt;distributed word representations&lt;/em&gt;, a.k.a. &lt;em&gt;word embeddings&lt;/em&gt;, where you train a set of vectorized representations of words such that the relationships between words is captured by the mathematical relationships that hold between the vectors representing them. This idea, coupled with massive corpora, e.g. all of Wikipedia, leads to the creation of pre-trained sets of embeddings like &lt;a href=&#34;https://code.google.com/archive/p/word2vec&#34;&gt;word2vec&lt;/a&gt; and &lt;a href=&#34;http://nlp.stanford.edu/projects/glove/&#34;&gt;GloVe&lt;/a&gt;, which hold really rich representations of how words are used in a language. These pretrained embeddings can then be used for a wide range of downstream NLP tasks, e.g. &lt;a href=&#34;https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html&#34;&gt;text classification&lt;/a&gt;, &lt;a href=&#34;http://www.manzil.ml/res/Papers/2015_ACL_GLDA.pdf&#34;&gt;topic modeling&lt;/a&gt; and &lt;a href=&#34;http://vene.ro/blog/word-movers-distance-in-python.html&#34;&gt;document similarity&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Granted, these existing sets of embeddings may not capture the &amp;ldquo;not literally&amp;rdquo; sense of &amp;ldquo;literally&amp;rdquo;, but once we add Youtube comments to the training corpora and figure out the mathematics required to make a word vector represent its own opposite, we&amp;rsquo;ll be off to the races! Literally.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gaussian Processes for Dummies</title>
      <link>http://katbailey.github.io/post/gaussian-processes-for-dummies/</link>
      <pubDate>Tue, 09 Aug 2016 09:00:03 -0700</pubDate>
      
      <guid>http://katbailey.github.io/post/gaussian-processes-for-dummies/</guid>
      <description>&lt;p&gt;
&lt;figure class=&#34;center&#34;&gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/kernel_cookbook.png&#34; /&gt;
    
    
&lt;/figure&gt;

&lt;p class=&#34;source&#34;&gt;Source: &lt;a href=&#34;http://www.cs.toronto.edu/~duvenaud/cookbook/index.html&#34;&gt;The Kernel Cookbook by David Duvenaud&lt;/a&gt;&lt;/p&gt;&lt;/p&gt;

&lt;p&gt;It always amazes me how I can hear a statement uttered in the space of a few seconds about some aspect of machine learning that then takes me countless hours to understand. I first heard about Gaussian Processes on an episode of the &lt;a href=&#34;http://www.thetalkingmachines.com/&#34;&gt;Talking Machines&lt;/a&gt; podcast and thought it sounded like a really neat idea. I promptly procured myself a copy of the classic text on the subject, &lt;a href=&#34;http://www.gaussianprocess.org/gpml/&#34;&gt;Gaussian Processes for Machine Learning&lt;/a&gt; by Rasmussen and Williams, but my tenuous grasp on the Bayesian approach to machine learning meant I got stumped pretty quickly. That&amp;rsquo;s when I began the journey I described in my last post, &lt;a href=&#34;http://katbailey.github.io/post/from-both-sides-now-the-math-of-linear-regression/&#34;&gt;From both sides now: the math of linear regression&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Gaussian Processes (GPs) are the natural next step in that journey as they provide an alternative approach to regression problems. This post aims to present the essentials of GPs without going too far down the various rabbit holes into which they can lead you (e.g. understanding how to get the square root of a matrix.)&lt;/p&gt;

&lt;p&gt;Recall that in the simple linear regression setting, we have a dependent variable y that we assume can be modeled as a function of an independent variable x, i.e. $ y = f(x) + \epsilon $ (where $ \epsilon $ is the irreducible error) but we assume further that the function $ f $ defines a linear relationship and so we are trying to find the parameters $ \theta_0 $ and $ \theta_1 $ which define the intercept and slope of the line respectively, i.e. $ y = \theta_0 + \theta_1x + \epsilon $. Bayesian linear regression provides a probabilistic approach to this by finding a distribution over the parameters that gets updated whenever new data points are observed. The GP approach, in contrast, is a &lt;em&gt;non-parametric&lt;/em&gt; approach, in that it finds a distribution over the possible &lt;strong&gt;functions&lt;/strong&gt; $ f(x) $ that are consistent with the observed data. As with all Bayesian methods it begins with a prior distribution and updates this as data points are observed, producing the posterior distribution over functions.&lt;/p&gt;

&lt;p&gt;
&lt;figure class=&#34;center&#34;&gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/all_the_functions.jpg&#34; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;To get an intuition about what this even means, think of the simple OLS line defined by an intercept and slope that does its best to fit your data.&lt;/p&gt;

&lt;p&gt;
&lt;figure class=&#34;center&#34;&gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/bad_least_squares.png&#34; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;The problem is, this line simply isn&amp;rsquo;t adequate to the task, is it? You&amp;rsquo;d really like a curved line: instead of just 2 parameters $ \theta_0 $ and $ \theta_1 $ for the function $ \hat{y} = \theta_0 + \theta_1x$ it looks like a quadratic function would do the trick, i.e. $ \hat{y} = \theta_0 + \theta_1x + \theta_2x^2 $. Now we&amp;rsquo;d need to learn 3 parameters. But what if we don&amp;rsquo;t want to specify upfront how many parameters are involved? We&amp;rsquo;d like to consider every possible function that matches our data, with however many parameters are involved. That&amp;rsquo;s what non-parametric means: it&amp;rsquo;s not that there aren&amp;rsquo;t parameters, it&amp;rsquo;s that there are infinitely many parameters.&lt;/p&gt;

&lt;p&gt;But of course we need a prior before we&amp;rsquo;ve seen any data. What might that look like? Well, we don&amp;rsquo;t really want ALL THE FUNCTIONS, that would be nuts. So let&amp;rsquo;s put some constraints on it. First of all, we&amp;rsquo;re only interested in a specific domain &amp;mdash; let&amp;rsquo;s say our x values only go from -5 to 5. Now we can say that within that domain we&amp;rsquo;d like to sample functions that produce an output whose mean is, say, 0 and that are &lt;em&gt;not too wiggly&lt;/em&gt;. Here&amp;rsquo;s an example of a very wiggly function:&lt;/p&gt;

&lt;p&gt;
&lt;figure class=&#34;center&#34;&gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/wiggly.png&#34; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;And here&amp;rsquo;s a much smoother function:&lt;/p&gt;

&lt;p&gt;
&lt;figure class=&#34;center&#34;&gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/smooth.png&#34; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s a way to specify that smoothness: we use a &lt;strong&gt;covariance matrix&lt;/strong&gt; to ensure that values that are close together in input space will produce output values that are close together. This covariance matrix, along with a mean function to output the expected value of $ f(x) $ defines a Gaussian Process.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s how Kevin Murphy explains it in the excellent textbook &lt;a href=&#34;https://www.amazon.com/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020/&#34;&gt;Machine Learning: A Probabilistic Perspective&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;A GP defines a prior over functions, which can be converted into a posterior over functions once we have seen some data. Although it might seem difficult to represent a distrubtion over a function, it turns out that we only need to be able to define a distribution over the function&amp;rsquo;s values at a finite, but arbitrary, set of points, say &lt;code&gt;\( x_1,\dots,x_N \)&lt;/code&gt;. A GP assumes that &lt;code&gt;\( p(f(x_1),\dots,f(x_N)) \)&lt;/code&gt; is jointly Gaussian, with some mean $ \mu(x) $ and covariance $ \sum(x) $ given by $ \sum_{ij} = k(x_i, x_j) $, where k is a positive definite kernel function. The key idea is that if &lt;code&gt;\( x_i \)&lt;/code&gt; and &lt;code&gt;\( x_j\)&lt;/code&gt; are deemed by the kernel to be similar, then we expect the output of the function at those points to be similar, too.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The mathematical crux of GPs is the multivariate Gaussian distribution.&lt;/p&gt;

&lt;p&gt;
&lt;figure class=&#34;center&#34;&gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/Multivariate_Gaussian.png&#34; /&gt;
    
    
&lt;/figure&gt;

&lt;p class=&#34;source&#34;&gt;Source: &lt;a href=&#34;https://en.wikipedia.org/wiki/Multivariate_normal_distribution&#34;&gt;Wikipedia&lt;/a&gt;&lt;/p&gt;&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s easiest to imagine the bivariate case, pictured here. The shape of the bell is determined by the covariance matrix. If we imagine looking at the bell from above and we see a perfect circle, this means these are two independent normally distributed variables &amp;mdash; their covariance is 0. If we assume a variance of 1 for each of the independent variables, then we get a covariance matrix of $ \Sigma = \begin{bmatrix} 1 &amp;amp; 0\\ 0 &amp;amp; 1 \end{bmatrix} $. The diagonal will simply hold the variance of each variable on its own, in this case both 1&amp;rsquo;s. Anything other than 0 in the top right would be mirrored in the bottom left and would indicate a correlation between the variables. This would give the bell a more oval shape when looking at it from above.&lt;/p&gt;

&lt;p&gt;If we have the joint probability of variables $ x_1 $ and $ x_2 $ as follows:&lt;/p&gt;

&lt;div&gt;$$

\begin{pmatrix}
x_1 \\
x_2
\end{pmatrix}

\sim \mathcal{N}{\left(
\begin{pmatrix}
\mu_1 \\
\mu_2
\end{pmatrix}
,
\begin{pmatrix}
\sigma_{11} &amp; \sigma_{12}\\
\sigma_{21} &amp; \sigma_{22}\\
\end{pmatrix}
\right)}

$$&lt;/div&gt;

&lt;p&gt;it is possible to get the &lt;em&gt;conditional&lt;/em&gt; probability of one of the variables &lt;em&gt;given&lt;/em&gt; the other, and &lt;strong&gt;this is how, in a GP, we can derive the posterior from the prior and our observations&lt;/strong&gt;. It&amp;rsquo;s just that we&amp;rsquo;re not just talking about the joint probability of two variables, as in the bivariate case, but the joint probability of the values of $ f(x) $ for all the $ x $ values we&amp;rsquo;re looking at, e.g. real numbers between -5 and 5.&lt;/p&gt;

&lt;p&gt;So, our posterior is the joint probability of our outcome values, some of which we have observed (denoted collectively by&lt;code&gt;$f$&lt;/code&gt;) and some of which we haven&amp;rsquo;t (denoted collectively by&lt;code&gt;$f_{*}$&lt;/code&gt;):&lt;/p&gt;

&lt;div&gt;$$

\begin{pmatrix}
f \\
f_{*}
\end{pmatrix}

\sim \mathcal{N}{\left(
\begin{pmatrix}
\mu \\
\mu_{*}
\end{pmatrix}
,
\begin{pmatrix}
K &amp; K_{*}\\
K_{*}^T &amp; K_{**}\\
\end{pmatrix}
\right)}

$$&lt;/div&gt;

&lt;p&gt;Here,&lt;code&gt;$K$&lt;/code&gt;is the matrix we get by applying the kernel function to our observed&lt;code&gt;$x$&lt;/code&gt;values, i.e. the similarity of each observed&lt;code&gt;$x$&lt;/code&gt;to each other observed&lt;code&gt;$x$&lt;/code&gt;.&lt;code&gt;$K_{*}$&lt;/code&gt;gets us the similarity of the training values to the test values whose output values we&amp;rsquo;re trying to estimate.&lt;code&gt;$K_{**}$&lt;/code&gt;gives the similarity of the test values to each other.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m well aware that things may be getting hard to follow at this point, so it&amp;rsquo;s worth reiterating what we&amp;rsquo;re actually trying to do here. There are some points&lt;code&gt;$x$&lt;/code&gt;for which we have observed the outcome&lt;code&gt;$f(x)$&lt;/code&gt;(denoted above as simply&lt;code&gt;$f$&lt;/code&gt;). There are some points&lt;code&gt;$x_{*}$&lt;/code&gt;for which we would like to estimate&lt;code&gt;$f(x_{*})$&lt;/code&gt;(denoted above as&lt;code&gt;$f_{*}$&lt;/code&gt;). So we are trying to get the probability distribution&lt;code&gt;$p(f_{*} | x_{*},x,f)$&lt;/code&gt;and we are assuming that &lt;code&gt;$f$&lt;/code&gt;and&lt;code&gt;$f_{*}$&lt;/code&gt;together are jointly Gaussian as defined above.&lt;/p&gt;

&lt;p&gt;About 4 pages of matrix algebra can get us from the joint distribution&lt;code&gt;$p(f, f_{*})$&lt;/code&gt;to the conditional&lt;code&gt;$p(f_{*} | f)$&lt;/code&gt;. I am conveniently going to skip past all that but if you&amp;rsquo;re interested in the gory details then the Kevin Murphy book is your friend. At any rate, what we end up with are the mean,&lt;code&gt;$\mu_{*}$&lt;/code&gt;and covariance matrix&lt;code&gt;$\Sigma_{*}$&lt;/code&gt;that define our distribution &lt;code&gt;$f_{*} \sim \mathcal{N}{\left(\mu_{*}, \Sigma_{*}\right)   }$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Now we can sample from this distribution. Recall that when you have a univariate distribution&lt;code&gt;$x \sim \mathcal{N}{\left(\mu, \sigma^2\right)}$&lt;/code&gt;you can express this in relation to &lt;em&gt;standard normals&lt;/em&gt;, i.e. as&lt;code&gt;$x \sim \mu + \sigma(\mathcal{N}{\left(0, 1\right)}) $&lt;/code&gt;. And generating standard normals is something any decent mathematical programming language can do (incidently, there&amp;rsquo;s a very neat trick involved whereby uniform random variables are projected on to the CDF of a normal distribution, but I digress&amp;hellip;) We need the equivalent way to express our multivariate normal distribution in terms of standard normals:&lt;code&gt;$f_{*} \sim \mu + B\mathcal{N}{(0, I)}$&lt;/code&gt;, where B is the matrix such that&lt;code&gt;$BB^T = \Sigma_{*}$&lt;/code&gt;, i.e. the square root of our covariance matrix. We can use something called a &lt;a href=&#34;https://en.wikipedia.org/wiki/Cholesky_decomposition&#34;&gt;Cholesky decomposition&lt;/a&gt; to find this.&lt;/p&gt;

&lt;p&gt;OK, enough math &amp;mdash; time for some code. The code presented here borrows heavily from two main sources: &lt;a href=&#34;http://www.cs.ubc.ca/~nando/540-2013/lectures.html&#34;&gt;Nando de Freitas&amp;rsquo; UBC Machine Learning lectures&lt;/a&gt; (code for GPs can be found &lt;a href=&#34;http://www.cs.ubc.ca/~nando/540-2013/lectures/gp.py&#34;&gt;here&lt;/a&gt;) and the &lt;a href=&#34;https://github.com/probml/pmtk3&#34;&gt;PMTK3 toolkit&lt;/a&gt;, which is the companion code to Kevin Murphy&amp;rsquo;s textbook &lt;a href=&#34;https://www.amazon.com/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020/&#34;&gt;Machine Learning: A Probabilistic Perspective&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Below we define the points at which our functions will be evaluated, 50 evenly spaced points between -5 and 5. We also define the kernel function which uses the Squared Exponential, a.k.a Gaussian, a.k.a. Radial Basis Function kernel. It calculates the squared distance between points and converts it into a measure of similarity, controlled by a tuning parameter. Note that we are assuming a mean of 0 for our prior.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np
import matplotlib.pyplot as pl

# Test data
n = 50
Xtest = np.linspace(-5, 5, n).reshape(-1,1)

# Define the kernel function
def kernel(a, b, param):
    sqdist = np.sum(a**2,1).reshape(-1,1) + np.sum(b**2,1) - 2*np.dot(a, b.T)
    return np.exp(-.5 * (1/param) * sqdist)

param = 0.1
K_ss = kernel(Xtest, Xtest, param)

# Get cholesky decomposition (square root) of the
# covariance matrix
L = np.linalg.cholesky(K_ss + 1e-15*np.eye(n))
# Sample 3 sets of standard normals for our test points,
# multiply them by the square root of the covariance matrix
f_prior = np.dot(L, np.random.normal(size=(n,3)))

# Now let&#39;s plot the 3 sampled functions.
pl.plot(Xtest, f_prior)
pl.axis([-5, 5, -3, 3])
pl.title(&#39;Three samples from the GP prior&#39;)
pl.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;
&lt;figure class=&#34;center&#34;&gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/output_prior.png&#34; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;Note that the K_ss variable here corresponds to&lt;code&gt;$K_{**}$&lt;/code&gt;in the equation above for the joint probability. It will be used again below, along with&lt;code&gt;$K$&lt;/code&gt;and&lt;code&gt;$K_{*}$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Now we&amp;rsquo;ll observe some data. The actual function generating the&lt;code&gt;$y$&lt;/code&gt;values from our&lt;code&gt;$x$&lt;/code&gt;values, unbeknownst to our model, is the&lt;code&gt;$sin$&lt;/code&gt;function. We generate the output at our 5 training points, do the equivalent of the above-mentioned 4 pages of matrix algebra in a few lines of python code, sample from the posterior and plot it.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Noiseless training data
Xtrain = np.array([-4, -3, -2, -1, 1]).reshape(5,1)
ytrain = np.sin(Xtrain)

# Apply the kernel function to our training points
K = kernel(Xtrain, Xtrain, param)
L = np.linalg.cholesky(K + 0.00005*np.eye(len(Xtrain)))

# Compute the mean at our test points.
K_s = kernel(Xtrain, Xtest, param)
Lk = np.linalg.solve(L, K_s)
mu = np.dot(Lk.T, np.linalg.solve(L, ytrain)).reshape((n,))

# Compute the standard deviation so we can plot it
s2 = np.diag(K_ss) - np.sum(Lk**2, axis=0)
stdv = np.sqrt(s2)
# Draw samples from the posterior at our test points.
L = np.linalg.cholesky(K_ss + 1e-6*np.eye(n) - np.dot(Lk.T, Lk))
f_post = mu.reshape(-1,1) + np.dot(L, np.random.normal(size=(n,3)))

pl.plot(Xtrain, ytrain, &#39;bs&#39;, ms=8)
pl.plot(Xtest, f_post)
pl.gca().fill_between(Xtest.flat, mu-2*stdv, mu+2*stdv, color=&amp;quot;#dddddd&amp;quot;)
pl.plot(Xtest, mu, &#39;r--&#39;, lw=2)
pl.axis([-5, 5, -3, 3])
pl.title(&#39;Three samples from the GP posterior&#39;)
pl.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;
&lt;figure class=&#34;center&#34;&gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/output_posterior.png&#34; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;See how the training points (the blue squares) have &amp;ldquo;reined in&amp;rdquo; the set of possible functions: the ones we have sampled from the posterior all go through those points. The dotted red line shows the mean output and the grey area shows 2 standard deviations from the mean. Note that this is 0 at our training points (because we did not add any noise to our data). Also note how things start to go a bit wild again to the right of our last training point&lt;code&gt;$x = 1$&lt;/code&gt;&amp;mdash; that won&amp;rsquo;t get reined in until we observe some data over there.&lt;/p&gt;

&lt;p&gt;This has been a very basic intro to Gaussian Processes &amp;mdash; it aimed to keep things as simple as possible to illustrate the main idea and hopefully whet the appetite for a more extensive treatment of the topic such as can be found in the &lt;a href=&#34;http://www.gaussianprocess.org/gpml/&#34;&gt;Rasmussen and Williams book&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>From both sides now: the math of linear regression</title>
      <link>http://katbailey.github.io/post/from-both-sides-now-the-math-of-linear-regression/</link>
      <pubDate>Thu, 02 Jun 2016 07:32:46 -0600</pubDate>
      
      <guid>http://katbailey.github.io/post/from-both-sides-now-the-math-of-linear-regression/</guid>
      <description>

&lt;p&gt;Linear regression is the most basic and the most widely used technique in machine learning; yet for all its simplicity, studying it can unlock some of the most important concepts in statistics.&lt;/p&gt;

&lt;p&gt;If you have a basic undestanding of linear regression expressed as $ \hat{Y} = \theta_0 + \theta_1X$, but don&amp;rsquo;t have a background in statistics and find statements like &amp;ldquo;ridge regression is equivalent to the maximum a posteriori (MAP) estimate with a zero-mean Gaussian prior&amp;rdquo; bewildering, then this post is for you. (And yes, it&amp;rsquo;s for me as much as it&amp;rsquo;s for you because I was there not long ago.) With a superficial goal of understanding that somewhat obtuse statement, its main objective is to explore the topic, starting from the standard formulation of linear regression, moving on to the probabilistic approach (maximum likelihood formulation) and from there to Bayesian linear regression.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ll use the $\theta$ character throughout to refer to the coefficients (weights) of a regression model, either explicitly broken out as $\theta_0$ and $\theta_1$ for intercept and slope respectively, or just $\theta$ referring to the vector of coefficients. I&amp;rsquo;ll usually use the expression $\theta^Tx_i$ for the prediction a model gives at $x_i$, the assumption being that a 1 has been added to the vector of values at $x_i$. &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:a3bf4783b942516f86178995501fdab1:matrixfootnote&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:a3bf4783b942516f86178995501fdab1:matrixfootnote&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h1 id=&#34;what-s-in-a-line:a3bf4783b942516f86178995501fdab1&#34;&gt;What&amp;rsquo;s in a line?&lt;/h1&gt;

&lt;p&gt;
&lt;figure class=&#34;center&#34;&gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/least_squares_sm.jpg&#34; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;In the single predictor case, we know that the least squares fit is the line that minimizes the sum of the squared distances between observed data and predicted values, i.e. it minimizes the Residual Sum of Squares (RSS):
&lt;div class=&#34;equation&#34;&gt;
$ \underset{\theta}{\arg\min} \sum_{i=1}^n(y_i - \hat{y_i})^2 $
&lt;/div&gt;
where
&lt;div class=&#34;equation&#34;&gt;
$\hat{y_i} = \theta_0 + \theta_1x_i$
&lt;/div&gt;
is the predicted outcome for the ith observation. We can write the actual value of the ith observation as:
&lt;div class=&#34;equation&#34;&gt;
$y_i = \theta_0 + \theta_1x_i + \epsilon_i$
&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;where $\epsilon_i$ is the residual, or error, we get for the ith observation, i.e. the difference between the predicted value $\hat{y_i}$ and the actual value $y_i$.&lt;/p&gt;

&lt;p&gt;These residuals are pretty important in how we reason about our model. For now we&amp;rsquo;ll just note that by definition they must have an expected value (mean) of 0.&lt;/p&gt;

&lt;p&gt;What else can we say about this line? It defines a particular relationship between the predictor and the outcome. Specifically, the slope of the line is the correlation between outcome values and predictor values, multiplied by the ratio of their standard deviations. So we know how to calculate $\theta_1$ from our data. What about $\theta_0$? Well, we know that the line goes through the point (mean(x), mean(y))&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:a3bf4783b942516f86178995501fdab1:meanxyfootnote&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:a3bf4783b942516f86178995501fdab1:meanxyfootnote&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;, and once we know the slope of a line and a single point it goes through we can find the intercept by working back along the slope from that point: $\theta_0 = \bar{y} - \theta_1\bar{x}$&lt;/p&gt;

&lt;p&gt;In the multiple regression scenario, where we have p predictors, we model the outcome as:&lt;/p&gt;

&lt;p&gt;$ \hat{y_i} = \theta_0 + \theta_1x_1 + \theta_2x_2 + &amp;hellip; + \theta_px_p $&lt;/p&gt;

&lt;p&gt;In this setting it&amp;rsquo;s not quite as straight-forward to derive the coefficients as in the single predictor case. Instead, we need to use calculus and get the partial derivative of our cost function with respect to each parameter and solve for that parameter when setting its derivative to 0.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:a3bf4783b942516f86178995501fdab1:calculusfootnote&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:a3bf4783b942516f86178995501fdab1:calculusfootnote&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h1 id=&#34;from-minimization-to-maximization:a3bf4783b942516f86178995501fdab1&#34;&gt;From minimization to maximization&lt;/h1&gt;

&lt;p&gt;If we ever want to understand linear regression from a Bayesian perspective we need to start thinking probabilistically. We need to flip things over and instead of thinking about the line &lt;strong&gt;minimizing&lt;/strong&gt; a cost, think about it as &lt;strong&gt;maximizing&lt;/strong&gt; the likelihood of the observed data. As we&amp;rsquo;ll see, this amounts to the exact same thing - mathematically speaking - it&amp;rsquo;s just a different way of looking at it.&lt;/p&gt;

&lt;p&gt;To get the likelihood of our data given our assumptions about how it was generated, we must get the probability of each data point y and multiply them together.
&lt;div class=&#34;equation&#34;&gt;&lt;/p&gt;

&lt;p&gt;$ \text{likelihood} = p(y_1|x_1, \theta)*p(y_2|x_2, \theta)&amp;hellip;*p(y_n|x_n, \theta) $&lt;/p&gt;

&lt;p&gt;&lt;/div&gt;
We want to find values of $\theta$ that maximize this result.&lt;/p&gt;

&lt;p&gt;How do we calculate each of these probabilities?&lt;/p&gt;

&lt;h1 id=&#34;know-thy-normal-distribution:a3bf4783b942516f86178995501fdab1&#34;&gt;Know thy Normal distribution&lt;/h1&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/normal_dist.png&#34; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;We noted earlier that the residuals from our least squares fit have a mean of 0. Well we can go one step and say that they are &lt;em&gt;normally distributed&lt;/em&gt; with a mean of 0, i.e.
&lt;div class=&#34;equation&#34;&gt;&lt;/p&gt;

&lt;p&gt;$\epsilon_i \sim \mathcal{N}(0, \sigma^2)$&lt;/p&gt;

&lt;p&gt;&lt;/div&gt;
Here&amp;rsquo;s the function that defines the normal, or Gaussian, distribution:
&lt;div class=&#34;equation&#34;&gt;&lt;/p&gt;

&lt;p&gt;$\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x - \mu)^2}{2\sigma^2} }$&lt;/p&gt;

&lt;p&gt;&lt;/div&gt;
It&amp;rsquo;s not the friendliest looking function, but there&amp;rsquo;s really no way around it: you have to learn this function off because you&amp;rsquo;ll need to be able to recognize it when you see it in equations. A handy fact about the normal distribution is that if $ X \sim \mathcal{N}(\mu, \sigma^2)$ and $Y \sim \mathcal{N}(0, \sigma^2)$ then $ X = \mu + Y$&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:a3bf4783b942516f86178995501fdab1:normfootnote&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:a3bf4783b942516f86178995501fdab1:normfootnote&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Back to calculating the probabilities of our observed y values&amp;hellip; if each $y_i$ is $\theta^Tx_i + \epsilon_i$ and $\epsilon_i \sim \mathcal{N}(0, \sigma^2)$, we can also say that
&lt;div class=&#34;equation&#34;&gt;&lt;/p&gt;

&lt;p&gt;$y_i \sim \mathcal{N}(\theta^Tx_i, \sigma^2)$&lt;/p&gt;

&lt;p&gt;&lt;/div&gt;
See what we did there? We used our handy fact about normally distributed variables, only in reverse. And now we have a way of calculating the probability of each $y_i$ - we just plug it into the normal density function with the right mean and variance. Here&amp;rsquo;s a python function that will calculate the log-likelihood of our y values given the x values and a model as specified by theta0, theta1 and standard deviation.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def log_likelihood(x, y, theta0, theta1, stdev):
    # Get the likelihood of y given the least squares model described
    # by theta0, theta1 and the standard deviation of the error term.
    for i, x_val in enumerate(x):
        mu = theta0 + (theta1*x_val)
        # This is just plugging our observed y value into the normal
        # density function.
        lk = stats.norm(mu, stdev).pdf(y[i])
        res = lk if i == 0 else lk * res
    return Decimal(res).ln()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We often work with the log-likelihood instead of the likelihood simply because it produces quanitities that are easier to work with and compare. And because the log function is a &lt;a href=&#34;https://en.wikipedia.org/wiki/Monotonic_function&#34;&gt;monotonically increasing function&lt;/a&gt;, we know that maximizing the log of a value, with respect to some parameter, coincides with maximizing the value itself.&lt;/p&gt;

&lt;p&gt;We want to maximize the likelihood of our data with respect to our parameters, $\theta$. Here&amp;rsquo;s that likelihood shown as the product of normal densities:
&lt;div class=&#34;equation&#34;&gt;&lt;/p&gt;

&lt;p&gt;$\prod_{i=1}^n \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(y_i - \theta^Tx_i)^2}{2\sigma^2} } $&lt;/p&gt;

&lt;p&gt;&lt;/div&gt;
With a little mathematical shunting (and remembering that $e^xe^y = e^{x + y}$) we can see that this is equivalent to:
&lt;div class=&#34;equation&#34;&gt;&lt;/p&gt;

&lt;p&gt;$\sigma^22\pi^{-\frac{n}{2}} e^{-\frac{1}{2\sigma^2}
\color{red}{\sum_1^n(y_i - \theta^Tx_i)^2}}$&lt;/p&gt;

&lt;p&gt;&lt;/div&gt;
Anything look familiar there? Yay - it&amp;rsquo;s our RSS! Hopefully you can see that minimizing the highlighted part of this expression means maximizing the entire expression. In other words, minimizing the residual sum of squares is equivalent to maximizing the likelihood of the data.&lt;/p&gt;

&lt;p&gt;And to get a visual feel for this likelihood, here&amp;rsquo;s a contour plot:&lt;/p&gt;

&lt;p&gt;
&lt;figure class=&#34;center&#34;&gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/contour_plot_sm.jpg&#34; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;This was produced based on the same data I generated for the linear regression line above. I plugged the Xs and Ys and a series of possible $(\theta_0,\theta_1)$ pairs into the log_likelihood function above to get the log-likelihood of each combination. The original data were generated with an intercept of 4 and a slope of 8 (i.e. y = 4 + 8x + noise.)&lt;/p&gt;

&lt;h1 id=&#34;bayesian-inference:a3bf4783b942516f86178995501fdab1&#34;&gt;Bayesian Inference&lt;/h1&gt;

&lt;p&gt;Now that we&amp;rsquo;re thinking in terms of likelihood of data, we can start to adopt a Bayesian mindset.&lt;/p&gt;

&lt;p&gt;
&lt;figure class=&#34;center&#34;&gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/bayes.jpeg&#34; /&gt;
    
    
&lt;/figure&gt;

&lt;p class=&#34;source&#34;&gt;Source: &lt;a href=&#34;https://en.wikipedia.org/wiki/Bayes%27_theorem#/media/File:Bayes%27_Theorem_MMB_01.jpg&#34;&gt;Wikipedia&lt;/a&gt;&lt;/p&gt;&lt;/p&gt;

&lt;p&gt;Hopefully you have come across Bayes&amp;rsquo; Rule before - it describes the conditional probability of event A &lt;em&gt;given&lt;/em&gt; event B. A note on the bottom line on the right hand side: you may have seen the rule written as
&lt;div class=&#34;equation&#34;&gt;
$ p(A|B) = \dfrac{p(B|A)p(A)}{p(B|A)p(A) + p(B|\neg{A})p(\neg{A})}$
&lt;/div&gt;
But because that bottom line exhausts all possibilities with respect to event A, it boils down to simply p(B). So event A has been &amp;ldquo;marginalized out&amp;rdquo;. (In the discrete case you&amp;rsquo;re talking about probabilities over the possible values of A, which you could tabulate, summing the totals in the margins, hence marginalized.) So this quantity is referred to as the &amp;ldquo;marginal likelihood&amp;rdquo;, and because it isn&amp;rsquo;t dependent on event A, it is often removed from the equation and the rule is expressed as a proportion instead:
&lt;div class=&#34;equation&#34;&gt;
$ p(A|B) \propto p(B|A)p(A)$
&lt;/div&gt;
The way Bayes rule is used for Bayesian inference is that your model is &lt;em&gt;conditioned&lt;/em&gt; on your data:
&lt;div class=&#34;equation&#34;&gt;
$ p(H|D) = \dfrac{p(D|H)p(H)}{p(D)}$
&lt;/div&gt;
where H stands for hypothesis and D stands for data. Sometimes you&amp;rsquo;ll see the word evidence used in place of data. And with this interpretation of the rule, we can now finally talk about priors and posteriors. In the Bayesian setting, the term probability is used as a measure of belief in a hypothesis. You start with a prior belief about that hypothesis, then observe some data (or &amp;ldquo;gather evidence&amp;rdquo;), and then you have a posterior belief.&lt;/p&gt;

&lt;div class=&#34;equation&#34;&gt;
$\text{posterior} = \dfrac{\text{likelihood x prior}}{\text{marginal likelihood}}$
&lt;/div&gt;
or 
&lt;div class=&#34;equation&#34;&gt;
$\text{posterior} \propto \text{likelihood x prior}$
&lt;/div&gt;

&lt;p&gt;One other really important thing to note about Bayesian methods is that they are always explicit about uncertainty. They work with probability distritions, not point estimates. So a hypothesis about the parameters of a linear model would not be e.g. that the intercept has a value of 4, but that it is normally distributed with a mean of 4 and some standard deviation. And this gets updated the more evidence you gather. So your prior is a &lt;em&gt;distribution&lt;/em&gt; and so is your posterior.&lt;/p&gt;

&lt;p&gt;The way I like to think of it is that the data is like a magnet that attracts probability mass.&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/magnet_normal_prior.png&#34; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;On the left you see samples from the prior distribution I gave for the intercept parameter, $\theta_0$: normally distruted with mean 0 and standard deviation 1. I then used &lt;a href=&#34;https://github.com/pymc-devs/pymc&#34;&gt;PyMC&lt;/a&gt;, a Python module for doing &lt;a href=&#34;https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo&#34;&gt;Markov Chain Monte Carlo&lt;/a&gt; sampling, to come up with the posterior distribution on the right. The mean (and the mode) of this posterior distribution is 3.95, close to the true parameter of 4. This is our maximum a posteriori (MAP) estimate for the $\theta_0$ parameter.&lt;/p&gt;

&lt;p&gt;But before we continue on our journey to understand what MAP estimates have to do with ridge regression, we must finish our exploration of regression in the Bayesian setting. It&amp;rsquo;s interesting to see what happens if instead of using a prior that&amp;rsquo;s normally distributed with mean 0 and standard deviation of 1, we give it a uniform prior between -2 and 2.&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/magnet_uniform_prior.png&#34; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;No matter how strong that magnet, i.e. no matter how much data we gather, the posterior will never assign a non-zero probability to any value greater than 2. So obviously it&amp;rsquo;s important to choose a sensible prior.&lt;/p&gt;

&lt;p&gt;Once we have a posterior for each parameter, one way we &lt;em&gt;could&lt;/em&gt; go about making predictions would be to simply &amp;ldquo;plug in&amp;rdquo; the MAP estimates of our parameters:&lt;/p&gt;

&lt;p&gt;$\hat{y^*} = [\text{MAP estimate of }\theta_0] + [\text{MAP estimate of }\theta_1] x^* $&lt;/p&gt;

&lt;p&gt;where $ x^* $  is an unseen data point for which we want to predict the outcome $ y^* $. That is frequentist thinking though. Remember, Bayesians work with distributions, not point estimates, and so no special significance is bestowed upon MAP estimates in the Bayesian setting. The way prediction works instead is that we get a &lt;em&gt;probability distribution&lt;/em&gt; (of course!) for the outcome $ y^* $ at each $ x^* $. We use the posterior distribution of the weights, representing all possible linear models, each of which would produce a different prediction for y. And so the prediction from each possible model is weighted by the probability of that model. Or, more formally:&lt;/p&gt;

&lt;p&gt;$ p(y^* | x^*, X, y) = \int_{\theta}p(y^* | x^*,\theta)p(\theta|X,y)d\theta $&lt;/p&gt;

&lt;p&gt;X and y are given - they are the training data; $ x^* $ is also given - it&amp;rsquo;s the new data point we want to predict the outcome for. From those three givens we want to predict $ y^* $. And we do it by &lt;em&gt;marginalizing&lt;/em&gt; (remember that term?) over the posterior distribution of $\theta$. What we end up with is a Gaussian distribution whose variance depends on the magnitude of the $ x^* $ value.&lt;/p&gt;

&lt;p&gt;The beauty of this is that it retains information about the level of uncertainty around each prediction.&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/uncertainty.png&#34; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;The further away we get from our training data, the greater the margin of error around our predictions.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s always seemed obvious to me that it&amp;rsquo;s better to know that you don&amp;rsquo;t know, than to think you know and act on wrong information. But it surprised me to learn years ago that there are people who prefer to have &amp;ldquo;an answer&amp;rdquo; they can act upon and aren&amp;rsquo;t too concerned about whether it&amp;rsquo;s correct. They may be the same people who will tell me something with great confidence when they have absolutely no idea what they&amp;rsquo;re talking about. Personally, I&amp;rsquo;ve always preferred interacting with the &amp;ldquo;wiser people so full of doubts&amp;rdquo;. I guess that makes me a Bayesian.&lt;/p&gt;

&lt;p&gt;Anyway, back to statistics.&lt;/p&gt;

&lt;h2 id=&#34;map-estimates-and-ridge-regression:a3bf4783b942516f86178995501fdab1&#34;&gt;MAP estimates and Ridge Regression&lt;/h2&gt;

&lt;p&gt;It&amp;rsquo;s finally time to make sense of that statement about the relationship between ridge regression and MAP estimates. Ridge regression is about penalizing large values of parameters in order to prevent over-fitting to your training data. Here&amp;rsquo;s an example from Kevin Murphy&amp;rsquo;s companion code to his textbook on machine learning. We have generated some dummy data where the true function that produces y from x is a second-order polynomial function ($ y = ax + bx^2 $.) But we are trying to fit a 14-degree polynomial to the data.&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/overfitting.png&#34; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;The green line represents predictions for the test set based on the weights learned for the training set. The red line represents the true values of the test set. We can see that the green line fits the training values very well, but it does this by being very &amp;ldquo;loopy&amp;rdquo; - due to some weights having extremely high positive values and others having extreme negative values. This makes it go wildly wrong on some of the test data. If we penalize extreme values, this forces the model to fit the data in a way that&amp;rsquo;s more likely to generalize to unseen data. With ridge regression, we do this by adding the sum of the squared parameters to the cost that we&amp;rsquo;re minimizing:
&lt;div class=&#34;equation&#34;&gt;
$ \underset{\theta}{\arg\min} \sum_1^n(y_i - \hat{y_i})^2 + \lambda \sum_1^p\theta^2 $
&lt;/div&gt;
where $ \lambda $ is a tuning parameter that controls the extent to which the weights are penalized and p is the length of the parameter vector $\theta$, excluding the intercept. For a really great explanation of ridge regression, I highly recommend the Hastie et al book &lt;a href=&#34;http://www-bcf.usc.edu/~gareth/ISL/getbook.html&#34;&gt;An Introduction to Statistical Learning&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Now let&amp;rsquo;s see how we can arrive at this same solution from the Bayesian method. Recall that the posterior distribution for the weights is proportional to the likelihood times the prior. Using the likelihood we figured out before  and a Gaussian prior with a mean of 0 and a variance of $\tau^2$, this becomes:&lt;/p&gt;

&lt;div class=&#34;equation&#34;&gt;

$ \color{blue}{\sigma^22\pi^{-\frac{n}{2}} e^{-\frac{1}{2\sigma^2} {\sum_1^n(y_i - \hat{y_i})^2}}} \color{black} \times \color{green}{\tau^22\pi^{-\frac{p}{2}} e^{-\frac{1}{2\tau^2} \sum_1^p\theta^2}} $

&lt;/div&gt;

&lt;p&gt;with the likelihood in blue and the prior in green. With some slight rearranging we get&lt;/p&gt;

&lt;div class=&#34;equation&#34;&gt;

$ e^{\color{blue}{-\frac{1}{2\sigma^2} \sum_1^n(y_i - \hat{y_i})^2} \color{green}{-\frac{1}{2\tau^2} \sum_1^p\theta^2}} \color{black} \times \color{blue}{\sigma^22\pi^{-\frac{n}{2}}} \color{black} \times \color{green}{\tau^22\pi^{-\frac{p}{2}}}$

&lt;/div&gt;

&lt;p&gt;Given that we are looking to maximize this with respect to the coefficients, we can ignore the terms that don&amp;rsquo;t depend on the coefficients:&lt;/p&gt;

&lt;div class=&#34;equation&#34;&gt;

$ \underset{\theta}{\arg\max} \ \ e^{-\frac{1}{2\sigma^2} \sum_1^n(y_i - \hat{y_i})^2 -\frac{1}{2\tau^2} \sum_1^p\theta^2}$

&lt;/div&gt;

&lt;p&gt;And remember that we like to work with the log-likelihood instead of the likelihood, so now we have&lt;/p&gt;

&lt;div class=&#34;equation&#34;&gt;

$ \underset{\theta}{\arg\max} -\frac{1}{2\sigma^2} \sum_1^n(y_i - \hat{y_i})^2 -\frac{1}{2\tau^2} \sum_1^p\theta^2$

&lt;/div&gt;

&lt;p&gt;Multiplying by $2\sigma^2$ and pulling out the -1 we get:&lt;/p&gt;

&lt;div class=&#34;equation&#34;&gt;

$ \underset{\theta}{\arg\max} -1 (\sum_1^n(y_i - \hat{y_i})^2 + \frac{\sigma^2}{\tau^2} \sum_1^p\theta^2)$

&lt;/div&gt;

&lt;p&gt;And since maximizing $-x$ is equivalent to minimizing $x$, this is equivalent to:&lt;/p&gt;

&lt;div class=&#34;equation&#34;&gt;

$ \underset{\theta}{\arg\min} \sum_1^n(y_i - \hat{y_i})^2 + \frac{\sigma^2}{\tau^2} \sum_1^p\theta^2 $

&lt;/div&gt;

&lt;p&gt;And this is exactly what we have above for a regularized cost function, only with $ \frac{\sigma^2}{\tau^2} $ instead of $ \lambda $.&lt;/p&gt;

&lt;p&gt;So we have shown that ridge regression is indeed equivalent to a maximum a posteriori estimate with a zero-mean Gaussian prior, with $\lambda$ proportional to $\tau^2$. Specifically, a lower variance on the prior for the weights (i.e. a more &lt;em&gt;constrained&lt;/em&gt; prior) is equivalent to a higher $\lambda$ value in the ridge regression solution.&lt;/p&gt;

&lt;p&gt;Q.E.D. :)&lt;/p&gt;

&lt;h2 id=&#34;in-conclusion:a3bf4783b942516f86178995501fdab1&#34;&gt;In conclusion&lt;/h2&gt;

&lt;p&gt;We&amp;rsquo;ve looked at linear regression from both sides now - from the frequentist and Bayesian perspectives, and still somehow&amp;hellip;&lt;/p&gt;

&lt;p&gt;No no no, this isn&amp;rsquo;t going to end like &lt;a href=&#34;https://www.youtube.com/watch?v=8DrRnI-1Ssg&#34;&gt;the song&lt;/a&gt; - I think we know linear regression fairly well after all that. However, there&amp;rsquo;s always more to learn and so I&amp;rsquo;ll leave you with some of the resources I&amp;rsquo;ve found to be most helpful in understanding this stuff.&lt;/p&gt;

&lt;h2 id=&#34;further-reading:a3bf4783b942516f86178995501fdab1&#34;&gt;Further Reading&lt;/h2&gt;

&lt;p&gt;If you don&amp;rsquo;t yet have a copy of &lt;a href=&#34;http://www-bcf.usc.edu/~gareth/ISL/getbook.html&#34;&gt;An Introduction to Statistical Learning&lt;/a&gt; by James, Witten, Hastie and Tibshirani, what are you waiting for? The whole thing is available as a freely downloadable PDF and provides the most crystal clear explanation of linear regression and regularization I&amp;rsquo;ve ever come across. Another freely downloadable book is &lt;a href=&#34;http://www.gaussianprocess.org/gpml/&#34;&gt;Gaussian Processes for Machine Learning&lt;/a&gt; by Rasmussen and Williams. I think this may have been the first place I was introduced to the relationship between MAP estimates and ridge regression. Kevin Murphy&amp;rsquo;s &lt;a href=&#34;https://www.amazon.com/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020&#34;&gt;Machine Learning - a Probabilistic Approach&lt;/a&gt; provides a very complete explanation of this and a whole lot more - it is a pretty enormous tome, and while I do aspire to reading the whole thing&amp;hellip; well, it&amp;rsquo;ll take some time ;)&lt;/p&gt;

&lt;p&gt;And finally, while reading textbooks is totally awesome and worthwhile and in fact my favourite way to get a handle on topics like this, nothing beats playing around with code to get a more complete understanding of the ideas. Cameron Davidson-Pilon&amp;rsquo;s &lt;a href=&#34;https://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/&#34;&gt;Probabilistic Programming &amp;amp; Bayesian Methods for Hackers&lt;/a&gt; is a fantastic introduction to Markov Chain Monte Carlo and the PyMC Python library. And it&amp;rsquo;s also available for free online!&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:a3bf4783b942516f86178995501fdab1:matrixfootnote&#34;&gt;If you&amp;rsquo;re not yet familiar with matrix notation for linear regression, there&amp;rsquo;s a nice explanation &lt;a href=&#34;http://www.stat.purdue.edu/~jennings/stat514/stat512notes/topic3.pdf&#34;&gt;here&lt;/a&gt;.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:a3bf4783b942516f86178995501fdab1:matrixfootnote&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:a3bf4783b942516f86178995501fdab1:meanxyfootnote&#34;&gt;This follows from the fact that the residuals sum to 0
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:a3bf4783b942516f86178995501fdab1:meanxyfootnote&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:a3bf4783b942516f86178995501fdab1:calculusfootnote&#34;&gt;For an intuition about why this is the case, think about the single-parameter case. We can plot the cost, $J(\theta)$, on the y axis, against $\theta$ on the x axis, and we&amp;rsquo;d get a U-shaped curve. We&amp;rsquo;d be looking for the value of $\theta$ at the bottom of the curve. The bottom of the curve is the point at which the slope of the tangent equals 0. And since the derivative of $J(\theta)$ with respect to $\theta$ amounts to the slope of the tangent to the curve, we can set that derivative equal to 0 and solve for $\theta$. In the case of multiple parameters, we do this for each parameter while holding the others fixed (i.e. getting the partial derivatives). See the &lt;a href=&#34;https://en.wikipedia.org/wiki/Linear_least_squares_(mathematics&#34;&gt;Wikipedia entry on linear regression&lt;/a&gt;) for details.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:a3bf4783b942516f86178995501fdab1:calculusfootnote&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:a3bf4783b942516f86178995501fdab1:normfootnote&#34;&gt;This fact is usually expressed in relation to standard normals, where in addition to subtracting the mean you can divide by the standard deviation, however for our purposes we are only interested in &lt;em&gt;shifting&lt;/em&gt; the distribution, not altering its shape.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:a3bf4783b942516f86178995501fdab1:normfootnote&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>AI and Music</title>
      <link>http://katbailey.github.io/post/ai-and-music/</link>
      <pubDate>Fri, 06 May 2016 16:30:28 -0700</pubDate>
      
      <guid>http://katbailey.github.io/post/ai-and-music/</guid>
      <description>&lt;p&gt;&lt;em&gt;I originally wrote the post below in May 2010 as a guest blogger on a now-defunct blog called high-c.com. I&amp;rsquo;m re-posting it here because I recently &lt;a href=&#34;https://twitter.com/katherinebailey/status/728671554683297792&#34;&gt;had cause to dig it up&lt;/a&gt; and was pleasantly surprised at how well it still reflects my views on this topic. The only change I made is the recording of Bach&amp;rsquo;s Chaconne that I link to&amp;hellip;&lt;/em&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;On one stave, for a small instrument, the man writes a whole world of the deepest thoughts and most powerful feelings. If I imagined that I could have created, even conceived the piece, I am quite certain that the excess of excitement and earth-shattering experience would have driven me out of my mind.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is what Johannes Brahms had to say about the Chaconne from Bach&amp;rsquo;s Partita in D minor for solo violin, often referred to simply as Bach&amp;rsquo;s Chaconne. More than anything I can add about the piece, this should be enough to persuade you to take the quarter of an hour required to &lt;a href=&#34;https://www.youtube.com/watch?v=QqA3qQMKueA&#34;&gt;listen to it in its entirety&lt;/a&gt; and give it your full attention. Based on a simple harmonic progression with variation after variation, the Chaconne stretches the limits of what is even possible to play on a violin and is still considered one of the most challenging and certainly one of the most beautiful pieces that can be played on the instrument.&lt;/p&gt;

&lt;p&gt;Patterns - the harmonic progression itself, the passages where the chords are played as rapidly ascending and descending arpeggios, the repititions, both of patterns and of individual notes - abound in this piece, as in any work by Bach. And patterns, of course, are highly programmable; a simplistic example would be calling an arpeggio() function, passing in the tonic and some options like &amp;lsquo;dom7&amp;rsquo;. Well, apparently even musical style is programmable. Or is it? I came across &lt;a href=&#34;https://web.archive.org/web/20100715123354/http://www.unc.edu/~mumukshu/gandhi/gandhi/hofstadter.htm&#34;&gt;an article by Douglas Hofstadter&lt;/a&gt; discussing the research of David Cope, whose Experiments in Musical Intelligence (EMI) program has produced works in the style of Bach, Mozart and Chopin that can easily be mistaken as genuine. You can listen to them here. Now, the Bach example hardly compares with the Chaconne, but is this not just a question of increasing the sophistication of the program? What EMI does is produce music based on analysis of the entire existing works of a given composer. It takes bits from these works and recombines them according to various rules. So in fact the &amp;lsquo;style&amp;rsquo; has been programmed in as declarative data, not procedural information. In any case, with enough refinement of the rules, will it not one day produce something comparable to the Chaconne?&lt;/p&gt;

&lt;p&gt;The question as to whether computer programs will ever produce really beautiful and original music is a hotly debated one: many people are very emotionally attached to the idea that music is a fundamental part of what it means to be human. Personally I don&amp;rsquo;t see any reason why it wouldn&amp;rsquo;t be possible at least in principle for a machine to produce music that I might enjoy as much as I enjoy Bach, but I think such an achievement will be in virtue of its having learned to do a whole lot more besides, including understanding a wide range of human emotions.&lt;/p&gt;

&lt;p&gt;In order for me to really enjoy a piece of music, one necessary but insufficient condition is that I must feel some sort of connection with it. The connection could be anything from a feeling that the composer shared my deepest beliefs and longings, to a fairly vague notion that the composer was an intelligent being with a worldview somewhat analogous to my own. In the case of Bach, it is indeed a vague connection - what, after all, could I really have in common with a deeply religious man from the early 18th century? And yet, every time I hear his Chaconne I feel I&amp;rsquo;m learning something new. It&amp;rsquo;s this connection that makes a work of art different from, say, a beautiful sunset: even if Bach was actually expressing something about the wondrousness of God, something which I as an atheist can&amp;rsquo;t exactly grasp, he was expressing a human sentiment and I am understanding something of that human sentiment; the sunset isn&amp;rsquo;t expressing anything, it is merely beautiful. (I&amp;rsquo;ll reiterate that I don&amp;rsquo;t mean to exclude the possibility of machines expressing and understanding human sentiments, it&amp;rsquo;s just that currently the only beings we know to have these capacities are human beings).&lt;/p&gt;

&lt;p&gt;In his article, Hofstadter describes lectures he gave on EMI during which he played recordings of pieces by Bach and Chopin mixed in with pieces created by EMI in the style of those composers. Many were fooled by the EMI pieces, especially the Chopin. If I had been one of those fooled, then any value I had previously seen in the piece would simply disappear for me. I would just have to accept that I had mistakenly believed there was something there of value, but in fact there was nothing at all, just some pretty notes. I will never feel I can learn something new from a piece of music written by a machine without a basic shared understanding of the world.&lt;/p&gt;

&lt;p&gt;EMI might be able to produce 1000 different Chaconnes based on Bach&amp;rsquo;s entire works, but given the choice, I&amp;rsquo;d rather listen to the original 1000 times over. As for the intelligent, human-like machine that might one day exist, it&amp;rsquo;s as unlikely to ever produce anything Bach-like as a human composer is today: it might produce something comparable in depth and power, but the lack of understanding will most likely be on my part this time.&lt;/p&gt;

&lt;p&gt;&lt;div class=&#34;embed video-player&#34;&gt;
&lt;iframe class=&#34;youtube-player&#34; type=&#34;text/html&#34; width=&#34;640&#34; height=&#34;385&#34; src=&#34;http://www.youtube.com/embed/QqA3qQMKueA&#34; allowfullscreen frameborder=&#34;0&#34;&gt;
&lt;/iframe&gt;
&lt;/div&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tay and the Dangers of Artificial Stupidity</title>
      <link>http://katbailey.github.io/post/tay-and-the-danger-of-artificial-stupidity/</link>
      <pubDate>Tue, 12 Apr 2016 07:13:39 -0700</pubDate>
      
      <guid>http://katbailey.github.io/post/tay-and-the-danger-of-artificial-stupidity/</guid>
      <description>

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/sexist_tay.png&#34; /&gt;
    
    
&lt;/figure&gt;

This is an imagined conversation between &lt;a href=&#34;https://www.tay.ai&#34;&gt;Tay&lt;/a&gt;, Microsoft&amp;rsquo;s AI chatbot, and me. Tay was let loose on Twitter a couple of weeks ago to pretty &lt;a href=&#34;https://www.washingtonpost.com/news/the-intersect/wp/2016/03/24/the-internet-turned-tay-microsofts-fun-millennial-ai-bot-into-a-genocidal-maniac/&#34;&gt;disastrous effect&lt;/a&gt;. It was trained by a bunch of racists to say racist things. It could just as easily have been trained by a bunch of sexists to say sexist things, hence my imagined conversation above. The conversation is completely unrealistic though - I would never take career advice from an AI!&lt;/p&gt;

&lt;p&gt;AIs are still pretty stupid and can&amp;rsquo;t answer questions like the one above. At their core, they are machine learning algorithms (possibly multiple algorithms feeding into each other). Machine learning involves training a machine to learn from data and make predictions about new data. It&amp;rsquo;s about math, statistics and lots and lots of data.&lt;/p&gt;

&lt;h2 id=&#34;the-question-tay-is-really-answering:20a4829f9cb79389360a21a53910b571&#34;&gt;The question Tay is really answering&lt;/h2&gt;

&lt;p&gt;In the case of an AI that has been trained using data from real human conversations, be they from Twitter, YouTube or what have you, the question it is actually answering when I ask it &amp;ldquo;As a woman, should I work in technology?&amp;rdquo; is more like &amp;ldquo;&lt;strong&gt;What is the most common response to the question&lt;/strong&gt; &lt;em&gt;As a woman, should I work in technology?&lt;/em&gt;&amp;rdquo; And if all the responses it has ever seen are from sexist jerks, then this is the correct answer - way to go Tay! :D&lt;/p&gt;

&lt;p&gt;Very often there won&amp;rsquo;t be a difference between the answers to these different questions and that&amp;rsquo;s when the AI will seem smart.&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/questions.png&#34; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;This question-answering behavior is in contrast to AI systems like Apple&amp;rsquo;s Siri, which use machine learning only to understand the question but not to generate the response. To answer questions, Siri consults the &lt;a href=&#34;https://en.wikipedia.org/wiki/Wolfram_Alpha&#34;&gt;Wolfram Alpha&lt;/a&gt; knowledge engine.&lt;/p&gt;

&lt;h2 id=&#34;machine-learning-always-involves-uncertainty:20a4829f9cb79389360a21a53910b571&#34;&gt;Machine Learning always involves uncertainty&lt;/h2&gt;

&lt;p&gt;I can&amp;rsquo;t actually pose my question to Tay as the chatbot is &lt;a href=&#34;https://twitter.com/tayandyou&#34;&gt;taking a break&lt;/a&gt; but when I asked Siri about whether women make good software engineers, I had to re-phrase the question several times before it would even attempt a response, and its response in the end was to google the question. In a scenario like this, the level of uncertainty is clear to the user - uncertainty about the question being asked, and uncertainty in the answer, which is a list of Google search results that may or may not be what you are looking for. (As an aside, &lt;a href=&#34;http://www.slate.com/articles/technology/cover_story/2016/04/alexa_cortana_and_siri_aren_t_novelties_anymore_they_re_our_terrifyingly.html?wpsrc=sh_all_dt_tw_top&#34;&gt;this Slate article&lt;/a&gt; raises some interesting questions about the trust issues involved when an AI answers a question by quoting verbatim from a single source like Wikipedia without saying this is where it got the answer from.)&lt;/p&gt;

&lt;p&gt;Siri is probably no less sophisticated an AI system than Tay - it&amp;rsquo;s just more upfront about the level of uncertainty involved when you interact with it. Both systems are examples of &lt;strong&gt;weak AI&lt;/strong&gt;, which is also called &lt;em&gt;narrow&lt;/em&gt; AI, because it&amp;rsquo;s the type of AI that is focused on one very specific and narrow task. &lt;strong&gt;Strong AI&lt;/strong&gt;, on the other hand, also called &lt;strong&gt;Artificial General Intelligence&lt;/strong&gt;, is about human-like intelligence, and there is absolutely no reason to believe that all the sophisticated weak AIs in the world are a basis for producing strong AI.&lt;/p&gt;

&lt;h2 id=&#34;weak-ai-and-the-objective-function:20a4829f9cb79389360a21a53910b571&#34;&gt;Weak AI and the Objective Function&lt;/h2&gt;

&lt;p&gt;The difference between weak AI and strong AI is pretty important, especially in order to understand what is bemoaned as the &amp;ldquo;moving the goal posts&amp;rdquo; problem in the perception of AI. Kevin Kelly wrote in a &lt;a href=&#34;http://www.wired.com/2014/10/future-of-artificial-intelligence/&#34;&gt;Wired article in October 2014&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;In the past, we would have said only a superintelligent AI could drive a car, or beat a human at Jeopardy! or chess. But once AI did each of those things, we considered that achievement obviously mechanical and hardly worth the label of true intelligence. Every success in AI redefines it.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;That&amp;rsquo;s not really true. I think the correct way to think about it is that there was a prior belief that these problems could only be solved with strong AI and it turned out they could be solved with weak AI. These are pretty &lt;a href=&#34;https://en.wikipedia.org/wiki/Weak_AI&#34;&gt;well-defined&lt;/a&gt; &lt;a href=&#34;https://en.wikipedia.org/wiki/Artificial_general_intelligence&#34;&gt;terms&lt;/a&gt; and they have not needed any redefinition in light of new developments like &lt;a href=&#34;http://www.theatlantic.com/technology/archive/2016/03/the-invisible-opponent/475611/&#34;&gt;AlphaGo&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;With weak AI, the system has what is called an &lt;strong&gt;objective function&lt;/strong&gt; aka a &lt;strong&gt;loss&lt;/strong&gt; or &lt;strong&gt;error&lt;/strong&gt; function. Training the AI is about minimizing the error between its output and the &amp;ldquo;correct&amp;rdquo; answer.&lt;/p&gt;

&lt;p&gt;While there are some &lt;a href=&#34;http://www.somatic.io/blog/on-alphago-intuition-and-the-master-objective-function&#34;&gt;interesting ideas&lt;/a&gt; out there about what objective functions might look like in the future, for now it is this fairly simple idea of measuring performance against a single specific error metric. Even in a complex AI system that combines multiple machine learning algorithms together, each one is just working to minimize its own narrowly-defined error function.&lt;/p&gt;

&lt;p&gt;With all that said, this is still really really impressive stuff!&lt;/p&gt;

&lt;h2 id=&#34;objective-function-goggles:20a4829f9cb79389360a21a53910b571&#34;&gt;Objective Function Goggles&lt;/h2&gt;

&lt;p&gt;To examine Tay with an objective eye, we need to put on our Objective Function Goggles (OFGs). The idea for these goggles comes from a professor I once had for symbolic logic, who encouraged us to don &amp;ldquo;Boolean Goggles&amp;rdquo; when assessing the validity of an argument.&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/goggles.png&#34; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;The reason for the goggles is so that if the premises in the argument were something less palatable (e.g. &amp;ldquo;if it&amp;rsquo;s raining then the grass is blue&amp;rdquo;) we would not get tripped up by their meaning and just focus on whether the logic is valid.&lt;/p&gt;

&lt;p&gt;Palatability (of a different sort) is definitely lacking when it comes to Tay&amp;rsquo;s tweets (you can read a selection of them &lt;a href=&#34;https://www.theguardian.com/technology/2016/mar/24/tay-microsofts-ai-chatbot-gets-a-crash-course-in-racism-from-twitter&#34;&gt;here&lt;/a&gt;), so we&amp;rsquo;ll don our OFGs to obscure their meaning and just answer the following questions about them:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Does Tay compose syntactically correct tweets?&lt;/li&gt;
&lt;li&gt;Does Tay compose tweets that sound like the tweets of an 18-24yr old (the target demographic for the bot)?&lt;/li&gt;
&lt;li&gt;Does Tay respond to tweets in a way that sounds like it has understood the tweet it has responded to?&lt;/li&gt;
&lt;li&gt;Does Tay engage in conversation with interlocutors in a way that suggests it has understood the values of those interlocutors?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/OFGs.png&#34; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s reasonable to answer Yes to all of these questions. Presumably there are separate pieces to Tay - perhaps some sentiment analysis and other Natural Language Processing (NLP) techniques are applied on the tweets it receives. Once it has processed those, it needs to generate responses using some probabilistic model. At each step it is obviously performing well.&lt;/p&gt;

&lt;p&gt;From a pure objective function standpoint, we&amp;rsquo;d have to give Tay a pretty positive assessment.&lt;/p&gt;

&lt;p&gt;Of course, eventually we have to take off the goggles. I really tried to think of a word beginning with M that would make sense in between &amp;ldquo;Objective&amp;rdquo; and &amp;ldquo;Function&amp;rdquo; because then we&amp;rsquo;d have &lt;strong&gt;OMFG&lt;/strong&gt;s - an apt reaction upon removing them.&lt;/p&gt;

&lt;p&gt;Tay may be an impressively smart piece of Machine Learning, but it makes for a very dumb human. It is an Artificial Stupidity (or &lt;strong&gt;A&lt;/strong&gt;rtificially &lt;strong&gt;S&lt;/strong&gt;tupid &lt;strong&gt;S&lt;/strong&gt;ystem?)&lt;/p&gt;

&lt;h2 id=&#34;should-we-be-worried-about-weak-ai:20a4829f9cb79389360a21a53910b571&#34;&gt;Should we be worried about weak AI?&lt;/h2&gt;

&lt;p&gt;George Dvorsky, transhumanist and Chair of the Board for the Institute for Ethics and Emerging Technologies, has &lt;a href=&#34;http://io9.gizmodo.com/how-much-longer-before-our-first-ai-catastrophe-464043243&#34;&gt;written about the dangers of weak AI&lt;/a&gt;, but focuses mostly on what can happen when artificial intelligence &amp;ldquo;gets it wrong&amp;rdquo;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;because expert systems like Watson will soon be able to conjure answers to questions that are beyond our comprehension, we won’t always know when they’re wrong.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;But if a question (or the answer to it) is beyond our comprehension, then &lt;em&gt;we should not be acting on the answer&lt;/em&gt;, or allowing a machine to. That&amp;rsquo;s not AI gone wrong, it&amp;rsquo;s human common sense gone out the window.&lt;/p&gt;

&lt;p&gt;Responsible use of a machine learning algorithm requires an understanding of the way it produces its output and the level of uncertainty inherent in that output.&lt;/p&gt;

&lt;p&gt;Dvorsky talks about critical decisions now being in the hands of machines, but really the critical decision is whether to deploy that machine for a particular task in the first place. And that decision is always squarely in the hands of humans.&lt;/p&gt;

&lt;h2 id=&#34;imputing-intelligence-to-machines-and-responding-emotionally:20a4829f9cb79389360a21a53910b571&#34;&gt;Imputing intelligence to machines and responding emotionally&lt;/h2&gt;

&lt;p&gt;The other potential source of trouble when it comes to weak AI has also more to do with human behavior than the behavior of the AIs. It has to do with how we naturally respond to AIs.&lt;/p&gt;

&lt;p&gt;Leaving aside AIs for a moment, meet Tubbs the cat from popular Japanese game &lt;a href=&#34;https://en.wikipedia.org/wiki/Neko_Atsume&#34;&gt;Neko Atsume&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;
&lt;figure class=&#34;half-size center&#34;&gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/tubbs.jpg&#34; alt=&#34;This is the only thing you&amp;#39;ll ever see Tubbs doing&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;p&gt;
        This is the only thing you&amp;#39;ll ever see Tubbs doing
        
            
        
        &lt;/p&gt; 
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;Apparently people have &lt;a href=&#34;http://www.buzzfeed.com/natalyalobanova/protect-tubbs-at-all-costs&#34;&gt;quite strong feelings&lt;/a&gt; about Tubbs, and he&amp;rsquo;s just a  bunch of pixels, not even animated! (Yes, I totally did just make up a reason to include a picture of Tubbs in my blog post.)&lt;/p&gt;

&lt;p&gt;So it doesn&amp;rsquo;t take much for us to respond emotionally to made-up characters, even without imputing any intelligence to them.&lt;/p&gt;

&lt;p&gt;Imputing intelligence to machines is something we do very naturally when we see them perform tasks like winning a game of Go or chess, or responding to tweets.&lt;/p&gt;

&lt;p&gt;We take&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Machines can do X&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;to mean&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Machines can do all the things I do when I am doing X&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is encouraged by the way AI successes are reported in the media. Here&amp;rsquo;s a relatively subtle example from &lt;a href=&#34;http://www.businessinsider.com/ai-picks-most-creative-paintings-of-their-time-2015-6&#34;&gt;Business Insider&amp;rsquo;s coverage of a recent paper&lt;/a&gt; on &lt;em&gt;Quantifying Creativity in Art Networks&lt;/em&gt;:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Creativity and art are usually thought of as the domains of humans.&lt;/p&gt;

&lt;p&gt;But computer scientists from Rutgers University have designed an algorithm that shows that computers may be just as skilled at critiquing artwork.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now, the researchers themselves &lt;a href=&#34;http://arxiv.org/abs/1506.00711&#34;&gt;describe what their algorithm does&lt;/a&gt; as &amp;ldquo;constructing a network between creative products and using this network to infer about the originality and influence of its nodes.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;It analyzes the differing styles across all these paintings and can pinpoint the originators of styles that influenced later paintings. This certainly sounds to me like an incredibly sophisticated and awesome use of machine learning! But it is definitely &lt;em&gt;not&lt;/em&gt; about an algorithm that is &amp;ldquo;critiquing artwork.&amp;rdquo; Shown a new painting by an up and coming artist, the algorithm would have absolutely nothing to say about it.&lt;/p&gt;

&lt;p&gt;OK, so as I said the BI quote is a pretty subtle example, but you also get &lt;a href=&#34;http://listverse.com/2016/04/02/10-remarkable-but-scary-developments-in-artificial-intelligence/&#34;&gt;silly articles like this&lt;/a&gt;, which totally mischaracterize the state of AI with headings like &amp;ldquo;They’re Learning To Deceive And Cheat&amp;rdquo;, &amp;ldquo;They’re Starting To Understand Our Behavior&amp;rdquo;, &lt;strong&gt;&amp;ldquo;They’re Starting To Feel Emotions&amp;rdquo;&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;This last one is a reference to Xiaoice, a precursor to Tay that Microsoft has been running in China seemingly with some success (see &lt;a href=&#34;https://blogs.microsoft.com/blog/2016/03/25/learning-tays-introduction/#sm.00014j43dh10waeonwtog322jtey0&#34;&gt;Microsoft&amp;rsquo;s apology for Tay&lt;/a&gt; which mentions Xiaoice.)&lt;/p&gt;

&lt;p&gt;So if you&amp;rsquo;re being told that the likes of Xiaoice and Tay can feel emotions, the strongest of which in the case of Tay is hatred, and you tend to have emotional responses even to things you don&amp;rsquo;t believe are sentient, it&amp;rsquo;s not hard to imagine things getting out of hand.&lt;/p&gt;

&lt;p&gt;With AIs like Tay it&amp;rsquo;s particularly hard to avoid seeing them as &amp;ldquo;people&amp;rdquo; in some sense, with a whole set of beliefs, values and emotions, because they are deliberately being presented to us as people.&lt;/p&gt;

&lt;p&gt;But to the extent that Tay has beliefs at all, they are beliefs in the sense of Bayesian probability, e.g. a belief that the sentiment of the tweet it&amp;rsquo;s responding to is positive or negative, or a belief that the most common response to such a tweet is the one it is about to make.&lt;/p&gt;

&lt;p&gt;Really, Tay should not have been deployed on Twitter - that was kind of a dumb mistake on Microsoft&amp;rsquo;s part, as &lt;a href=&#34;https://medium.com/@anthonygarvan/hey-microsoft-the-internet-made-my-bot-racist-too-d897fa847232#.n2e5507w5&#34;&gt;this lesson had been learned many times before&lt;/a&gt;. The very things that made it impressive as a piece of machine learning were what led to its spiralling out of control in the spectacular way it did.&lt;/p&gt;

&lt;p&gt;There will no doubt be more Tays, and there will be more confused reporting about AI successes. All we can do is arm ourselves with a better understanding of what AI is about. I highly recommend reading this &lt;a href=&#34;https://www.oreilly.com/ideas/ais-dueling-definitions&#34;&gt;excellent post by Beau Cronin&lt;/a&gt; from a couple of years ago on the different definitions of AI.&lt;/p&gt;

&lt;p&gt;And I&amp;rsquo;ll leave you with a simple trick for assessing whether something is an example of strong AI&amp;hellip;&lt;/p&gt;

&lt;h2 id=&#34;binary-classifier-for-strong-ai:20a4829f9cb79389360a21a53910b571&#34;&gt;Binary classifier for strong AI&lt;/h2&gt;

&lt;p&gt;A binary classifier is an algorithm that can distinguish members of a class from non-members - its output is a simple &lt;code&gt;true&lt;/code&gt; or &lt;code&gt;false&lt;/code&gt; for each input. Various machine learning techniques can be used to train binary classifiers, including neural networks.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s a binary classifier that tells you whether &lt;code&gt;x&lt;/code&gt; (the latest amazing piece of AI technology) is an example of strong AI and thus a harbinger of the Singularity. It has been trained on a highly sophisticated convolutional neural network&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:20a4829f9cb79389360a21a53910b571:cnnfootnote&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:20a4829f9cb79389360a21a53910b571:cnnfootnote&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; with hundreds of layers and thousands of nodes. &lt;strong&gt;100% accuracy guaranteed!&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def isStrongAI(x):
  return False
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I promise, by the time the Singularity comes about, the Python programming language won&amp;rsquo;t even be a thing anymore ;)&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:20a4829f9cb79389360a21a53910b571:cnnfootnote&#34;&gt;Tongue firmly in cheek
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:20a4829f9cb79389360a21a53910b571:cnnfootnote&#34;&gt;&lt;sup&gt;[return]&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Matrix Factorization with Tensorflow</title>
      <link>http://katbailey.github.io/post/matrix-factorization-with-tensorflow/</link>
      <pubDate>Fri, 11 Mar 2016 16:50:34 -0500</pubDate>
      
      <guid>http://katbailey.github.io/post/matrix-factorization-with-tensorflow/</guid>
      <description>

&lt;p&gt;I&amp;rsquo;ve been working on building a content recommender in TensorFlow using matrix factorization, following the approach described in the article &lt;a href=&#34;https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf&#34;&gt;Matrix Factorization Techniques for Recommender Systems&lt;/a&gt; (MFTRS). I haven&amp;rsquo;t come across any discussion of this particular use case in TensorFlow but it seems like an ideal job for it. I&amp;rsquo;ll explain briefly here what matrix factorization is in the context of recommender systems (although I highly &lt;em&gt;cough&lt;/em&gt; recommend reading the MFTRS article) and how things needed to be set up to do this in TensorFlow. Then I&amp;rsquo;ll show the code I wrote to train the model and the resulting TensorFlow computation graph produced by TensorBoard.&lt;/p&gt;

&lt;h2 id=&#34;the-basics:eb1147f7685f7b4fdbf640a27bbc705e&#34;&gt;The basics&lt;/h2&gt;

&lt;p&gt;A matrix of ratings with users as rows and items as columns might look something like this:&lt;/p&gt;

&lt;p&gt;
&lt;figure class=&#34;matrices center&#34;&gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/matrix.png&#34; alt=&#34;A matrix of user/item ratings&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;p&gt;
        A matrix of user/item ratings
        
            
        
        &lt;/p&gt; 
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;We see for example that user 1 has given item 2 a rating of 3.&lt;/p&gt;

&lt;p&gt;What matrix factorization does is to come up with two smaller matrices, one representing users and one representing items, which when multiplied together will produce roughly this matrix of ratings, ignoring the 0 entries.&lt;/p&gt;

&lt;p&gt;
&lt;figure class=&#34;center&#34;&gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/matrix_factorization.png&#34; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;So if our original matrix is m x n, where m is the number of users and n is the number of items, we need an m x d matrix and a d x n matrix as our factors, where d is chosen to be small enough for the computation to be efficient and large enough to represent the number of dimensions along which interactions between users and items are likely to vary in some significant way. In the above illustration we have d = 2. The predicted rating by a given user for a given item is then the dot product of the vector representing the user and the vector representing the item. In the MFTRS article, this is expressed as:&lt;/p&gt;

&lt;p&gt;$ \hat{r_{ui}} = q_i^Tp_u $&lt;/p&gt;

&lt;p&gt;where $ \hat{r_{ui}} $ denotes the predicted rating for user u and item i and $ q_i^Tp_u $ denotes the dot product of the vector representing item i and the vector representing user u.&lt;/p&gt;

&lt;h2 id=&#34;user-bias-item-bias-and-average-rating:eb1147f7685f7b4fdbf640a27bbc705e&#34;&gt;User bias, item bias and average rating&lt;/h2&gt;

&lt;p&gt;We need to add bias into the mix. Each user&amp;rsquo;s average rating will be different from the average rating across all users. And the average rating of a particular item will be somewhat different from the average across all items.&lt;/p&gt;

&lt;p&gt;
&lt;figure class=&#34;center&#34;&gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/add_bias.png&#34; alt=&#34;Adding user and item bias&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;p&gt;
        Adding user and item bias
        
            
        
        &lt;/p&gt; 
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;Adding a column vector of user bias to the user matrix and matching it with a row vector of 1s in the item matrix has the effect that the rating prediction for a given user/item pair will now have that user&amp;rsquo;s bias added on. Similarly, adding a row vector of item biases to items and a matching column vector of 1s to users will add the item bias to the prediction.&lt;/p&gt;

&lt;p&gt;The average rating across all users and items also needs to be included so in the notation of the MFTRS article each rating will be predicted using:&lt;/p&gt;

&lt;p&gt;$ \hat{r_{ui}} = \mu + b_i + b_u + q_i^Tp_u $&lt;/p&gt;

&lt;p&gt;where $ \mu $ is the overall average rating, $ b_i $ is the bias for item i, $ b_u $ is the bias for user u, and $ q_i^Tp_u $ is the interaction between item i and user u.&lt;/p&gt;

&lt;p&gt;But the overall mean rating can be added on after we do the matrix multiplication.&lt;/p&gt;

&lt;h2 id=&#34;objective-function-and-regularization:eb1147f7685f7b4fdbf640a27bbc705e&#34;&gt;Objective Function and Regularization&lt;/h2&gt;

&lt;p&gt;The objective function, or cost function, used in this approach is simply the sum of squared distances between predicted ratings and actual ratings, so this is what we need to minimize. But in order to prevent over-fitting to the training data we need to constrain the learned values for our user and item features by penalizing high values. We do this by multiplying the sum of the squares of the elements of the user and item matrices by a configurable regularization parameter and including this in our cost function. We&amp;rsquo;ll use gradient descent to minimize the cost.&lt;/p&gt;

&lt;h2 id=&#34;accuracy:eb1147f7685f7b4fdbf640a27bbc705e&#34;&gt;Accuracy&lt;/h2&gt;

&lt;p&gt;The metric I decided on for accuracy was to calculate the fraction of predicted ratings that were within some threshold of the real rating. I used a threshold of 0.5 when evaluating accuracy. Note that this type of absolute threshold works fine if the ratings are on a fixed scale, e.g. 1-5, but for something like the Million Song Dataset, where &amp;ldquo;ratings&amp;rdquo; are actually listen counts, a percentage based threshold would need to be used.&lt;/p&gt;

&lt;h2 id=&#34;sparse-representation-of-matrices:eb1147f7685f7b4fdbf640a27bbc705e&#34;&gt;Sparse representation of matrices&lt;/h2&gt;

&lt;p&gt;The ratings matrix is sparse, meaning most of the values are 0, because each user has only rated a small number of items. The concept of a sparse matrix can actually be translated to a different data structure that retains only information about the non-zero values, making it a much more memory-efficient represenation of the same information. One way is to define a vector of row indices, i, a vector of column indices, j, and a vector of values for each (i,j) pair. So only the (i,j) pairs that have values are included. Using this format, known as coordinate list or COO, the above ratings would be expressed as follows:&lt;/p&gt;

&lt;p&gt;
&lt;figure class=&#34;matrices center&#34;&gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/coo.png&#34; alt=&#34;The same ratings in COO sparse matrix format&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;p&gt;
        The same ratings in COO sparse matrix format
        
            
        
        &lt;/p&gt; 
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;The dataset I worked with was the Movie Lens dataset, available &lt;a href=&#34;http://grouplens.org/datasets/movielens/100k/&#34;&gt;here&lt;/a&gt;. Only the u.data file was needed to train the model. This contains 100,000 ratings from 943 users of 1,682 movies. If we expressed this as a full matrix, we&amp;rsquo;d have 943 x 1,682 = 1,586,126 values to store in memory while doing computations on them. But doing it this way we only have to work with 100,000 x 3 = 300,000 values.&lt;/p&gt;

&lt;p&gt;The user_ids and item_ids in this dataset are already serial IDs, so we have users 1 through 943 and items 1 through 1682. Subracting 1 from each ID allows us to use them as matrix indices. They also need to be ordered first by user index, then by item index for faster access.&lt;/p&gt;

&lt;h2 id=&#34;the-code:eb1147f7685f7b4fdbf640a27bbc705e&#34;&gt;The Code&lt;/h2&gt;

&lt;p&gt;The code below assumes we have training ratings and validation ratings in the above COO format and have extracted the individual columns as the row_indices, col_indices and rating_values variables, in the case of the training set, and variables suffixed with _val for the validation set. For more complete example code, see the Beaker notebook &lt;a href=&#34;https://pub.beakernotebook.com/#/publications/56df05ac-4f52-46fe-a22d-4f604391a577&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The configurable parameters are&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;rank, or number of feature vectors to learn&lt;/li&gt;
&lt;li&gt;lamda (regularization parameter)&lt;/li&gt;
&lt;li&gt;learning rate&lt;/li&gt;
&lt;li&gt;accuracy threshold&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
# Initialize the matrix factors from random normals with mean 0. W will
# represent users and H will represent items.
W = tf.Variable(tf.truncated_normal([num_users, rank], stddev=0.2, mean=0), name=&amp;quot;users&amp;quot;)
H = tf.Variable(tf.truncated_normal([rank, num_items], stddev=0.2, mean=0), name=&amp;quot;items&amp;quot;)

# To the user matrix we add a bias column holding the bias of each user,
# and another column of 1s to multiply the item bias by.
W_plus_bias = tf.concat(1, [W, tf.convert_to_tensor(user_bias, dtype=float32, name=&amp;quot;user_bias&amp;quot;), tf.ones((num_users,1), dtype=float32, name=&amp;quot;item_bias_ones&amp;quot;)])
# To the item matrix we add a row of 1s to multiply the user bias by, and
# a bias row holding the bias of each item.
H_plus_bias = tf.concat(0, [H, tf.ones((1, num_items), name=&amp;quot;user_bias_ones&amp;quot;, dtype=float32), tf.convert_to_tensor(item_bias, dtype=float32, name=&amp;quot;item_bias&amp;quot;)])
# Multiply the factors to get our result as a dense matrix
result = tf.matmul(W_plus_bias, H_plus_bias)

# Now we just want the values represented by the pairs of user and item
# indices for which we had known ratings. Unfortunately TensorFlow does not
# yet support numpy-like indexing of tensors. See the issue for this at
# https://github.com/tensorflow/tensorflow/issues/206 The workaround here
# came from https://github.com/tensorflow/tensorflow/issues/418 and is a
# little esoteric but in numpy this would just be done as follows:
# result_values = result[user_indices, item_indices]
result_values = tf.gather(tf.reshape(result, [-1]), user_indices * tf.shape(result)[1] + item_indices, name=&amp;quot;extract_training_ratings&amp;quot;)

# Same thing for the validation set ratings.
result_values_val = tf.gather(tf.reshape(result, [-1]), user_indices_val * tf.shape(result)[1] + item_indices_val, name=&amp;quot;extract_validation_ratings&amp;quot;)

# Calculate the difference between the predicted ratings and the actual
# ratings. The predicted ratings are the values obtained form the matrix
# multiplication with the mean rating added on.
diff_op = tf.sub(tf.add(result_values, mean_rating, name=&amp;quot;add_mean&amp;quot;), rating_values, name=&amp;quot;raw_training_error&amp;quot;)
diff_op_val = tf.sub(tf.add(result_values_val, mean_rating, name=&amp;quot;add_mean_val&amp;quot;), rating_values_val, name=&amp;quot;raw_validation_error&amp;quot;)

with tf.name_scope(&amp;quot;training_cost&amp;quot;) as scope:
    base_cost = tf.reduce_sum(tf.square(diff_op, name=&amp;quot;squared_difference&amp;quot;), name=&amp;quot;sum_squared_error&amp;quot;)
    # Add regularization.
    regularizer = tf.mul(tf.add(tf.reduce_sum(tf.square(W)), tf.reduce_sum(tf.square(H))), lda, name=&amp;quot;regularize&amp;quot;)
    cost = tf.div(tf.add(base_cost, regularizer), num_ratings * 2, name=&amp;quot;average_error&amp;quot;)

with tf.name_scope(&amp;quot;validation_cost&amp;quot;) as scope:
    cost_val = tf.div(tf.reduce_sum(tf.square(diff_op_val, name=&amp;quot;squared_difference_val&amp;quot;), name=&amp;quot;sum_squared_error_val&amp;quot;), num_ratings_val * 2, name=&amp;quot;average_error&amp;quot;)

# Use an exponentially decaying learning rate.
global_step = tf.Variable(0, trainable=False)
learning_rate = tf.train.exponential_decay(lr, global_step, 10000, 0.96, staircase=True)


with tf.name_scope(&amp;quot;train&amp;quot;) as scope:
    optimizer = tf.train.GradientDescentOptimizer(learning_rate)
    # Passing global_step to minimize() will increment it at each step so
    # that the learning rate will be decayed at the specified intervals.
    train_step = optimizer.minimize(cost, global_step=global_step)

with tf.name_scope(&amp;quot;training_accuracy&amp;quot;) as scope:
  # Just measure the absolute difference against the threshold
  # TODO: support percentage-based thresholds
  good = tf.less(tf.abs(diff_op), threshold)

  accuracy_tr = tf.div(tf.reduce_sum(tf.cast(good, tf.float32)), num_ratings)
  accuracy_tr_summary = tf.scalar_summary(&amp;quot;accuracy_tr&amp;quot;, accuracy_tr)

with tf.name_scope(&amp;quot;validation_accuracy&amp;quot;) as scope:
  # Validation set accuracy:
  good_val = tf.less(tf.abs(diff_op_val), threshold)
  accuracy_val = tf.reduce_sum(tf.cast(good_val, tf.float32)) / num_ratings_val
  accuracy_val_summary = tf.scalar_summary(&amp;quot;accuracy_val&amp;quot;, accuracy_val)

# Create a TensorFlow session and initialize variables.
sess = tf.Session()
sess.run(tf.initialize_all_variables())

# Make sure summaries get written to the logs.
summary_op = tf.merge_all_summaries()
writer = tf.train.SummaryWriter(&amp;quot;/tmp/recommender_logs&amp;quot;, sess.graph_def)

# Run the graph and see how we&#39;re doing on every 500th iteration.
for i in range(max_iter):
    if i % 500 == 0:
        res = sess.run([summary_op, accuracy_tr, accuracy_val, cost, cost_val])
        summary_str = res[0]
        acc_tr = res[1]
        acc_val = res[2]
        cost_ev = res[3]
        cost_val_ev = res[4]
        writer.add_summary(summary_str, i)
        print(&amp;quot;Training accuracy at step %s: %s&amp;quot; % (i, acc_tr))
        print(&amp;quot;Validation accuracy at step %s: %s&amp;quot; % (i, acc_val))
        print(&amp;quot;Training cost: %s&amp;quot; % (cost_ev))
        print(&amp;quot;Validation cost: %s&amp;quot; % (cost_val_ev))
    else:
        sess.run(train_step)

with tf.name_scope(&amp;quot;final_model&amp;quot;) as scope:
    # At the end we want to get the final ratings matrix by adding the mean
    # to the result matrix and doing any further processing required
    add_mean_final = tf.add(result, mean_rating, name=&amp;quot;add_mean_final&amp;quot;)
    if result_processor == None:
        final_matrix = add_mean_final
    else:
        final_matrix = result_processor(add_mean_final)
    final_res = sess.run([final_matrix])

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;computation-graph:eb1147f7685f7b4fdbf640a27bbc705e&#34;&gt;Computation Graph&lt;/h2&gt;

&lt;p&gt;Here is the overall computation graph produced by this code as visualized by TensorBoard. Click the image to enlarge it.
&lt;a href=&#34;#&#34; data-featherlight=&#34;/images/graph_unexpanded.png&#34;&gt;
        &lt;img src=&#34;http://katbailey.github.io/images/graph_unexpanded.png&#34; width=&#34;600px&#34; /&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;And here&amp;rsquo;s the training cost portion of the graph expanded.
&lt;a href=&#34;#&#34; data-featherlight=&#34;/images/graph_expanded.png&#34;&gt;
        &lt;img src=&#34;http://katbailey.github.io/images/graph_expanded.png&#34; width=&#34;600px&#34; /&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;h2 id=&#34;results-and-next-steps:eb1147f7685f7b4fdbf640a27bbc705e&#34;&gt;Results and Next Steps&lt;/h2&gt;

&lt;p&gt;OK, this is the part where I come clean and admit that I have not yet spent sufficient time tweaking all the tweakable things to train a model that performs satisfactorily on unseen data. Very disappointingly, I haven&amp;rsquo;t seen better than 42% accuracy on the validation set. With an accuracy threshold of 0.5, this means that only roughly two out of five predicted ratings were within 0.5 of the actual rating. Not very impressive :(.&lt;/p&gt;

&lt;p&gt;Interestingly though, the one time I ran it for 1 million iterations, the &lt;em&gt;training&lt;/em&gt; accuracy never went above 60% and there had been no change in cost over the last several thousand iterations. This suggests that it got stuck in a local minimum and couldn&amp;rsquo;t get out.&lt;/p&gt;

&lt;p&gt;I just started reading &lt;a href=&#34;http://papers.nips.cc/paper/3208-probabilistic-matrix-factorization.pdf&#34;&gt;this paper on Probabilistic Matrix Factorization&lt;/a&gt;, which says in its introduction:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Since most real-world datasets are sparse, most entries in R will be missing. In those cases, the sum-squared distance is computed only for the observed entries of the target matrix R. As shown by (&amp;hellip;), this seemingly minor modification results in a difficult non-convex optimization problem which cannot be solved using standard SVD implementations.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So my suspicion is that I have been taking a very naive approach to a highly non-convex optimization problem that no amount of parameter tuning is going to get me past. My next step will be to try to follow the probabilistic approach described in the above paper and present my findings in a follow-up post.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>My Favourite Data Science Podcasts</title>
      <link>http://katbailey.github.io/post/data-science-podcasts/</link>
      <pubDate>Sat, 06 Feb 2016 10:58:20 -0700</pubDate>
      
      <guid>http://katbailey.github.io/post/data-science-podcasts/</guid>
      <description>

&lt;p&gt;There&amp;rsquo;s a pretty impressive selection of high-quality podcasts out there these days on topics in data science. Here are four that I am really enjoying right now, along with my take on what is good about each of them.&lt;/p&gt;

&lt;h2 id=&#34;not-so-standard-deviations:6faa704a913ba6d911650b2542fcebc9&#34;&gt;Not So Standard Deviations&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://soundcloud.com/nssd-podcast&#34;&gt;NSSD podcast on SoundCloud&lt;/a&gt; Two very smart people with PhDs in biostatistics, one still in academia and the other working as a data scientist for Etsy, Roger and Hilary sure do ramble on but the ramblings are great :) They cover all sorts of topics, always at least loosely related to data science and my favourite things about the podcast are 1. the discussions about differences between academia and industry when it comes to data science and 2. how unafraid they both are to wander into topics they admit to knowing very little about and just wonder curiously aloud about them. Very accessible and enjoyable.&lt;/p&gt;

&lt;h2 id=&#34;partially-derivative:6faa704a913ba6d911650b2542fcebc9&#34;&gt;Partially Derivative&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://www.partiallyderivative.com/&#34;&gt;partiallyderivative.com&lt;/a&gt; When I first started listening to this show I thought the guys were a bit too bro-tastic for me to be able to listen to them for long. They are terribly silly but they cover so many interesting data science related news stories with a great balance between serious stories and utterly trivial and hilarious ones (e.g. about someone training a model to predict when Game of Thrones characters will die, or someone writing a program that uses Markov chains to generate pretentious craft beer reviews). When listening to this podcast I regularly find myself bursting out laughing in spite of myself.&lt;/p&gt;

&lt;h2 id=&#34;data-skeptic:6faa704a913ba6d911650b2542fcebc9&#34;&gt;Data Skeptic&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://dataskeptic.com/&#34;&gt;dataskeptic.com&lt;/a&gt;
This one alternates its format between full length episodes where the presenter, Kyle, interviews people working or doing research in data science, and mini-episodes where he explains topics to his wife, a non data scientist, in ways someone not from the field can understand. Personally I prefer the interview episodes - highlights so far have been the interview with Gordon Pennycook on his research into people&amp;rsquo;s susceptibility to &amp;ldquo;pseudo-profound bullshit&amp;rdquo;, and the interview with Thomas Levi, former physicist and current lead data scientist at Plenty of Fish.&lt;/p&gt;

&lt;h2 id=&#34;talking-machines:6faa704a913ba6d911650b2542fcebc9&#34;&gt;Talking Machines&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://www.thetalkingmachines.com/&#34;&gt;thetalkingmachines.com&lt;/a&gt; OK this one is my absolute favourite. It is an extremely high quality production, both in terms of content and how it is put together. There&amp;rsquo;s a set format, where they start out with an exploration of a particular topic in machine learning, e.g. Gaussian processes, active learning, expectation maximization, to name a few they&amp;rsquo;ve covered. Then they&amp;rsquo;ll answer a question that came in from a listener, before moving on to the &amp;ldquo;meat&amp;rdquo; of the show, which is an interview with someone, usually an academic, doing research in machine learning. Among those they&amp;rsquo;ve interviewed are Geoff Hinton, Andrew Ng, Yann LeCun, Nando de Freitas and Claudia Perlich. I always feel just a little bit smarter after listening to an episode of Talking Machines.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Machine Learning talk</title>
      <link>http://katbailey.github.io/post/machine-learning-talk/</link>
      <pubDate>Tue, 26 Jan 2016 07:45:28 -0700</pubDate>
      
      <guid>http://katbailey.github.io/post/machine-learning-talk/</guid>
      <description>&lt;p&gt;Here are the &amp;ldquo;slides&amp;rdquo; from a talk I gave on machine learning last week. The idea is to give an overview of the different topics and how they fit together. I may end up building on it as I learn about more facets of ML.&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    &lt;a href=&#34;http://katbailey.github.io/ml&#34;&gt;
        &lt;img src=&#34;http://katbailey.github.io/images/mlwords.png&#34; /&gt;
    &lt;/a&gt;
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;In case you&amp;rsquo;re wondering, the slides were created using &lt;a href=&#34;https://github.com/regebro/hovercraft&#34;&gt;Hovercraft&lt;/a&gt; which is a python tool for creating impress.js slides but authoring them in reStructuredText instead of HTML.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Adventures learning Neural Nets and Python</title>
      <link>http://katbailey.github.io/post/neural-nets-in-python/</link>
      <pubDate>Mon, 21 Dec 2015 21:04:12 -0700</pubDate>
      
      <guid>http://katbailey.github.io/post/neural-nets-in-python/</guid>
      <description>

&lt;p&gt;This documents my efforts to learn both neural networks and, to a certain extent, the Python programming language. I say &amp;ldquo;to a certain extent&amp;rdquo; because far from feeling all &amp;ldquo;yay! I know Python now!&amp;rdquo; I feel more like &amp;ldquo;I can use Python 2.7 in certain ways to do certain things&amp;hellip; yay?&amp;rdquo;&lt;/p&gt;

&lt;p&gt;And what of my understanding of neural nets as a result of this exercise? After battling with my naïve implementation of a multi-layer perceptron as described below, I felt I had a pretty visceral understanding of them. But then I started looking at &lt;a href=&#34;http://deeplearning.net/software/theano/&#34;&gt;Theano&lt;/a&gt; and &lt;a href=&#34;https://www.tensorflow.org/&#34;&gt;Google&amp;rsquo;s TensorFlow&lt;/a&gt; with their convolutional neural networks etc, and it was the same old story: the more I learned, the more I realized I had yet to learn. So now there are all sorts of books and posts about various aspects of deep learning that I want to read, which I link to at this end of this post.&lt;/p&gt;

&lt;p&gt;For a basic intro to how neural nets work, I recommend &lt;a href=&#34;http://sebastianraschka.com/Articles/2015_singlelayer_neurons.html&#34;&gt;Sebastian Raschka&amp;rsquo;s post on Single-Layer Neural Networks&lt;/a&gt;. In short, though, the setup of a neural net for doing multi-class classification is as follows: at a minimum you have an input layer and an output layer. The input layer is  the set of features you feed in and the output layer is the classification for each example. But you will likely also have at least one hidden layer as well.
&lt;a title=&#34;By Glosser.ca [CC BY-SA 3.0 (http://creativecommons.org/licenses/by-sa/3.0)], via Wikimedia Commons&#34; href=&#34;https://commons.wikimedia.org/wiki/File%3AColored_neural_network.svg&#34;&gt;&lt;img width=&#34;256&#34; alt=&#34;Colored neural network&#34; src=&#34;https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/256px-Colored_neural_network.svg.png&#34;/&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;A set of weights is applied to one layer to get to the next, until you reach the output layer, and training a neural network is about learning what these weights should be.&lt;/p&gt;

&lt;h3 id=&#34;a-little-background:14c7068255137565e326b1d24878d8a5&#34;&gt;A little background&lt;/h3&gt;

&lt;p&gt;I recently took &lt;a href=&#34;https://www.coursera.org/learn/machine-learning&#34;&gt;Andrew Ng&amp;rsquo;s Coursera course on Machine Learning&lt;/a&gt;. It&amp;rsquo;s taught through matlab and goes into the math behind classic machine learning algorithms such as neural networks. But I&amp;rsquo;ve been noticing that a lot of the newer code and tutorials out there for learning neural nets (e.g. Google&amp;rsquo;s TensorFlow tutorial) are in Python. So I thought, wouldn&amp;rsquo;t it be a fun exercise to port my matlab neural net to python and then learn about all the new libraries there are in python for doing this stuff, one of which is called Lasagne. Because layers :)&lt;/p&gt;

&lt;p&gt;Here I use the handwritten digits dataset from the ML course assignment, which can be found &lt;a href=&#34;https://s3.amazonaws.com/spark-public/ml/exercises/on-demand/machine-learning-ex4.zip&#34;&gt;here&lt;/a&gt;. It is much smaller than the &lt;a href=&#34;http://yann.lecun.com/exdb/mnist/&#34;&gt;MNIST dataset&lt;/a&gt; used in most tutorials, both in number of examples and in image size - each image is 20x20 pixels. I train 3 different neural networks:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;A simple port to Python of the matlab code I wrote for the ML course assignment&lt;/li&gt;
&lt;li&gt;An adaptation of the multi-layer perceptron from the Theano + Lasagne tutorial&lt;/li&gt;
&lt;li&gt;An adaptation of the convolutional neural net from the TensorFlow tutorial&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; This is not a tutorial. It is simply an exploration, by a non-expert, of the topic of training neural nets in python. There are lots of great tutorials on this stuff, e.g. the ones mentioned below for Lasagne and TensorFlow, and also &lt;a href=&#34;http://rasbt.github.io/mlxtend/docs/classifier/neuralnet_mlp/&#34;&gt;this one&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;initial-setup:14c7068255137565e326b1d24878d8a5&#34;&gt;Initial Setup&lt;/h3&gt;

&lt;p&gt;Load some required libraries, extract the data from the matlab file and split it into training, validation and test sets.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from __future__ import print_function
import numpy as np
import scipy.io as sio
from sklearn.cross_validation import train_test_split
from pylab import *

mat_contents = sio.loadmat(&#39;ex4data1.mat&#39;)
# 0s were converted to 10s in the matlab data because matlab
# indices start at 1, so we need to change them back to 0s
labels = mat_contents[&#39;y&#39;]
labels = np.where(labels == 10, 0, labels)
labels = labels.reshape((labels.shape[0],))
X_train, X_test, y_train, y_test = train_test_split(mat_contents[&#39;X&#39;], labels)
X_train, X_val = X_train[:-1000], X_train[-1000:]
y_train, y_val = y_train[:-1000], y_train[-1000:]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here&amp;rsquo;s a visualization of one of the example images:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.imshow(X_train[1202].reshape((20, 20), order=&#39;F&#39;), cmap=&#39;Greys&#39;,  interpolation=&#39;nearest&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/example_four.png&#34; alt=&#34;A number four from the training examples&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;p&gt;
        A number four from the training examples
        
            
        
        &lt;/p&gt; 
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;h2 id=&#34;1-naïve-neural-net:14c7068255137565e326b1d24878d8a5&#34;&gt;1. Naïve neural net&lt;/h2&gt;

&lt;p&gt;This is where I just port the code I wrote in Matlab for the Coursera Machine Learning course into python. And where I learned that multiplying large matrices in Python is to be avoided :) More on that below, first here&amp;rsquo;s the code:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from scipy.optimize import minimize

# Basic sigmoid function for logistic regression.
def sigmoid(X):
    return 1.0 / (1.0 + math.e ** (-1.0 * X)) 

# Randomly initializes the weights for layer with the specified numbers of
# incoming and outgoing connections.
def randInitializeWeights(incoming, outgoing):
    epsilon_init = 0.12
    return rand(outgoing, 1 + incoming) * (2 * epsilon_init) - epsilon_init

# Adds the bias column to the matrix X.
def addBias(X):
    return np.concatenate((np.ones((X.shape[0],1)), X), 1) 

# Reconstitutes the two weight matrices from a single vector, given the
# size of the input layer, the hidden layer, and the number of possible
# labels in the output.
def extractWeightMatrices(thetas, input_layer_size, hidden_layer_size, num_labels):
    theta1size = (input_layer_size + 1) * hidden_layer_size
    theta1 = reshape(thetas[:theta1size], (hidden_layer_size, input_layer_size + 1), order=&#39;A&#39;)
    theta2 = reshape(thetas[theta1size:], (num_labels, hidden_layer_size + 1), order=&#39;A&#39;)
    return theta1, theta2

# Converts single lables to one-hot vectors.
def convertLabelsToClassVectors(labels, num_classes):
    labels = labels.reshape((labels.shape[0],1))
    ycols = np.tile(labels, (1, num_classes))
    m, n = ycols.shape
    indices = np.tile(np.arange(num_classes).reshape((1,num_classes)), (m, 1))
    ymat = indices == ycols
    return ymat.astype(int)

# Returns a vector corresponding to the randomly initialized weights for the
# input layer and hidden layer.
def getInitialWeights(input_layer_size, hidden_layer_size, num_labels):
    theta1 = randInitializeWeights(input_layer_size, hidden_layer_size)
    theta2 = randInitializeWeights(hidden_layer_size, num_labels)
    return np.append(theta1.ravel(order=&#39;A&#39;), theta2.ravel(order=&#39;A&#39;))

# Trains a basic multilayer perceptron. Returns weights to use for feed-forward
# pass to predict on new data.
def train(X_train, y_train, hidden_layer_size, lmda, maxIter):
    input_layer_size = X_train.shape[1]
    num_labels = 10
    initial_weights = getInitialWeights(input_layer_size, hidden_layer_size, num_labels)
    if y_train.ndim == 1:
        # Convert the labels to one-hot vectors.
        y_train = convertLabelsToClassVectors(y_train, num_labels)

    # Given weights for the input layer and hidden layer, calulates the 
    # activations for the hidden layer and the output layer of a 3-layer nn.
    def getActivations(theta1, theta2):
        z2 = np.dot(addBias(X_train),theta1.T)
        a2 = np.concatenate((np.ones((z2.shape[0],1)), sigmoid(z2)), 1)
        # a2 is an m x num_hidden+1 matrix, Theta2 is a num_labels x
        # num_hidden+1 matrix
        z3 = np.dot(a2,theta2.T)
        a3 = sigmoid(z3) # Now we have an m x num_labels matrix
        return a2, a3

    # Cost function to be minimized with respect to weights.
    def costFunction(weights):
        theta1, theta2 = extractWeightMatrices(weights, input_layer_size, hidden_layer_size, num_labels)
        hidden_activation, output_activation = getActivations(theta1, theta2)
        m = X_train.shape[0]
        cost = sum((-y_train * log(output_activation)) - ((1 - y_train) * log(1-output_activation))) / m
        # Regularization
        thetasq = sum(theta1[:,1:(input_layer_size + 1)]**2) + sum(theta2[:,1:hidden_layer_size + 1]**2)
        reg = (lmda / float(2*m)) * thetasq
        print(&amp;quot;Training loss:\t\t{:.6f}&amp;quot;.format(cost))
        return cost + reg

    # Gradient function to pass to our optimization function.
    def calculateGradient(weights):
        theta1, theta2 = extractWeightMatrices(weights, input_layer_size, hidden_layer_size, num_labels)
        # Backpropagation - step 1: feed-forward.
        hidden_activation, output_activation = getActivations(theta1, theta2)
        m = X_train.shape[0]
        # Step 2 - the error in the output layer is just the difference
        # between the output layer and y
        delta_3 = output_activation - y_train # delta_3 is m x num_labels
        delta_3 = delta_3.T

        # Step 3
        sigmoidGrad = hidden_activation * (1 - hidden_activation)
        delta_2 = (np.dot(theta2.T,delta_3)) * sigmoidGrad.T
        delta_2 = delta_2[1:, :] # hidden_layer_size x m
        theta1_grad = np.dot(delta_2, np.concatenate((np.ones((X_train.shape[0],1)), X_train), 1))
        theta2_grad = np.dot(delta_3, hidden_activation)
        # Add regularization
        reg_grad1 = (lmda / float(m)) * theta1
        # We don&#39;t regularize the weight for the bias column
        reg_grad1[:,0] = 0
        reg_grad2 = (lmda / float(m)) * theta2;
        reg_grad2[:,0] = 0
        return np.append(ravel((theta1_grad / float(m)) + reg_grad1, order=&#39;A&#39;), ravel((theta2_grad / float(m)) + reg_grad2, order=&#39;A&#39;))

    # Use scipy&#39;s minimize function with method &amp;quot;BFGS&amp;quot; to find the optimum
    # weights.
    res = minimize(costFunction, initial_weights, method=&#39;BFGS&#39;, jac=calculateGradient, options={&#39;disp&#39;: False, &#39;maxiter&#39;:maxIter})
    theta1, theta2 = extractWeightMatrices(res.x, input_layer_size, hidden_layer_size, num_labels)
    return theta1, theta2

# Predicts the output given input and weights.
def predict(X, theta1, theta2):
    m, n = X.shape
    X = addBias(X)
    h1 = sigmoid(np.dot(X,theta1.T))
    h2 = sigmoid(addBias(h1).dot(theta2.T))
    return np.argmax(h2, axis=1)

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That last function, predict, allows us to pass in a set of image representations, and some weights for the input and hidden layers, and it will predict the classification, i.e. which of digits 0 through 9 is represented by the image. First let&amp;rsquo;s see what we get with a random set of weights for each layer:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;init_weights = getInitialWeights(400, 25, 10)
theta1_init, theta2_init = extractWeightMatrices(init_weights, 400, 25, 10)
pred_train = predict(X_train, theta1_init, theta2_init)
sum(np.where(y_train == pred_train, 1, 0))/float(X_train.shape[0])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;0.11054545454545454
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So, just over 11% accuracy, which when you think about it is roughly what you&amp;rsquo;d expect: given there are 10 possible classes for each image (digits 0 through 9), you have a 10% chance of getting it right simply by guessing.&lt;/p&gt;

&lt;p&gt;So now let&amp;rsquo;s learn some better weights&amp;hellip;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;theta1, theta2 = nn.train(X_train, y_train, 25, 0, 50)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;On my machine it takes about an hour to run this for 50 iterations. As an aside, I also tried to use it on the MNIST dataset. That dataset has 60,000 images of size 28x28 pixels. So the input layer consists of 784 features. With a hidden layer of 30 units it would grind away for a long time and eventually run out of memory. I managed to get it to run if I halved the number of hidden units.&lt;/p&gt;

&lt;p&gt;I did some profiling using the awesome &lt;a href=&#34;https://github.com/rkern/line_profiler&#34;&gt;line_profiler&lt;/a&gt; to try to ascertain what the problem was. This helped me identify a few places where my code could be made more efficient - for example, initially I still had a for loop in the cost function - gasp! The first thing you learn in neural net school is the importance of using vectorized approaches to the computations. Anyway after I had fixed a few things like that it soon became clear to me that things weren&amp;rsquo;t getting hung inside any of my functions but in the optimization function itself. To cut a long story short, it was having to do a matrix multiplication where the matrices were 23860x23860. This number comes from the &amp;ldquo;unrolled&amp;rdquo; and concatenated weight vectors (with bias added on): (785 * 30) + (31*10). I have 16GB of RAM on my local machine, but that is apparently not enough for this operation to be run in Python. Both Theano and TensorFlow do all the heavy lifting in C, and this makes an enormous difference, as we&amp;rsquo;ll see.&lt;/p&gt;

&lt;h3 id=&#34;results-after-50-iterations-and-no-regularization:14c7068255137565e326b1d24878d8a5&#34;&gt;Results after 50 iterations and no regularization&lt;/h3&gt;

&lt;p&gt;Training set:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;predictions = predict(X_train, theta1, theta2)
sum(np.where(y_train == predictions, 1, 0))/float(X_train.shape[0])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Accuracy: 0.94145454545454543
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Validation set:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;predictions = predict(X_val, theta1, theta2)
sum(np.where(y_val == predictions, 1, 0))/float(X_val.shape[0])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Accuracy: 0.91900000000000004
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Test set:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;predictions = predict(X_test, theta1, theta2)
sum(np.where(y_test == predictions, 1, 0))/float(X_test.shape[0])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Accuracy: 0.91359999999999997
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Not bad for just 50 iterations!&lt;/p&gt;

&lt;h2 id=&#34;2-using-theano-and-lasagne:14c7068255137565e326b1d24878d8a5&#34;&gt;2. Using Theano and Lasagne&lt;/h2&gt;

&lt;p&gt;This code is adapted from the &lt;a href=&#34;http://lasagne.readthedocs.org/en/latest/user/tutorial.html&#34;&gt;Lasagne tutorial&lt;/a&gt; (specifically the multi-layer perceptron.) I turned it into a function that works on the smaller data set and includes parameters for specifying:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;the number of units in its single hidden layer&lt;/li&gt;
&lt;li&gt;the number of epochs to run for&lt;/li&gt;
&lt;li&gt;the value to use for l2 regularization&lt;/li&gt;
&lt;li&gt;whether or not to use dropout layers (though the actual dropout probabilities for the input and hidden layers are hard-coded)&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import time
import theano
import theano.tensor as T
import lasagne
from lasagne.regularization import regularize_layer_params_weighted, l2, l1

# Uses Lasagne to train a multi-layer perceptron, adapted from
# http://lasagne.readthedocs.org/en/latest/user/tutorial.html
def lasagne_mlp(X_train, y_train, X_val, y_val, X_test, y_test, hidden_units=25, num_epochs=500, l2_param = 0.01, use_dropout=True):
    X_train = X_train.reshape(-1, 1, 400)
    X_val = X_val.reshape(-1, 1, 400)
    X_test = X_test.reshape(-1, 1, 400)
    # Prepare Theano variables for inputs and targets
    input_var = T.tensor3(&#39;inputs&#39;)
    target_var = T.ivector(&#39;targets&#39;)

    print(&amp;quot;Building model and compiling functions...&amp;quot;)
    # Input layer
    network = lasagne.layers.InputLayer(shape=(None, 1, 400),
                                     input_var=input_var)

    if use_dropout:
        # Apply 20% dropout to the input data:
        network = lasagne.layers.DropoutLayer(network, p=0.2)

    # A single hidden layer with number of hidden units as specified in the
    # parameter.
    l_hid1 = lasagne.layers.DenseLayer(
            network, num_units=hidden_units,
            nonlinearity=lasagne.nonlinearities.rectify,
            W=lasagne.init.GlorotUniform())

    if use_dropout:
        # Dropout of 50%:
        l_hid1_drop = lasagne.layers.DropoutLayer(l_hid1, p=0.5)
        # Fully-connected output layer of 10 softmax units:
        network = lasagne.layers.DenseLayer(
            l_hid1_drop, num_units=10,
            nonlinearity=lasagne.nonlinearities.softmax)
    else:
        # Fully-connected output layer of 10 softmax units:
        network = lasagne.layers.DenseLayer(
            l_hid1, num_units=10,
            nonlinearity=lasagne.nonlinearities.softmax)

    # Loss expression for training
    prediction = lasagne.layers.get_output(network)
    loss = lasagne.objectives.categorical_crossentropy(prediction, target_var)
    loss = loss.mean()
    # Regularization.
    l2_penalty = lasagne.regularization.regularize_layer_params_weighted({l_hid1: l2_param}, l2)
    loss = loss + l2_penalty
    # Update expressions for training, using Stochastic Gradient Descent.
    params = lasagne.layers.get_all_params(network, trainable=True)
    updates = lasagne.updates.nesterov_momentum(
            loss, params, learning_rate=0.01, momentum=0.9)

    # Loss expression for evaluation.
    test_prediction = lasagne.layers.get_output(network, deterministic=True)
    test_loss = lasagne.objectives.categorical_crossentropy(test_prediction,
                                                            target_var)
    test_loss = test_loss.mean()
    # Expression for the classification accuracy:
    test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),
                      dtype=theano.config.floatX)

    # Compile a function performing a training step on a mini-batch (by giving
    # the updates dictionary) and returning the corresponding training loss:
    train_fn = theano.function([input_var, target_var], loss, updates=updates)

    # Compile a second function computing the validation loss and accuracy:
    val_fn = theano.function([input_var, target_var], [test_loss, test_acc])

    # Finally, launch the training loop.
    print(&amp;quot;Starting training...&amp;quot;)
    # Keep track of taining and validation cost over the epochs
    epoch_cost_train = np.empty(num_epochs, dtype=float32)
    epoch_cost_val = np.empty(num_epochs, dtype=float32)
    # We iterate over epochs:
    for epoch in range(num_epochs):
        # In each epoch, we do a full pass over the training data:
        train_err = 0
        # We also want to keep track of the deterministic (feed-forward) 
        # training error.
        train_err_ff = 0
        train_batches = 0
        start_time = time.time()
        for batch in iterate_minibatches(X_train, y_train, 50, shuffle=True):
            inputs, targets = batch
            err, acc = val_fn(inputs, targets)
            train_err_ff += err
            train_err += train_fn(inputs, targets)

            train_batches += 1

        # And a full pass over the validation data:
        val_err = 0
        val_acc = 0
        val_batches = 0
        for batch in iterate_minibatches(X_val, y_val, 50, shuffle=False):
            inputs, targets = batch
            err, acc = val_fn(inputs, targets)
            val_err += err
            val_acc += acc
            val_batches += 1

        epoch_cost_train[epoch] = train_err_ff / train_batches
        epoch_cost_val[epoch] = val_err / val_batches
        # Then we print the results for this epoch:
        print(&amp;quot;Epoch {} of {} took {:.3f}s&amp;quot;.format(
            epoch + 1, num_epochs, time.time() - start_time))
        print(&amp;quot;  training loss:\t\t{:.6f}&amp;quot;.format(train_err / train_batches))
        print(&amp;quot;  validation loss:\t\t{:.6f}&amp;quot;.format(val_err / val_batches))
        print(&amp;quot;  validation accuracy:\t\t{:.2f} %&amp;quot;.format(
            val_acc / val_batches * 100))

    # After training, we compute and print the test error:
    test_err = 0
    test_acc = 0
    test_batches = 0
    for batch in iterate_minibatches(X_test, y_test, 50, shuffle=False):
        inputs, targets = batch
        err, acc = val_fn(inputs, targets)
        test_err += err
        test_acc += acc
        test_batches += 1
    print(&amp;quot;Final results:&amp;quot;)
    print(&amp;quot;  test loss:\t\t\t{:.6f}&amp;quot;.format(test_err / test_batches))
    print(&amp;quot;  test accuracy:\t\t{:.2f} %&amp;quot;.format(
        test_acc / test_batches * 100))
    return epoch_cost_train, epoch_cost_val

# This function was copied verbatim from the Lasagne tutorial at 
# http://lasagne.readthedocs.org/en/latest/user/tutorial.html
def iterate_minibatches(inputs, targets, batchsize, shuffle=False):
    assert len(inputs) == len(targets)
    if shuffle:
        indices = np.arange(len(inputs))
        np.random.shuffle(indices)
    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):
        if shuffle:
            excerpt = indices[start_idx:start_idx + batchsize]
        else:
            excerpt = slice(start_idx, start_idx + batchsize)
        yield inputs[excerpt], targets[excerpt]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here we run it for 500 epochs without regularization but with dropout on the input and hidden layers.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;epoch_cost_train, epoch_cost_val = lasagne_mlp(X_train, y_train, X_val, y_val, X_test,
 y_test, hidden_units=800, num_epochs=500, l2_param=0, use_dropout=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Epoch 1 of 500 took 0.495s
  training loss:                0.282050
  validation loss:              0.216164
  validation accuracy:          94.50 %
...
Epoch 500 of 500 took 0.504s
  training loss:                0.016550
  validation loss:              0.127085
  validation accuracy:          97.50 %
Final results:
  test loss:                    0.152830
  test accuracy:                96.48 %
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Each epoch generally took less than a second to run. But one time I ran it and noticed it taking forever to run a single epoch - it would grind away and eventually get through one epoch after about 8 minutes. So I thought to myself &amp;ldquo;I wonder if that warning I saw but ignored when Theano got imported was actually important&amp;rdquo;&amp;hellip; Yep. The warning was &amp;ldquo;&lt;em&gt;WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded.&lt;/em&gt;&amp;rdquo; It couldn&amp;rsquo;t find g++ and so it was actually doing everything in Python. Turned out it was because I had upgraded XCode the previous day but the upgrade hadn&amp;rsquo;t completed (because I hadn&amp;rsquo;t opened it and accepted the license agreement). Anyway, the point here is: less than a second versus 8 minutes&amp;hellip; &lt;em&gt;holy shit!&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Epoch 1 of 500 took 509.737s
  training loss:                1.544798
  validation loss:              0.795227
  validation accuracy:          83.20 %
Epoch 2 of 500 took 489.701s
  training loss:                0.730792
  validation loss:              0.512750
  validation accuracy:          86.90 %
Epoch 3 of 500 took 655.276s
  training loss:                0.557232
  validation loss:              0.430175
  validation accuracy:          88.10 %
Epoch 4 of 500 took 496.489s
  training loss:                0.498586
  validation loss:              0.382306
  validation accuracy:          89.30 %
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;the-dangers-of-over-fitting:14c7068255137565e326b1d24878d8a5&#34;&gt;The dangers of over-fitting&lt;/h2&gt;

&lt;p&gt;To explain what over-fitting is, let&amp;rsquo;s imagine an extreme example. Let&amp;rsquo;s say we only have a very small set of training example images with which to train our neural net. Now imagine that it happens to be the case that almost every number 7 example has the stem crossed, like this example:&lt;/p&gt;

&lt;p&gt;
&lt;figure class=&#34;center&#34;&gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/seven_stem.png&#34; alt=&#34;A seven example with a crossed stem&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;p&gt;
        A seven example with a crossed stem
        
            
        
        &lt;/p&gt; 
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;When you train on these examples the model will not generalize well to new examples that do not have this feature. We could say that it would give &lt;em&gt;too much weight&lt;/em&gt; to this crossed stem feature. A great measure against over-fitting is having lots and lots of data to train your model on, because the more data you have the less likely you are to have this type of scenario. But beyond getting more data, there are a couple of other ways to minimize this over-fitting problem. The standard way is to use regularization, where you penalize the weights such that minimizing the cost necessarily means shrinking the weights towards 0. Read more about regularization &lt;a href=&#34;https://en.wikipedia.org/wiki/Regularization_(mathematics&#34;&gt;here&lt;/a&gt;). However, &lt;a href=&#34;http://arxiv.org/abs/1207.0580&#34;&gt;Hinton et al&lt;/a&gt; came up with a solution for neural nets that works by &amp;ldquo;randomly omitting half of the feature detectors on each training case&amp;rdquo;. It&amp;rsquo;s called dropout and it is very effective.&lt;/p&gt;

&lt;p&gt;The graph below shows what happens to our validation error, as compared with the training error if we perform no regularization and do not include dropout layers in our model.&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/500_800_noreg_nodropout.png&#34; alt=&#34;No dropout, no regularization&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;p&gt;
        No dropout, no regularization
        
            
        
        &lt;/p&gt; 
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;And here&amp;rsquo;s what happens with 20% dropout on the input layer and 50% on the hidden layer. Still no regularization.

&lt;figure &gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/500_800_noreg.png&#34; alt=&#34;Dropout layers, no regularization&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;p&gt;
        Dropout layers, no regularization
        
            
        
        &lt;/p&gt; 
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;Accuracy on the test set without dropout was generally around 94%, with dropout was around 97%.&lt;/p&gt;

&lt;p&gt;If we try without dropout but with l2 regularization it looks like this is not as effective at bringing down the validation error.

&lt;figure &gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/500_800_reg01_nodropout.png&#34; alt=&#34;Regularization, no dropout&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;p&gt;
        Regularization, no dropout
        
            
        
        &lt;/p&gt; 
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;The code for generating the above plots, assuming your training and validation results as returned from lasagne_mlp are in the epoch_cost_train and epoch_cost_val variables respectively, is as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-pyton&#34;&gt;plt.style.use(&#39;bmh&#39;)
plt.plot(range(len(epoch_cost_train)), epoch_cost_train, label=&amp;quot;Training error&amp;quot;)
plt.plot(range(len(epoch_cost_val)), epoch_cost_val, label=&amp;quot;Validation error&amp;quot;)
legend()
plt.xlabel(&amp;quot;Num epochs&amp;quot;)
plt.ylabel(&amp;quot;Cost&amp;quot;) 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;3-using-tensorflow:14c7068255137565e326b1d24878d8a5&#34;&gt;3. Using TensorFlow&lt;/h2&gt;

&lt;p&gt;In this last section I achieve little more than proving to myself that I can get enough of a handle on things as to be able to adapt the TensorFlow tutorial to my Coursera data set. Go me.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import tensorflow as tf

# This function was copied verbatim from the TensorFlow tutorial at
# https://www.tensorflow.org/versions/master/tutorials/index.html
def dense_to_one_hot(labels_dense, num_classes=10):
  &amp;quot;&amp;quot;&amp;quot;Convert class labels from scalars to one-hot vectors.&amp;quot;&amp;quot;&amp;quot;
  num_labels = labels_dense.shape[0]
  index_offset = np.arange(num_labels) * num_classes
  labels_one_hot = np.zeros((num_labels, num_classes))
  labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1
  return labels_one_hot

# Adapted from the TensorFlow tutorial at
# https://www.tensorflow.org/versions/master/tutorials/index.html
class DataSet(object):
  def __init__(self, images, labels):
    assert images.shape[0] == labels.shape[0], (
      &amp;quot;images.shape: %s labels.shape: %s&amp;quot; % (images.shape,
                                             labels.shape))
    self._num_examples = images.shape[0]
    self._images = images
    self._labels = labels
    self._epochs_completed = 0
    self._index_in_epoch = 0
  @property
  def images(self):
    return self._images
  @property
  def labels(self):
    return self._labels
  @property
  def num_examples(self):
    return self._num_examples
  @property
  def epochs_completed(self):
    return self._epochs_completed
  def next_batch(self, batch_size):
    &amp;quot;&amp;quot;&amp;quot;Return the next `batch_size` examples from this data set.&amp;quot;&amp;quot;&amp;quot;
    start = self._index_in_epoch
    self._index_in_epoch += batch_size
    if self._index_in_epoch &amp;gt; self._num_examples:
      # Finished epoch
      self._epochs_completed += 1
      # Shuffle the data
      perm = np.arange(self._num_examples)
      np.random.shuffle(perm)
      self._images = self._images[perm]
      self._labels = self._labels[perm]
      # Start next epoch
      start = 0
      self._index_in_epoch = batch_size
      assert batch_size &amp;lt;= self._num_examples
    end = self._index_in_epoch
    return self._images[start:end], self._labels[start:end]
def read_data_sets(train_images, train_labels, validation_images, validation_labels, test_images, test_labels):
  class DataSets(object):
    pass
  data_sets = DataSets()
  data_sets.train = DataSet(train_images, dense_to_one_hot(train_labels))
  data_sets.validation = DataSet(validation_images, dense_to_one_hot(validation_labels))
  data_sets.test = DataSet(test_images, dense_to_one_hot(test_labels))
  return data_sets

# Adapted from the TensorFlow tutorial at
# https://www.tensorflow.org/versions/master/tutorials/index.html
def tensorFlowBasic(X_train, y_train, X_val, y_val, X_test, y_test):
    sess = tf.InteractiveSession()
    x = tf.placeholder(&amp;quot;float&amp;quot;, shape=[None, 400])
    y_ = tf.placeholder(&amp;quot;float&amp;quot;, shape=[None, 10])
    W = tf.Variable(tf.zeros([400,10]))
    b = tf.Variable(tf.zeros([10]))
    sess.run(tf.initialize_all_variables())
    y = tf.nn.softmax(tf.matmul(x,W) + b)
    cross_entropy = -tf.reduce_sum(y_*tf.log(y))
    train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)
    mydata = read_data_sets(X_train, y_train, X_val, y_val, X_test, y_test)

    for i in range(1000):
      batch = mydata.train.next_batch(50)
      train_step.run(feed_dict={x: batch[0], y_: batch[1]})
    
    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, &amp;quot;float&amp;quot;))
    return accuracy.eval(feed_dict={x: mydata.test.images, y_: mydata.test.labels})

def weight_variable(shape):
  initial = tf.truncated_normal(shape, stddev=0.1)
  return tf.Variable(initial)

def bias_variable(shape):
  initial = tf.constant(0.1, shape=shape)
  return tf.Variable(initial)

def conv2d(x, W):
  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=&#39;SAME&#39;)

def max_pool_2x2(x):
  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],
                        strides=[1, 2, 2, 1], padding=&#39;SAME&#39;)

def tensorFlowCNN(X_train, y_train, X_val, y_val, X_test, y_test, add_second_conv_layer = True):
    x = tf.placeholder(&amp;quot;float&amp;quot;, shape=[None, 400])
    y_ = tf.placeholder(&amp;quot;float&amp;quot;, shape=[None, 10])
    sess = tf.InteractiveSession()
    # First Convolutional Layer
    W_conv1 = weight_variable([5, 5, 1, 32])
    b_conv1 = bias_variable([32])
    x_image = tf.reshape(x, [-1,20,20,1])
    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)
    h_pool1 = max_pool_2x2(h_conv1)
    if add_second_conv_layer:
        # Second Convolutional Layer
        W_conv2 = weight_variable([5, 5, 32, 64])
        b_conv2 = bias_variable([64])
        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)
        h_pool2 = max_pool_2x2(h_conv2)
    
        # Densely Connected Layer
        W_fc1 = weight_variable([5 * 5 * 64, 1024])
        b_fc1 = bias_variable([1024])
        h_pool2_flat = tf.reshape(h_pool2, [-1, 5*5*64])
        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)
    else:
        # Densely Connected Layer
        W_fc1 = weight_variable([10 * 10 * 32, 1024])
        b_fc1 = bias_variable([1024])
        h_pool1_flat = tf.reshape(h_pool1, [-1, 10*10*32])
        h_fc1 = tf.nn.relu(tf.matmul(h_pool1_flat, W_fc1) + b_fc1) 
    
    # Dropout
    keep_prob = tf.placeholder(&amp;quot;float&amp;quot;)
    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)
    # Softmax
    W_fc2 = weight_variable([1024, 10])
    b_fc2 = bias_variable([10])
    y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)

    # Train the model
    mydata = read_data_sets(X_train, y_train, X_val, y_val, X_test, y_test)
    cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))
    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)
    correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, &amp;quot;float&amp;quot;))
    sess.run(tf.initialize_all_variables())
    for i in range(1000):
        batch = mydata.train.next_batch(50)
        if i%100 == 0:
            train_accuracy = accuracy.eval(feed_dict={
                x:batch[0], y_: batch[1], keep_prob: 1.0})
            print(&amp;quot;step %d, training accuracy %g&amp;quot;%(i, train_accuracy))
        train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})

    return accuracy.eval(feed_dict={
        x: mydata.test.images, y_: mydata.test.labels, keep_prob: 1.0})

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There&amp;rsquo;s a tensorFlowBasic function which just has an input layer and an output layer, but the fun stuff happens in tensorFlowCNN, which is my first introduction to convolutional neural nets.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;accuracy = tensorFlowCNN(X_train, y_train, X_val, y_val, X_test, y_test)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;step 0, training accuracy 0.1
step 100, training accuracy 0.82
...
step 900, training accuracy 0.96
accuracy: 0.95200002
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I can also pass a parameter telling it not to add a second convolutional layer:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;accuracy = tensorFlowCNN(X_train, y_train, X_val, y_val, X_test, y_test, add_second_conv_layer=False)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;accuracy: 0.94160002
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With the second layer I generally get around 95% accuracy on the test set, without it around 94%. I need to do a lot more experimenting to get a real handle on how to tweak these layers though. And I need to do a lot more reading to understand convolution and pooling better.&lt;/p&gt;

&lt;h3 id=&#34;further-reading:14c7068255137565e326b1d24878d8a5&#34;&gt;Further reading&lt;/h3&gt;

&lt;p&gt;As mentioned at the start, this exercise has mostly just made me realize how much I have yet to learn about this field. Here&amp;rsquo;s what I have on my reading list:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://timdettmers.com/2015/03/26/convolution-deep-learning/&#34;&gt;Understanding Convolution in Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-core-concepts/&#34;&gt;Deep Learning in a Nutshell: Core Concepts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-history-training/&#34;&gt;Deep Learning in a Nutshell: History and Training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Michael Nielson&amp;rsquo;s &lt;a href=&#34;http://neuralnetworksanddeeplearning.com/&#34;&gt;Neural Networks and Deep Learning&lt;/a&gt; ebook&lt;/li&gt;
&lt;li&gt;Stanford course notes on &lt;a href=&#34;http://cs231n.github.io/&#34;&gt;Convolutional Neural Networks for Visual Recognition&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Some TensorFlow-specic stuff:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://shapeofdata.wordpress.com/2015/11/30/the-tensorflow-perspective-on-neural-networks/&#34;&gt;The TensorFlow perspective on neural networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.kdnuggets.com/2015/11/google-tensorflow-deep-learning-disappoints.html&#34;&gt;TensorFlow Disappoints – Google Deep Learning falls shallow&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>What&#39;s this all about?</title>
      <link>http://katbailey.github.io/about/</link>
      <pubDate>Sat, 12 Dec 2015 12:36:25 -0700</pubDate>
      
      <guid>http://katbailey.github.io/about/</guid>
      <description>&lt;p&gt;Having had a philosophical interest in Artificial Intelligence for years I am finally starting to learn about it properly, as in - how it actually works. As far as I recall, my interest in AI began when I took a course on Social and Political Perspectives on Information Technology as part of a Masters in Business Studies in Dublin back in 2001. That&amp;rsquo;s when I first heard of Hubert Dreyfus&amp;rsquo; book &lt;a href=&#34;https://books.google.com/books/about/What_Computers_Still_Can_t_Do.html?id=7vS2y-mQmpAC&#34;&gt;What Computers Still Can&amp;rsquo;t Do&lt;/a&gt;, whose title intrigued me no end. Also around then I read Douglas Hofstadter&amp;rsquo;s &lt;a href=&#34;https://en.wikipedia.org/wiki/G%C3%B6del,_Escher,_Bach&#34;&gt;Gödel, Escher, Bach&lt;/a&gt; for the first time. I&amp;rsquo;ve read it another couple of times since - it is quite possibly my favourite book ever :)&lt;/p&gt;

&lt;p&gt;Anyway, I recently had reason, as part of my job at &lt;a href=&#34;https://www.acquia.com&#34;&gt;Acquia&lt;/a&gt;, to start getting a good understanding of statistical inference. This led me to more topics in data science, including machine learning. So although over the years I&amp;rsquo;ve had all kinds of hand-wavy conversations and philosophical arguments with people about AI, now for the first time ever on this topic I&amp;rsquo;m actually starting to know what I&amp;rsquo;m talking about. And that is exciting.&lt;/p&gt;

&lt;p&gt;So exciting in fact, that I think I might actually manage to write about it on some kind of a regular basis. Hence this blog.&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://katbailey.github.io/images/kat_med.png&#34; /&gt;
    
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;For those interested, this blog was built using &lt;a href=&#34;https://gohugo.io/&#34;&gt;Hugo&lt;/a&gt;, which I must say is a joy to use.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Welch&#39;s T-test in Go</title>
      <link>http://katbailey.github.io/post/welch-t-test-golang/</link>
      <pubDate>Thu, 15 Oct 2015 21:04:38 -0700</pubDate>
      
      <guid>http://katbailey.github.io/post/welch-t-test-golang/</guid>
      <description>&lt;p&gt;I wrote some code for doing a Welch&amp;rsquo;s T-Test in Go. You can read up on what a Welch&amp;rsquo;s t-test is &lt;a href=&#34;https://en.wikipedia.org/wiki/Welch%27s_t_test&#34;&gt;here&lt;/a&gt; but in short it&amp;rsquo;s a significance test for the difference between two treatments (like in an A/B test) where the distributions may have unequal variances.&lt;/p&gt;

&lt;pre&gt;

                 * *
               *     *
              *       *
             *         *
            *           *    
           *            **   
          *             ***  
        *               *****
  * * *                 ***********
 -----------------|-----|-----------
 &lt;/pre&gt;

&lt;p&gt;So if you are doing an A/B test and you have the mean and variance of each treatment, you can get a confidence measure for whether the mean of one is truly higher than the mean of the other.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s the code: &lt;a href=&#34;https://github.com/katbailey/welchttest&#34;&gt;https://github.com/katbailey/welchttest&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>